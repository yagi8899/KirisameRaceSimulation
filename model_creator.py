import psycopg2
import os
from pathlib2 import Path
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import pickle
import numpy as np
import lightgbm as lgb
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import mean_squared_error
import optuna
from sklearn.metrics import ndcg_score
from keiba_constants import get_track_name, format_model_description


def create_universal_model(track_code, kyoso_shubetsu_code, surface_type, 
                          min_distance, max_distance, model_filename, output_dir='models'):
    """
    æ±ç”¨çš„ãªç«¶é¦¬äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ä½œæˆé–¢æ•°
    
    Args:
        track_code (str): ç«¶é¦¬å ´ã‚³ãƒ¼ãƒ‰ ('01'=æœ­å¹Œ, '02'=å‡½é¤¨, ..., '09'=é˜ªç¥, '10'=å°å€‰)
        kyoso_shubetsu_code (str): ç«¶äº‰ç¨®åˆ¥ã‚³ãƒ¼ãƒ‰ ('12'=3æ­³, '13'=3æ­³ä»¥ä¸Š)
        surface_type (str): 'turf' or 'dirt' (èŠã¾ãŸã¯ãƒ€ãƒ¼ãƒˆ)
        min_distance (int): æœ€å°è·é›¢ (ä¾‹: 1000)
        max_distance (int): æœ€å¤§è·é›¢ (ä¾‹: 3600, ä¸Šé™ãªã—ã®å ´åˆã¯9999)
        model_filename (str): ä¿å­˜ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«å (ä¾‹: 'hanshin_turf_3ageup.sav')
        output_dir (str): ãƒ¢ãƒ‡ãƒ«ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 'models')
    
    Returns:
        None: ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
    """
    
    # ã‚«ãƒ¬ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ã‚¹ã‚¯ãƒªãƒ—ãƒˆé…ç½®ç®‡æ‰€ã«å¤‰æ›´
    os.chdir(Path(__file__).parent)
    print(f"ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª:{os.getcwd()}")
    
    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆï¼ˆå­˜åœ¨ã—ãªã„å ´åˆï¼‰
    output_path = Path(output_dir)
    output_path.mkdir(exist_ok=True)
    print(f"ğŸ“ ãƒ¢ãƒ‡ãƒ«ä¿å­˜å…ˆ: {output_path.absolute()}")

    # PostgreSQL ã‚³ãƒã‚¯ã‚·ãƒ§ãƒ³ã®ä½œæˆ
    conn = psycopg2.connect(
        host='localhost',
        port='5432',
        user='postgres',
        password='ahtaht88',
        dbname='keiba'
    )

    # ãƒˆãƒ©ãƒƒã‚¯æ¡ä»¶ã‚’å‹•çš„ã«è¨­å®š
    if surface_type.lower() == 'turf':
        # èŠã®å ´åˆ
        # TODO èŠã¯10ï½22ã¨åºƒãç¯„å›²æŒ‡å®šã™ã‚‹ã€‚èŠã¨ãƒ€ãƒ¼ãƒˆã§ç²¾åº¦ã«é•ã„ãŒå‡ºã‚‹ã‚ˆã†ã§ã‚ã‚Œã°å¯¾è±¡ãƒˆãƒ©ãƒƒã‚¯ã‚³ãƒ¼ãƒ‰ã‚’æ¸›ã‚‰ã™ãªã©ã®å·¥å¤«ãŒå¿…è¦ã‹ã‚‚ã€‚
        track_condition = "cast(rase.track_code as integer) between 10 and 22"
        baba_condition = "ra.babajotai_code_shiba"
    else:
        # ãƒ€ãƒ¼ãƒˆã®å ´åˆ
        track_condition = "cast(rase.track_code as integer) between 23 and 24"
        baba_condition = "ra.babajotai_code_dirt"

    # è·é›¢æ¡ä»¶ã‚’è¨­å®š
    if max_distance == 9999:
        distance_condition = f"cast(rase.kyori as integer) >= {min_distance}"
    else:
        distance_condition = f"cast(rase.kyori as integer) between {min_distance} and {max_distance}"
    
    # ç«¶äº‰ç¨®åˆ¥ã‚’è¨­å®š
    if kyoso_shubetsu_code == '12':
        # 3æ­³æˆ¦
        kyoso_shubetsu_condition = "cast(rase.kyoso_shubetsu_code as integer) = 12"
    elif kyoso_shubetsu_code == '13':
        kyoso_shubetsu_condition = "cast(rase.kyoso_shubetsu_code as integer) >= 13"

    # SQLã‚¯ã‚¨ãƒªã‚’å‹•çš„ã«ç”Ÿæˆ
    sql = f"""
    select * from (
        select
        ra.kaisai_nen,
        ra.kaisai_tsukihi,
        ra.keibajo_code,
        CASE 
            WHEN ra.keibajo_code = '01' THEN 'æœ­å¹Œ' 
            WHEN ra.keibajo_code = '02' THEN 'å‡½é¤¨' 
            WHEN ra.keibajo_code = '03' THEN 'ç¦å³¶' 
            WHEN ra.keibajo_code = '04' THEN 'æ–°æ½Ÿ' 
            WHEN ra.keibajo_code = '05' THEN 'æ±äº¬' 
            WHEN ra.keibajo_code = '06' THEN 'ä¸­å±±' 
            WHEN ra.keibajo_code = '07' THEN 'ä¸­äº¬' 
            WHEN ra.keibajo_code = '08' THEN 'äº¬éƒ½' 
            WHEN ra.keibajo_code = '09' THEN 'é˜ªç¥' 
            WHEN ra.keibajo_code = '10' THEN 'å°å€‰' 
            ELSE '' 
        END keibajo_name,
        ra.race_bango,
        ra.kyori,
        ra.tenko_code,
        {baba_condition} as babajotai_code,
        ra.grade_code,
        ra.kyoso_joken_code,
        ra.kyoso_shubetsu_code,
        ra.track_code,
        seum.ketto_toroku_bango,
        trim(seum.bamei),
        seum.wakuban,
        cast(seum.umaban as integer) as umaban_numeric,
        seum.barei,
        seum.kishu_code,
        seum.chokyoshi_code,
        seum.futan_juryo,
        seum.seibetsu_code,
        CASE WHEN seum.seibetsu_code = '1' THEN '1' ELSE '0' END AS mare_horse,
        CASE WHEN seum.seibetsu_code = '2' THEN '1' ELSE '0' END AS femare_horse,
        CASE WHEN seum.seibetsu_code = '3' THEN '1' ELSE '0' END AS sen_horse,
        CASE WHEN {baba_condition} = '1' THEN '1' ELSE '0' END AS baba_good,
        CASE WHEN {baba_condition} = '2' THEN '1' ELSE '0' END AS baba_slightly_heavy,
        CASE WHEN {baba_condition} = '3' THEN '1' ELSE '0' END AS baba_heavy,
        CASE WHEN {baba_condition} = '4' THEN '1' ELSE '0' END AS baba_defective,
        CASE WHEN ra.tenko_code = '1' THEN '1' ELSE '0' END AS tenko_fine,
        CASE WHEN ra.tenko_code = '2' THEN '1' ELSE '0' END AS tenko_cloudy,
        CASE WHEN ra.tenko_code = '3' THEN '1' ELSE '0' END AS tenko_rainy,
        CASE WHEN ra.tenko_code = '4' THEN '1' ELSE '0' END AS tenko_drizzle,
        CASE WHEN ra.tenko_code = '5' THEN '1' ELSE '0' END AS tenko_snow,
        CASE WHEN ra.tenko_code = '6' THEN '1' ELSE '0' END AS tenko_light_snow,
        18 - cast(seum.kakutei_chakujun as integer) + 1 as kakutei_chakujun_numeric, 
        1.0 / nullif(cast(seum.kakutei_chakujun as integer), 0) as chakujun_score,  --ä¸Šä½ç€é †ã»ã©1ã«è¿‘ããªã‚‹
        1 - (cast(seum.kakutei_chakujun as float) / cast(ra.shusso_tosu as float)) as sotai_chakujun_numeric,
        cast(ra.kyori as integer) /
        (
        FLOOR(cast(seum.soha_time as integer) / 1000) * 60 +
        FLOOR((cast(seum.soha_time as integer) % 1000) / 10) +
        (cast(seum.soha_time as integer) % 10) * 0.1
        ) AS time_index,
        SUM(
            CASE 
                WHEN seum.kakutei_chakujun = '01' THEN 100
                WHEN seum.kakutei_chakujun = '02' THEN 80
                WHEN seum.kakutei_chakujun = '03' THEN 60
                WHEN seum.kakutei_chakujun = '04' THEN 40
                WHEN seum.kakutei_chakujun = '05' THEN 30
                WHEN seum.kakutei_chakujun = '06' THEN 20
                WHEN seum.kakutei_chakujun = '07' THEN 10
                ELSE 5 
            END
            * CASE 
                WHEN ra.grade_code = 'A' THEN 1.00                                                                                          --G1
                WHEN ra.grade_code = 'B' THEN 0.80                                                                                          --G2
                WHEN ra.grade_code = 'C' THEN 0.60                                                                                          --G3
                WHEN ra.grade_code <> 'A' AND ra.grade_code <> 'B' AND ra.grade_code <> 'C' AND ra.kyoso_joken_code = '999' THEN 0.50       --OP
                WHEN ra.grade_code <> 'A' AND ra.grade_code <> 'B' AND ra.grade_code <> 'C' AND ra.kyoso_joken_code = '016' THEN 0.40       --3å‹ã‚¯ãƒ©ã‚¹
                WHEN ra.grade_code <> 'A' AND ra.grade_code <> 'B' AND ra.grade_code <> 'C' AND ra.kyoso_joken_code = '010' THEN 0.30       --2å‹ã‚¯ãƒ©ã‚¹
                WHEN ra.grade_code <> 'A' AND ra.grade_code <> 'B' AND ra.grade_code <> 'C' AND ra.kyoso_joken_code = '005' THEN 0.20       --1å‹ã‚¯ãƒ©ã‚¹
                ELSE 0.10                                                                                                                   --æœªå‹åˆ©
            END
        ) OVER (
            PARTITION BY seum.ketto_toroku_bango
            ORDER BY cast(ra.kaisai_nen as integer), cast(ra.kaisai_tsukihi as integer)
            ROWS BETWEEN 3 PRECEDING AND 1 PRECEDING  
        ) AS past_score  --ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ¥ã‚¹ã‚³ã‚¢
        ,cast(seum.kohan_3f AS FLOAT) / 10 as kohan_3f_sec
        ,CASE 
            WHEN cast(seum.kohan_3f as integer) > 0 THEN
            -- æ¨™æº–ã‚¿ã‚¤ãƒ ã‹ã‚‰ã®å·®ã«å¤‰æ›ï¼ˆå°ã•ã„ã»ã©é€Ÿã„ï¼‰
            CAST(seum.kohan_3f AS FLOAT) / 10 - 
            -- è·é›¢ã”ã¨ã®åŸºæº–ã‚¿ã‚¤ãƒ  (è·é›¢ã«å¿œã˜ãŸè£œæ­£)
            CASE
                WHEN cast(ra.kyori as integer) <= 1600 THEN 33.5  -- ãƒã‚¤ãƒ«ä»¥ä¸‹
                WHEN cast(ra.kyori as integer) <= 2000 THEN 35.0  -- ä¸­è·é›¢
                WHEN cast(ra.kyori as integer) <= 2400 THEN 36.0  -- ä¸­é•·è·é›¢
                ELSE 37.0  -- é•·è·é›¢
            END
            ELSE 0
        END AS kohan_3f_index
    from
        jvd_ra ra 
        inner join ( 
            select
                se.kaisai_nen
                , se.kaisai_tsukihi
                , se.keibajo_code
                , se.race_bango
                , se.kakutei_chakujun
                , se.ketto_toroku_bango
                , se.bamei
                , se.wakuban
                , se.umaban
                , se.barei
                , se.seibetsu_code
                , se.kishu_code
                , se.chokyoshi_code
                , se.futan_juryo
                , se.kohan_3f
                , se.soha_time
            from
                jvd_se se
            where 
                se.kohan_3f <> '000' 
                and se.kohan_3f <> '999'
        ) seum 
            on ra.kaisai_nen = seum.kaisai_nen 
            and ra.kaisai_tsukihi = seum.kaisai_tsukihi 
            and ra.keibajo_code = seum.keibajo_code 
            and ra.race_bango = seum.race_bango 
    where
        cast(ra.kaisai_nen as integer) between 2013 and 2022                  --2013ï½2022
    ) rase 
    where 
    rase.keibajo_code = '{track_code}'                                        --ç«¶é¦¬å ´æŒ‡å®š
    and {kyoso_shubetsu_condition}                                            --ç«¶äº‰ç¨®åˆ¥
    and {track_condition}                                                     --èŠ/ãƒ€ãƒ¼ãƒˆ
    and {distance_condition}                                                  --è·é›¢æ¡ä»¶
    """

    # ãƒ¢ãƒ‡ãƒ«èª¬æ˜ã‚’ç”Ÿæˆ
    model_desc = format_model_description(track_code, kyoso_shubetsu_code, surface_type, min_distance, max_distance)
    print(f"ğŸ‡ ãƒ¢ãƒ‡ãƒ«ä½œæˆé–‹å§‹: {model_desc}")
    
    # SELECTçµæœã‚’DataFrame
    df = pd.read_sql_query(sql=sql, con=conn)
    
    if len(df) == 0:
        print("âŒ æŒ‡å®šã—ãŸæ¡ä»¶ã«åˆè‡´ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚æ¡ä»¶ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        return

    print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(df)}ä»¶")

    # ç€é †ã‚¹ã‚³ã‚¢ãŒ0ã®ãƒ‡ãƒ¼ã‚¿ã¯ç„¡åŠ¹æ‰±ã„ã«ã—ã¦é™¤å¤–
    df = df[df['chakujun_score'] > 0]

    # ã¾ãšãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ã‚’ã—ã£ã‹ã‚Šè¡Œã†
    df = df.apply(pd.to_numeric, errors='coerce')  # æ•°å€¤ã«å¤‰æ›
    df = df.replace('0', np.nan)  # 0ã‚’NaNã«ç½®æ›
    df = df.fillna(0)  # æ¬ æå€¤ã‚’0ã«ç½®æ›

    X = df.loc[:, [
        "kyori",
        "tenko_code",  
        "babajotai_code",
        "seibetsu_code",  
        # "umaban_numeric", 
        # "barei",  
        "futan_juryo",
        "past_score",
        "kohan_3f_index",
        "sotai_chakujun_numeric",
        "time_index",
    ]].astype(float)
    
    # é«˜æ€§èƒ½ãªæ´¾ç”Ÿç‰¹å¾´é‡ã‚’è¿½åŠ ï¼
    # æ ç•ªã¨é ­æ•°ã®æ¯”ç‡ï¼ˆå†…æ æœ‰åˆ©åº¦ï¼‰
    max_wakuban = df.groupby(['kaisai_nen', 'kaisai_tsukihi', 'race_bango'])['wakuban'].transform('max')
    df['wakuban_ratio'] = df['wakuban'] / max_wakuban
    X['wakuban_ratio'] = df['wakuban_ratio']
    
    # æ–¤é‡ã¨é¦¬é½¢ã®æ¯”ç‡ï¼ˆè‹¥é¦¬ã®è² æ‹…èƒ½åŠ›ï¼‰
    df['futan_per_barei'] = df['futan_juryo'] / df['barei'].replace(0, 1)
    X['futan_per_barei'] = df['futan_per_barei']

    # ğŸš€ é«˜ç²¾åº¦äºŒæ¬¡ç‰¹å¾´é‡ã‚’è¿½åŠ ï¼ˆäºˆæ¸¬ã‚¹ã‚³ã‚¢é‡è¤‡å›é¿ + ç²¾åº¦å‘ä¸Šï¼‰
    # ã‚·ãƒ³ãƒ—ãƒ«ãªç‰¹å¾´é‡ã‹ã‚‰å§‹ã‚ã¦éå­¦ç¿’ã‚’é˜²ã
    
    # é¦¬ç•ªÃ—è·é›¢ã®ç›¸äº’ä½œç”¨ï¼ˆå†…å¤–æ ã®è·é›¢é©æ€§ï¼‰
    df['umaban_kyori_interaction'] = df['umaban_numeric'] * df['kyori'] / 1000  # ã‚¹ã‚±ãƒ¼ãƒ«èª¿æ•´
    X['umaban_kyori_interaction'] = df['umaban_kyori_interaction']
    
    # é¦¬é½¢ã®éç·šå½¢å¤‰æ›ï¼ˆç«¶èµ°é¦¬ã®ãƒ”ãƒ¼ã‚¯å¹´é½¢åŠ¹æœï¼‰
    # df['barei_squared'] = df['barei'] ** 2
    # X['barei_squared'] = df['barei_squared']
    df['barei_peak_distance'] = abs(df['barei'] - 4)  # 4æ­³ã‚’ãƒ”ãƒ¼ã‚¯ã¨ä»®å®š
    X['barei_peak_distance'] = df['barei_peak_distance']

    # ãƒ¬ãƒ¼ã‚¹å†…ã§ã®é¦¬ç•ªç›¸å¯¾ä½ç½®ï¼ˆé ­æ•°ã«ã‚ˆã‚‹æ­£è¦åŒ–ï¼‰
    df['umaban_percentile'] = df.groupby(['kaisai_nen', 'kaisai_tsukihi', 'race_bango'])['umaban_numeric'].transform(
        lambda x: x.rank(pct=True)
    )
    X['umaban_percentile'] = df['umaban_percentile']
    
    # # å¾®å°ãªå€‹ä½“è­˜åˆ¥å­ã‚’è¿½åŠ ï¼ˆé‡è¤‡å®Œå…¨å›é¿ã®ãŸã‚ï¼‰
    # # é¦¬ç•ªãƒ™ãƒ¼ã‚¹ã®æ¥µå°èª¿æ•´å€¤
    # df['micro_adjustment'] = df['umaban_numeric'] / 1000000  # 0.000001ã€œ0.000018ç¨‹åº¦
    # X['micro_adjustment'] = df['micro_adjustment']

    # ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã‚’ä½œæˆ
    X['kyori'] = X['kyori'].astype('category')
    X['tenko_code'] = X['tenko_code'].astype('category')
    X['babajotai_code'] = X['babajotai_code'].astype('category')
    X['seibetsu_code'] = X['seibetsu_code'].astype('category')
    categorical_features = ['kyori', 'tenko_code', 'babajotai_code', 'seibetsu_code']

    #ç›®çš„å¤‰æ•°ã‚’è¨­å®š
    y = df['kakutei_chakujun_numeric'].astype(int)

    # ãƒ¬ãƒ¼ã‚¹ã”ã¨ã®ã‚°ãƒ«ãƒ¼ãƒ—ã‚’æ­£ã—ãè¨ˆç®—
    df['group_id'] = df.groupby(['kaisai_nen', 'kaisai_tsukihi', 'race_bango']).ngroup()
    groups = df['group_id'].values
    print(f"ã‚°ãƒ«ãƒ¼ãƒ—æ•°: {len(set(groups))}")  # ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªã‚°ãƒ«ãƒ¼ãƒ—ã®æ•°
    print(f"ãƒ‡ãƒ¼ã‚¿æ•°: {len(groups)}")  # å…¨ãƒ‡ãƒ¼ã‚¿æ•°

    # ãƒ‡ãƒ¼ã‚¿ã¨ã‚°ãƒ«ãƒ¼ãƒ—æ•°ãŒä¸€è‡´ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª
    if len(groups) != len(X):
        raise ValueError(f"ãƒ‡ãƒ¼ã‚¿ä»¶æ•°({len(X)})ã¨ã‚°ãƒ«ãƒ¼ãƒ—ã®æ•°({len(groups)})ãŒä¸€è‡´ã—ã¾ã›ã‚“ï¼")

    # ğŸ”¥ã“ã“ã‹ã‚‰å¤‰æ›´ğŸ”¥
    # æ™‚ç³»åˆ—ã§ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²ï¼ˆå¤ã„å¹´æœˆã‚’è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«ã€æ–°ã—ã„å¹´æœˆã‚’ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«ï¼‰
    # ã¾ãšæ—¥ä»˜ã§ã‚½ãƒ¼ãƒˆ
    df['kaisai_date'] = df['kaisai_nen'].astype(str) + df['kaisai_tsukihi'].astype(str).str.zfill(4)
    sorted_df = df.sort_values('kaisai_date')
    
    # ãƒ‡ãƒ¼ã‚¿ã®75%ã‚’è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã€25%ã‚’ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«
    train_size = int(len(sorted_df) * 0.75)
    
    # æ™‚ç³»åˆ—é †ã«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’åˆ†ã‘ã‚‹
    train_indices = sorted_df.index[:train_size]
    test_indices = sorted_df.index[train_size:]
    
    # åˆ†å‰²ã—ãŸã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½¿ã£ã¦ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
    X_train = X.loc[train_indices]
    X_test = X.loc[test_indices]
    y_train = y.loc[train_indices]
    y_test = y.loc[test_indices]
    groups_train = groups[train_indices]
    groups_test = groups[test_indices]
    
    # ç¢ºèªã—ã¦ã¿ã‚‹
    train_dates = sorted_df.loc[train_indices, 'kaisai_date'].unique()
    test_dates = sorted_df.loc[test_indices, 'kaisai_date'].unique()
    print(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®æ—¥ä»˜ç¯„å›²: {min(train_dates)} ã€œ {max(train_dates)}")
    print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æ—¥ä»˜ç¯„å›²: {min(test_dates)} ã€œ {max(test_dates)}")

    # Optunaã®objectiveé–¢æ•°
    def objective(trial):
        param = {
            'objective': 'lambdarank',
            'metric': 'ndcg',                                                              # ä¸Šä½5ç€ã¾ã§ã®ä¸¦ã³é †ã‚’é‡è¦–
            'boosting_type': 'gbdt',
            'verbosity': -1,
            'random_state': 42,
            'num_leaves': trial.suggest_int('num_leaves', 20, 300),
            'max_depth': trial.suggest_int('max_depth', 3, 12),
            'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.3),
            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),
            'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),
            'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),
            'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),
            'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-8, 10.0),
            'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-8, 10.0),
        }

        # ã‚°ãƒ«ãƒ¼ãƒ—ã”ã¨ã®é ­æ•°ã«å¤‰æ›ï¼â†ã“ã“ãŒãƒã‚¤ãƒ³ãƒˆï¼
        train_group_sizes = pd.Series(groups_train).value_counts().sort_index().values
        test_group_sizes = pd.Series(groups_test).value_counts().sort_index().values
        
        dtrain = lgb.Dataset(X_train, label=y_train, group=train_group_sizes, categorical_feature=categorical_features)
        dvalid = lgb.Dataset(X_test, label=y_test, group=test_group_sizes, categorical_feature=categorical_features)

        tmp_model = lgb.train(
            param,
            dtrain,
            valid_sets=[dvalid],
            valid_names=['valid'],
            callbacks=[lgb.early_stopping(30), lgb.log_evaluation(0)]
        )

        preds = tmp_model.predict(X_test, num_iteration=tmp_model.best_iteration)
        ndcg = ndcg_score([y_test.values], [preds], k=10)

        return ndcg

    # Optunaã®ã‚¹ã‚¿ãƒ‡ã‚£ä½œæˆï¼†æœ€é©åŒ–å®Ÿè¡Œ
    print("ğŸ” ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ã‚’é–‹å§‹...")
    study = optuna.create_study(direction="maximize")
    study.optimize(objective, n_trials=50)

    print('Best trial:')
    print(study.best_trial.params)

    # æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½¿ã£ã¦å†å­¦ç¿’
    best_params = study.best_trial.params

    best_params.update({
        'objective': 'lambdarank',
        'metric': 'ndcg',
        'boosting_type': 'gbdt',
        'verbosity': 0,  # å­¦ç¿’ã®é€²æ—ã‚’è¡¨ç¤º
        'random_state': 42,
    })

    # ã‚°ãƒ«ãƒ¼ãƒ—ãƒ‡ãƒ¼ã‚¿ã‚’æ­£ã—ãæº–å‚™
    # ãƒ¬ãƒ¼ã‚¹ã”ã¨ã®å‡ºèµ°é ­æ•°ã‚’è¨ˆç®—
    train_group_sizes = pd.Series(groups_train).value_counts().sort_index().values
    test_group_sizes = pd.Series(groups_test).value_counts().sort_index().values
    
    print(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ãƒ¬ãƒ¼ã‚¹æ•°: {len(train_group_sizes)}")
    print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ãƒ¬ãƒ¼ã‚¹æ•°: {len(test_group_sizes)}")
    
    # LightGBMç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ
    dtrain = lgb.Dataset(X_train, label=y_train, group=train_group_sizes, categorical_feature=categorical_features)
    dvalid = lgb.Dataset(X_test, label=y_test, group=test_group_sizes, categorical_feature=categorical_features)

    # æœ€é©åŒ–ã•ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’
    print("ğŸ”¥ æœ€é©åŒ–ã•ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã™ã‚‹ã‚ˆï¼")
    model = lgb.train(
        best_params,
        dtrain,
        valid_sets=[dvalid],
        valid_names=['ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿'],
        num_boost_round=1000,  # æœ€å¤§åå¾©å›æ•°
        callbacks=[
            lgb.early_stopping(30),  # 30å›æ”¹å–„ãŒãªã‘ã‚Œã°æ—©æœŸçµ‚äº†
            lgb.log_evaluation(10)   # 10å›ã”ã¨ã«çµæœè¡¨ç¤º
        ]
    )

    # ç‰¹å¾´é‡ã®é‡è¦åº¦ã‚’ç¢ºèªã™ã‚‹ï¼ˆãƒ¢ãƒ‡ãƒ«ãŒã©ã®æƒ…å ±ã‚’é‡è¦–ã—ã¦ã„ã‚‹ã‹ï¼‰
    importance = model.feature_importance()
    feature_names = X.columns
    feature_importance = pd.DataFrame({'feature': feature_names, 'importance': importance})
    feature_importance = feature_importance.sort_values('importance', ascending=False)
    print("ç‰¹å¾´é‡ã®é‡è¦åº¦:")
    print(feature_importance)

    # ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã™ã‚‹
    model_filepath = output_path / model_filename
    pickle.dump(model, open(model_filepath, 'wb'))
    print(f"âœ… ãƒ¢ãƒ‡ãƒ«ã‚’ {model_filepath} ã«ä¿å­˜ã—ã¾ã—ãŸ")

    # ã‚³ãƒã‚¯ã‚·ãƒ§ãƒ³ã‚’ã‚¯ãƒ­ãƒ¼ã‚º
    conn.close()


# æ—§æ¥ã®é–¢æ•°ã‚’ç¶­æŒï¼ˆäº’æ›æ€§ã®ãŸã‚ï¼‰
def make_model():
    """
    æ—§ãƒãƒ¼ã‚¸ãƒ§ãƒ³äº’æ›æ€§ã®ãŸã‚ã®é–¢æ•°
    é˜ªç¥ç«¶é¦¬å ´ã®ï¼“æ­³ä»¥ä¸ŠèŠä¸­é•·è·é›¢ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
    
    æ³¨æ„: äº’æ›æ€§ã®ãŸã‚ã€ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜ã•ã‚Œã¾ã™
    """
    create_universal_model(
        track_code='09',  # é˜ªç¥
        kyoso_shubetsu_code='13',  # 3æ­³ä»¥ä¸Š
        surface_type='turf',  # èŠ
        min_distance=1700,  # ä¸­é•·è·é›¢
        max_distance=9999,  # ä¸Šé™ãªã—
        model_filename='hanshin_shiba_3ageup_model.sav',
        output_dir='.'  # ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜ï¼ˆæ—¢å­˜ã®å‹•ä½œã‚’ç¶­æŒï¼‰
    )


if __name__ == '__main__':
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œï¼šæ—§ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¨åŒã˜ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
    make_model()