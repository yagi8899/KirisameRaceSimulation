import psycopg2
import os
from pathlib2 import Path
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import pickle
import numpy as np
import lightgbm as lgb
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import mean_squared_error
import optuna
from sklearn.metrics import ndcg_score
from keiba_constants import get_track_name, format_model_description
from datetime import datetime
from db_query_builder import build_race_data_query


def create_universal_model(track_code, kyoso_shubetsu_code, surface_type, 
                          min_distance, max_distance, model_filename, output_dir='models',
                          year_start=2013, year_end=2022):
    """
    æ±ç”¨çš„ãªç«¶é¦¬äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ä½œæˆé–¢æ•°
    
    Args:
        track_code (str): ç«¶é¦¬å ´ã‚³ãƒ¼ãƒ‰ ('01'=æœ­å¹Œ, '02'=å‡½é¤¨, ..., '09'=é˜ªç¥, '10'=å°å€‰)
        kyoso_shubetsu_code (str): ç«¶äº‰ç¨®åˆ¥ã‚³ãƒ¼ãƒ‰ ('12'=3æ­³, '13'=3æ­³ä»¥ä¸Š)
        surface_type (str): 'turf' or 'dirt' (èŠã¾ãŸã¯ãƒ€ãƒ¼ãƒˆ)
        min_distance (int): æœ€å°è·é›¢ (ä¾‹: 1000)
        max_distance (int): æœ€å¤§è·é›¢ (ä¾‹: 3600, ä¸Šé™ãªã—ã®å ´åˆã¯9999)
        model_filename (str): ä¿å­˜ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«å (ä¾‹: 'hanshin_turf_3ageup.sav')
        output_dir (str): ãƒ¢ãƒ‡ãƒ«ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 'models')
        year_start (int): å­¦ç¿’ãƒ‡ãƒ¼ã‚¿é–‹å§‹å¹´ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 2013)
        year_end (int): å­¦ç¿’ãƒ‡ãƒ¼ã‚¿çµ‚äº†å¹´ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 2022)
    
    Returns:
        None: ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
    """
    
    # ã‚«ãƒ¬ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ã‚¹ã‚¯ãƒªãƒ—ãƒˆé…ç½®ç®‡æ‰€ã«å¤‰æ›´
    os.chdir(Path(__file__).parent)
    print(f"ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª:{os.getcwd()}")
    
    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆï¼ˆå­˜åœ¨ã—ãªã„å ´åˆï¼‰
    output_path = Path(output_dir)
    output_path.mkdir(exist_ok=True)
    print(f"[FILE] ãƒ¢ãƒ‡ãƒ«ä¿å­˜å…ˆ: {output_path.absolute()}")

    # PostgreSQL ã‚³ãƒã‚¯ã‚·ãƒ§ãƒ³ã®ä½œæˆ
    conn = psycopg2.connect(
        host='localhost',
        port='5432',
        user='postgres',
        password='ahtaht88',
        dbname='keiba'
    )

    # SQLã‚¯ã‚¨ãƒªã‚’å…±é€šåŒ–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã§ç”Ÿæˆ
    sql = build_race_data_query(
        track_code=track_code,
        year_start=year_start,
        year_end=year_end,
        surface_type=surface_type,
        distance_min=min_distance,
        distance_max=max_distance,
        kyoso_shubetsu_code=kyoso_shubetsu_code,
        include_payout=False  # model_creator.pyã§ã¯æ‰•ã„æˆ»ã—æƒ…å ±ä¸è¦
    )

    # ãƒ¢ãƒ‡ãƒ«èª¬æ˜ã‚’ç”Ÿæˆ
    model_desc = format_model_description(track_code, kyoso_shubetsu_code, surface_type, min_distance, max_distance)
    print(f"[RACE] ãƒ¢ãƒ‡ãƒ«ä½œæˆé–‹å§‹: {model_desc}")
    
    # SQLã‚’ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›ï¼ˆå¸¸ã«ä¸Šæ›¸ãï¼‰
    log_filepath = Path('sql_log.txt')
    with open(log_filepath, 'w', encoding='utf-8') as f:
        f.write(f"=== ãƒ¢ãƒ‡ãƒ«ä½œæˆSQL ===\n")
        f.write(f"ãƒ¢ãƒ‡ãƒ«: {model_desc}\n")
        f.write(f"ä½œæˆæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"\n{sql}\n")
    print(f"[NOTE] SQLã‚’ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›: {log_filepath}")
    
    # SELECTçµæœã‚’DataFrame
    df = pd.read_sql_query(sql=sql, con=conn)
    
    if len(df) == 0:
        print("[ERROR] æŒ‡å®šã—ãŸæ¡ä»¶ã«åˆè‡´ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚æ¡ä»¶ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        return

    print(f"[+] ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(df)}ä»¶")

    # ç€é †ã‚¹ã‚³ã‚¢ãŒ0ã®ãƒ‡ãƒ¼ã‚¿ã¯ç„¡åŠ¹æ‰±ã„ã«ã—ã¦é™¤å¤–
    df = df[df['chakujun_score'] > 0]

    # ä¿®æ­£: ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã‚’é©åˆ‡ã«å®Ÿæ–½
    # é¨æ‰‹ã‚³ãƒ¼ãƒ‰ãƒ»èª¿æ•™å¸«ã‚³ãƒ¼ãƒ‰ãƒ»é¦¬åãªã©ã®æ–‡å­—åˆ—åˆ—ã‚’ä¿æŒã—ãŸã¾ã¾ã€æ•°å€¤åˆ—ã®ã¿ã‚’å‡¦ç†
    print("[TEST] ãƒ‡ãƒ¼ã‚¿å‹ç¢ºèª...")
    print(f"  kishu_codeå‹ï¼ˆä¿®æ­£å‰ï¼‰: {df['kishu_code'].dtype}")
    print(f"  kishu_codeã‚µãƒ³ãƒ—ãƒ«: {df['kishu_code'].head(5).tolist()}")
    print(f"  kishu_codeãƒ¦ãƒ‹ãƒ¼ã‚¯æ•°: {df['kishu_code'].nunique()}")
    
    # æ•°å€¤åŒ–ã™ã‚‹åˆ—ã‚’æ˜ç¤ºçš„ã«æŒ‡å®šï¼ˆæ–‡å­—åˆ—åˆ—ã¯é™¤å¤–ï¼‰
    numeric_columns = [
        'wakuban', 'umaban_numeric', 'barei', 'futan_juryo', 'tansho_odds',
        'kaisai_nen', 'kaisai_tsukihi', 'race_bango', 'kyori', 'shusso_tosu',
        'tenko_code', 'babajotai_code', 'grade_code', 'kyoso_joken_code',
        'kyoso_shubetsu_code', 'track_code', 'seibetsu_code',
        'kakutei_chakujun_numeric', 'chakujun_score', 'past_avg_sotai_chakujun',
        'time_index', 'past_score', 'kohan_3f_index', 'corner_1', 'corner_2', 
        'corner_3', 'corner_4', 'kyakushitsu_hantei'
    ]
    
    # æ•°å€¤åŒ–ã™ã‚‹åˆ—ã®ã¿å‡¦ç†ï¼ˆæ–‡å­—åˆ—åˆ—ã¯ä¿æŒï¼‰
    for col in numeric_columns:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce')
    
    # æ¬ æå€¤ã‚’0ã§åŸ‹ã‚ã‚‹ï¼ˆæ•°å€¤åˆ—ã®ã¿ã€å­˜åœ¨ã™ã‚‹åˆ—ã®ã¿å‡¦ç†ï¼‰
    existing_numeric_columns = [col for col in numeric_columns if col in df.columns]
    df[existing_numeric_columns] = df[existing_numeric_columns].fillna(0)
    
    # æ–‡å­—åˆ—å‹ã®åˆ—ã¯ãã®ã¾ã¾ä¿æŒï¼ˆkishu_code, chokyoshi_code, bamei ãªã©ï¼‰
    print(f"  kishu_codeå‹ï¼ˆä¿®æ­£å¾Œï¼‰: {df['kishu_code'].dtype}")
    print(f"  kishu_codeã‚µãƒ³ãƒ—ãƒ«: {df['kishu_code'].head(5).tolist()}")
    print("[OK] ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†å®Œäº†ï¼ˆæ–‡å­—åˆ—åˆ—ã‚’ä¿æŒï¼‰")

    # past_avg_sotai_chakujunã¯SQLã§è¨ˆç®—æ¸ˆã¿ã®å˜ç´”ç§»å‹•å¹³å‡ã‚’ä½¿ç”¨
    # (EWMå®Ÿé¨“ã®çµæœã€å˜ç´”å¹³å‡ã®æ–¹ãŒè¤‡å‹ãƒ»ä¸‰é€£è¤‡ã§å®‰å®šã—ãŸæ€§èƒ½ã‚’ç¤ºã—ãŸ)

    X = df.loc[:, [
        # "futan_juryo",
        "past_score",
        "kohan_3f_index",
        "past_avg_sotai_chakujun",
        "time_index",
    ]].astype(float)
    
    # é«˜æ€§èƒ½ãªæ´¾ç”Ÿç‰¹å¾´é‡ã‚’è¿½åŠ ï¼
    # æ ç•ªã¨é ­æ•°ã®æ¯”ç‡ï¼ˆå†…æ æœ‰åˆ©åº¦ï¼‰
    max_wakuban = df.groupby(['kaisai_nen', 'kaisai_tsukihi', 'race_bango'])['wakuban'].transform('max')
    df['wakuban_ratio'] = df['wakuban'] / max_wakuban
    X['wakuban_ratio'] = df['wakuban_ratio']
    
    # æ–¤é‡ã¨é¦¬é½¢ã®æ¯”ç‡ï¼ˆè‹¥é¦¬ã®è² æ‹…èƒ½åŠ›ï¼‰
    df['futan_per_barei'] = df['futan_juryo'] / df['barei'].replace(0, 1)
    X['futan_per_barei'] = df['futan_per_barei']

    # [START] é«˜ç²¾åº¦äºŒæ¬¡ç‰¹å¾´é‡ã‚’è¿½åŠ ï¼ˆäºˆæ¸¬ã‚¹ã‚³ã‚¢é‡è¤‡å›é¿ + ç²¾åº¦å‘ä¸Šï¼‰
    # ã‚·ãƒ³ãƒ—ãƒ«ãªç‰¹å¾´é‡ã‹ã‚‰å§‹ã‚ã¦éå­¦ç¿’ã‚’é˜²ã
    
    # é¦¬ç•ªÃ—è·é›¢ã®ç›¸äº’ä½œç”¨ï¼ˆå†…å¤–æ ã®è·é›¢é©æ€§ï¼‰
    df['umaban_kyori_interaction'] = df['umaban_numeric'] * df['kyori'] / 1000  # ã‚¹ã‚±ãƒ¼ãƒ«èª¿æ•´
    X['umaban_kyori_interaction'] = df['umaban_kyori_interaction']
    
    # çŸ­è·é›¢ç‰¹åŒ–ç‰¹å¾´é‡
    # æ ç•ªÃ—è·é›¢ã®ç›¸äº’ä½œç”¨ï¼ˆçŸ­è·é›¢ã»ã©å†…æ æœ‰åˆ©ã‚’æ•°å€¤åŒ–ï¼‰
    # è·é›¢ãŒçŸ­ã„ã»ã©æ ç•ªã®å½±éŸ¿ãŒå¤§ãã„: (2000 - è·é›¢) / 1000 ã§é‡ã¿ä»˜ã‘
    df['wakuban_kyori_interaction'] = df['wakuban'] * (2000 - df['kyori']) / 1000
    X['wakuban_kyori_interaction'] = df['wakuban_kyori_interaction']
    
    # æ”¹å–„ã•ã‚ŒãŸç‰¹å¾´é‡
    # 2. futan_per_bareiã®éç·šå½¢å¤‰æ›
    # df['futan_per_barei_log'] = np.log(df['futan_per_barei'].clip(lower=0.1))
    # X['futan_per_barei_log'] = df['futan_per_barei_log']
    
    # æœŸå¾…æ–¤é‡ã‹ã‚‰ã®å·®åˆ†ï¼ˆå¹´é½¢åˆ¥æœŸå¾…æ–¤é‡ã¨ã®å·®ï¼‰
    expected_weight_by_age = {2: 48, 3: 52, 4: 55, 5: 57, 6: 57, 7: 56, 8: 55}
    df['futan_deviation'] = df.apply(
        lambda row: row['futan_juryo'] - expected_weight_by_age.get(row['barei'], 55), 
        axis=1
    )
    X['futan_deviation'] = df['futan_deviation']
    
    # # 4. è¤‡æ•°ã®ãƒ”ãƒ¼ã‚¯å¹´é½¢ãƒ‘ã‚¿ãƒ¼ãƒ³
    # df['barei_peak_distance'] = abs(df['barei'] - 4)  # 4æ­³ã‚’ãƒ”ãƒ¼ã‚¯ã¨ä»®å®šï¼ˆæ—¢å­˜ï¼‰
    # X['barei_peak_distance'] = df['barei_peak_distance']
    
    # # 3æ­³çŸ­è·é›¢ãƒ”ãƒ¼ã‚¯ï¼ˆæ—©ç†Ÿå‹ï¼‰
    # df['barei_peak_short'] = abs(df['barei'] - 3)
    # X['barei_peak_short'] = df['barei_peak_short']
    
    # # 5æ­³é•·è·é›¢ãƒ”ãƒ¼ã‚¯ï¼ˆæ™©æˆå‹ï¼‰
    # df['barei_peak_long'] = abs(df['barei'] - 5)
    # X['barei_peak_long'] = df['barei_peak_long']

    # 5. æ ç•ªãƒã‚¤ã‚¢ã‚¹ã‚¹ã‚³ã‚¢ï¼ˆæ ç•ªã®æ­´å²çš„å„ªä½æ€§ã‚’æ•°å€¤åŒ–ï¼‰
    # æ ç•ªåˆ¥ã®æ­´å²çš„ç€é †åˆ†å¸ƒã‚’è¨ˆç®—
    wakuban_stats = df.groupby('wakuban').agg({
        'kakutei_chakujun_numeric': ['mean', 'std', 'count']
    }).round(4)
    wakuban_stats.columns = ['waku_avg_rank', 'waku_std_rank', 'waku_count']
    wakuban_stats = wakuban_stats.reset_index()
    
    # å…¨ä½“å¹³å‡ã‹ã‚‰ã®åå·®ã§ãƒã‚¤ã‚¢ã‚¹ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
    overall_avg_rank = df['kakutei_chakujun_numeric'].mean()
    wakuban_stats['wakuban_bias_score'] = (overall_avg_rank - wakuban_stats['waku_avg_rank']) / wakuban_stats['waku_std_rank']
    wakuban_stats['wakuban_bias_score'] = wakuban_stats['wakuban_bias_score'].fillna(0)  # NaNã‚’0ã§åŸ‹ã‚ã‚‹
    
    # DataFrameã«ãƒãƒ¼ã‚¸
    df = df.merge(wakuban_stats[['wakuban', 'wakuban_bias_score']], on='wakuban', how='left')
    # X['wakuban_bias_score'] = df['wakuban_bias_score']

    # ãƒ¬ãƒ¼ã‚¹å†…ã§ã®é¦¬ç•ªç›¸å¯¾ä½ç½®ï¼ˆé ­æ•°ã«ã‚ˆã‚‹æ­£è¦åŒ–ï¼‰
    df['umaban_percentile'] = df.groupby(['kaisai_nen', 'kaisai_tsukihi', 'race_bango'])['umaban_numeric'].transform(
        lambda x: x.rank(pct=True)
    )
    X['umaban_percentile'] = df['umaban_percentile']
    
    # ç ”ç©¶ç”¨ç‰¹å¾´é‡ è¿½åŠ 
    # æ–¤é‡åå·®å€¤ï¼ˆãƒ¬ãƒ¼ã‚¹å†…ã§æ¨™æº–åŒ–ï¼‰
    # ãƒ¬ãƒ¼ã‚¹å†…ã®å¹³å‡ã¨æ¨™æº–åå·®ã‚’è¨ˆç®—ã—ã¦ã€å„é¦¬ã®æ–¤é‡ãŒã©ã‚Œãã‚‰ã„é‡ã„/è»½ã„ã‹ã‚’è¡¨ç¾
    race_group = df.groupby(['kaisai_nen', 'kaisai_tsukihi', 'race_bango'])['futan_juryo']
    df['futan_mean'] = race_group.transform('mean')
    df['futan_std'] = race_group.transform('std')
    
    # æ¨™æº–åå·®ãŒ0ã®å ´åˆï¼ˆå…¨é ­åŒã˜æ–¤é‡ï¼‰ã¯0ã«ã™ã‚‹
    df['futan_zscore'] = np.where(
        df['futan_std'] > 0,
        (df['futan_juryo'] - df['futan_mean']) / df['futan_std'],
        0
    )
    X['futan_zscore'] = df['futan_zscore']
    
    # ãƒ¬ãƒ¼ã‚¹å†…ã§ã®æ–¤é‡é †ä½ï¼ˆãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«ï¼‰
    # 0.0=æœ€è»½é‡ã€1.0=æœ€é‡é‡
    df['futan_percentile'] = race_group.transform(lambda x: x.rank(pct=True))
    X['futan_percentile'] = df['futan_percentile']

    # æ–°æ©Ÿèƒ½: è·é›¢é©æ€§ã‚¹ã‚³ã‚¢ã‚’è¿½åŠ ï¼ˆ3ç¨®é¡ï¼‰
    print("[RACE] è·é›¢é©æ€§ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ä¸­...")
    
    # è·é›¢ã‚«ãƒ†ã‚´ãƒªåˆ†é¡é–¢æ•°
    def categorize_distance(kyori):
        """è·é›¢ã‚’4ã‚«ãƒ†ã‚´ãƒªã«åˆ†é¡"""
        if kyori <= 1400:
            return 'short'  # çŸ­è·é›¢
        elif kyori <= 1800:
            return 'mile'   # ãƒã‚¤ãƒ«
        elif kyori <= 2400:
            return 'middle' # ä¸­è·é›¢
        else:
            return 'long'   # é•·è·é›¢
    
    # ä»Šå›ã®ãƒ¬ãƒ¼ã‚¹ã®è·é›¢ã‚«ãƒ†ã‚´ãƒªã‚’è¿½åŠ 
    df['distance_category'] = df['kyori'].apply(categorize_distance)
    
    # é‡è¦: é¦¬å ´æƒ…å ±ã‚‚å…ˆã«è¿½åŠ ï¼ˆdf_sortedã§ä½¿ã†ãŸã‚ï¼‰
    # èŠ/ãƒ€ãƒ¼ãƒˆåˆ†é¡é–¢æ•°
    def categorize_surface(track_code):
        """ãƒˆãƒ©ãƒƒã‚¯ã‚³ãƒ¼ãƒ‰ã‹ã‚‰èŠ/ãƒ€ãƒ¼ãƒˆã‚’åˆ¤å®š"""
        track_code_int = int(track_code)
        if 10 <= track_code_int <= 22:
            return 'turf'  # èŠ
        elif 23 <= track_code_int <= 24:
            return 'dirt'  # ãƒ€ãƒ¼ãƒˆ
        else:
            return 'unknown'
    
    # é¦¬å ´çŠ¶æ…‹åˆ†é¡é–¢æ•°
    def categorize_baba_condition(baba_code):
        """é¦¬å ´çŠ¶æ…‹ã‚³ãƒ¼ãƒ‰ã‚’åˆ†é¡"""
        if baba_code == 1:
            return 'good'      # è‰¯
        elif baba_code == 2:
            return 'slightly'  # ç¨é‡
        elif baba_code == 3:
            return 'heavy'     # é‡
        elif baba_code == 4:
            return 'bad'       # ä¸è‰¯
        else:
            return 'unknown'
    
    # ä»Šå›ã®ãƒ¬ãƒ¼ã‚¹ã®é¦¬å ´æƒ…å ±ã‚’è¿½åŠ 
    df['surface_type'] = df['track_code'].apply(categorize_surface)
    df['baba_condition'] = df['babajotai_code'].apply(categorize_baba_condition)
    
    # æ™‚ç³»åˆ—é †ã«ã‚½ãƒ¼ãƒˆï¼ˆé¦¬ã”ã¨ã«éå»ãƒ‡ãƒ¼ã‚¿ã‚’å‚ç…§ã™ã‚‹ãŸã‚ï¼‰
    df_sorted = df.sort_values(['ketto_toroku_bango', 'kaisai_nen', 'kaisai_tsukihi']).copy()
    
    # è·é›¢ã‚«ãƒ†ã‚´ãƒªåˆ¥é©æ€§ã‚¹ã‚³ã‚¢ï¼ˆåŒã˜ã‚«ãƒ†ã‚´ãƒªã§ã®éå»5èµ°ã®å¹³å‡ç›¸å¯¾ç€é †ï¼‰
    def calc_distance_category_score(group):
        """å„é¦¬ã®è·é›¢ã‚«ãƒ†ã‚´ãƒªåˆ¥é©æ€§ã‚’è¨ˆç®—"""
        scores = []
        for idx in range(len(group)):
            if idx == 0:
                scores.append(0.5)  # [OK] ä¿®æ­£: ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã¯ä¸­ç«‹å€¤
                continue
            
            # ç¾åœ¨ã®ãƒ¬ãƒ¼ã‚¹ã®è·é›¢ã‚«ãƒ†ã‚´ãƒª
            current_category = group.iloc[idx]['distance_category']
            
            # éå»ã®ãƒ¬ãƒ¼ã‚¹ï¼ˆåŒã˜ã‚«ãƒ†ã‚´ãƒªï¼‰ã‹ã‚‰ç›´è¿‘5èµ°ã‚’å–å¾—
            past_same_category = group.iloc[:idx][
                group.iloc[:idx]['distance_category'] == current_category
            ].tail(5)
            
            if len(past_same_category) > 0:
                # ç›¸å¯¾ç€é †ã®å¹³å‡ï¼ˆ1 - ç€é †/18ï¼‰
                avg_score = (1 - (past_same_category['kakutei_chakujun_numeric'] / 18.0)).mean()
                scores.append(avg_score)
            else:
                scores.append(0.5)  # [OK] ä¿®æ­£: ãƒ‡ãƒ¼ã‚¿ãªã—ã¯ä¸­ç«‹å€¤
        
        return pd.Series(scores, index=group.index)
    
    df_sorted['distance_category_score'] = df_sorted.groupby('ketto_toroku_bango', group_keys=False).apply(
        calc_distance_category_score
    ).values
    
    # è¿‘ä¼¼è·é›¢ã§ã®æˆç¸¾ï¼ˆÂ±200mä»¥å†…ã€ç›´è¿‘10èµ°ï¼‰
    def calc_similar_distance_score(group):
        """è¿‘ä¼¼è·é›¢ã§ã®é©æ€§ã‚’è¨ˆç®—"""
        scores = []
        for idx in range(len(group)):
            if idx == 0:
                scores.append(0.5)  # [OK] ä¿®æ­£: ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã¯ä¸­ç«‹å€¤
                continue
            
            current_kyori = group.iloc[idx]['kyori']
            
            # éå»ã®ãƒ¬ãƒ¼ã‚¹ï¼ˆÂ±200mä»¥å†…ï¼‰ã‹ã‚‰ç›´è¿‘10èµ°ã‚’å–å¾—
            past_similar = group.iloc[:idx][
                abs(group.iloc[:idx]['kyori'] - current_kyori) <= 200
            ].tail(10)
            
            if len(past_similar) > 0:
                avg_score = (1 - (past_similar['kakutei_chakujun_numeric'] / 18.0)).mean()
                scores.append(avg_score)
            else:
                scores.append(0.5)  # [OK] ä¿®æ­£: ãƒ‡ãƒ¼ã‚¿ãªã—ã¯ä¸­ç«‹å€¤
        
        return pd.Series(scores, index=group.index)
    
    df_sorted['similar_distance_score'] = df_sorted.groupby('ketto_toroku_bango', group_keys=False).apply(
        calc_similar_distance_score
    ).values
    
    # è·é›¢å¤‰åŒ–å¯¾å¿œåŠ›ï¼ˆå‰èµ°ã‹ã‚‰ã®è·é›¢å¤‰åŒ–Â±100mä»¥ä¸Šæ™‚ã®æˆç¸¾ã€ç›´è¿‘5èµ°ï¼‰
    def calc_distance_change_adaptability(group):
        """è·é›¢å¤‰åŒ–æ™‚ã®å¯¾å¿œåŠ›ã‚’è¨ˆç®—"""
        scores = []
        for idx in range(len(group)):
            if idx < 2:  # æœ€ä½2èµ°åˆ†ã®ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦
                scores.append(0.5)  # [OK] ä¿®æ­£: ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã¯ä¸­ç«‹å€¤
                continue
            
            # [OK] ä¿®æ­£: éå»6èµ°åˆ†ã‚’å–å¾—ï¼ˆå‰èµ°ã¨ã®å·®åˆ†ã‚’è¦‹ã‚‹ãŸã‚ï¼‰
            past_races = group.iloc[max(0, idx-6):idx].copy()
            
            if len(past_races) >= 3:  # [OK] ä¿®æ­£: æœ€ä½3èµ°å¿…è¦ï¼ˆå·®åˆ†2å€‹ï¼‰
                # è·é›¢ã®å¤‰åŒ–é‡ã‚’è¨ˆç®—
                past_races['kyori_diff'] = past_races['kyori'].diff().abs()
                
                # [OK] ä¿®æ­£: æœ€æ–°5èµ°ã®ã¿ã‚’è©•ä¾¡ï¼ˆæœ€åˆã®1è¡Œã¯NaNãªã®ã§é™¤å¤–ï¼‰
                past_races_eval = past_races.tail(5)
                
                # è·é›¢å¤‰åŒ–ãŒ100mä»¥ä¸Šã®ãƒ¬ãƒ¼ã‚¹ã‚’æŠ½å‡º
                changed_races = past_races_eval[past_races_eval['kyori_diff'] >= 100]
                
                if len(changed_races) > 0:
                    avg_score = (1 - (changed_races['kakutei_chakujun_numeric'] / 18.0)).mean()
                    scores.append(avg_score)
                else:
                    scores.append(0.5)  # [OK] ä¿®æ­£: å¤‰åŒ–ãªã—ã¯ä¸­ç«‹
            else:
                scores.append(0.5)  # [OK] ä¿®æ­£: ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã¯ä¸­ç«‹å€¤
        
        return pd.Series(scores, index=group.index)
    
    df_sorted['distance_change_adaptability'] = df_sorted.groupby('ketto_toroku_bango', group_keys=False).apply(
        calc_distance_change_adaptability
    ).values
    
    # çŸ­è·é›¢ç‰¹åŒ–: å‰èµ°è·é›¢å·®ã‚’è¨ˆç®—
    def calc_zenso_kyori_sa(group):
        """å‰èµ°ã‹ã‚‰ã®è·é›¢å·®ã‚’è¨ˆç®—ï¼ˆçŸ­è·é›¢ã®è·é›¢å¤‰åŒ–å½±éŸ¿ã‚’è©•ä¾¡ï¼‰"""
        diffs = []
        for idx in range(len(group)):
            if idx == 0:
                diffs.append(0)  # åˆå›ã¯å‰èµ°ãªã—
            else:
                current_kyori = group.iloc[idx]['kyori']
                previous_kyori = group.iloc[idx-1]['kyori']
                diffs.append(abs(current_kyori - previous_kyori))
        return pd.Series(diffs, index=group.index)
    
    df_sorted['zenso_kyori_sa'] = df_sorted.groupby('ketto_toroku_bango', group_keys=False).apply(
        calc_zenso_kyori_sa
    ).values
    
    # [NEW] é•·è·é›¢çµŒé¨“å›æ•°ï¼ˆ2400mä»¥ä¸Šã®ãƒ¬ãƒ¼ã‚¹çµŒé¨“æ•°ï¼‰
    def calc_long_distance_experience_count(group):
        """é•·è·é›¢(2400mä»¥ä¸Š)ã®ãƒ¬ãƒ¼ã‚¹çµŒé¨“å›æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ"""
        counts = []
        for idx in range(len(group)):
            if idx == 0:
                counts.append(0)  # åˆå›ã¯çµŒé¨“ãªã—
            else:
                # éå»ã®ãƒ¬ãƒ¼ã‚¹ã§2400mä»¥ä¸Šã‚’èµ°ã£ãŸå›æ•°
                past_long_count = (group.iloc[:idx]['kyori'] >= 2400).sum()
                counts.append(past_long_count)
        return pd.Series(counts, index=group.index)
    
    df_sorted['long_distance_experience_count'] = df_sorted.groupby('ketto_toroku_bango', group_keys=False).apply(
        calc_long_distance_experience_count
    ).values
    
    # å…ƒã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹é †ã«æˆ»ã™
    df = df.copy()
    df['distance_category_score'] = df_sorted.sort_index()['distance_category_score']
    df['similar_distance_score'] = df_sorted.sort_index()['similar_distance_score']
    df['distance_change_adaptability'] = df_sorted.sort_index()['distance_change_adaptability']
    df['zenso_kyori_sa'] = df_sorted.sort_index()['zenso_kyori_sa']
    df['long_distance_experience_count'] = df_sorted.sort_index()['long_distance_experience_count']
    
    # ç‰¹å¾´é‡ã«è¿½åŠ 
    # X['distance_category_score'] = df['distance_category_score']
    X['similar_distance_score'] = df['similar_distance_score']
    # X['distance_change_adaptability'] = df['distance_change_adaptability']
    X['zenso_kyori_sa'] = df['zenso_kyori_sa']
    X['long_distance_experience_count'] = df['long_distance_experience_count']
    
    print(f"[OK] è·é›¢é©æ€§ã‚¹ã‚³ã‚¢ã‚’4ç¨®é¡ + çŸ­è·é›¢ç‰¹åŒ–2ç¨®é¡ è¿½åŠ ã—ã¾ã—ãŸï¼")
    print(f"  - distance_category_score: è·é›¢ã‚«ãƒ†ã‚´ãƒªåˆ¥é©æ€§ï¼ˆç›´è¿‘5èµ°ï¼‰")
    print(f"  - similar_distance_score: è¿‘ä¼¼è·é›¢ã§ã®æˆç¸¾ï¼ˆÂ±200mã€ç›´è¿‘10èµ°ï¼‰")
    print(f"  - zenso_kyori_sa: å‰èµ°ã‹ã‚‰ã®è·é›¢å·®ï¼ˆçŸ­è·é›¢é©æ€§è©•ä¾¡ï¼‰")
    print(f"  - long_distance_experience_count: é•·è·é›¢çµŒé¨“å›æ•°ï¼ˆ>=2400mï¼‰[NEW]")
    print(f"  - wakuban_kyori_interaction: æ ç•ªÃ—è·é›¢ç›¸äº’ä½œç”¨ï¼ˆçŸ­è·é›¢å†…æ æœ‰åˆ©ï¼‰")
    print(f"  - distance_change_adaptability: è·é›¢å¤‰åŒ–å¯¾å¿œåŠ›ï¼ˆÂ±100mä»¥ä¸Šã€ç›´è¿‘5èµ°ï¼‰")
    
    # æ–°æ©Ÿèƒ½: ã‚¹ã‚¿ãƒ¼ãƒˆæŒ‡æ•°ã‚’è¿½åŠ ï¼ˆç¬¬1ã‚³ãƒ¼ãƒŠãƒ¼é€šéé †ä½ã‹ã‚‰ç®—å‡ºï¼‰
    if 'corner_1' in df.columns:
        print("[DONE] ã‚¹ã‚¿ãƒ¼ãƒˆæŒ‡æ•°ã‚’è¨ˆç®—ä¸­...")
        
        def calc_start_index(group):
            """
            éå»10èµ°ã®ç¬¬1ã‚³ãƒ¼ãƒŠãƒ¼é€šéé †ä½ã‹ã‚‰ã‚¹ã‚¿ãƒ¼ãƒˆèƒ½åŠ›ã‚’è©•ä¾¡
            - æ—©æœŸä½ç½®å–ã‚Šèƒ½åŠ›ï¼ˆé€šéé †ä½ãŒè‰¯ã„ = ã‚¹ã‚¿ãƒ¼ãƒˆè‰¯å¥½ï¼‰
            - ä¸€è²«æ€§ï¼ˆæ¨™æº–åå·®ãŒå°ã•ã„ = ã‚¹ã‚¿ãƒ¼ãƒˆå®‰å®šï¼‰
            """
            scores = []
            for idx in range(len(group)):
                if idx == 0:
                    scores.append(0.5)  # åˆå›ã¯ä¸­ç«‹å€¤
                    continue
                
                # éå»10èµ°ã®ç¬¬1ã‚³ãƒ¼ãƒŠãƒ¼é€šéé †ä½ã‚’å–å¾—ï¼ˆcorner_1ã¯æ—¢ã«æ•°å€¤åŒ–æ¸ˆã¿ï¼‰
                past_corners = group.iloc[max(0, idx-10):idx]['corner_1'].dropna()
                
                if len(past_corners) >= 3:  # æœ€ä½3èµ°å¿…è¦
                    avg_position = past_corners.mean()
                    std_position = past_corners.std()
                    
                    # ã‚¹ã‚³ã‚¢è¨ˆç®—: 
                    # 1. é€šéé †ä½ãŒè‰¯ã„ï¼ˆå°ã•ã„ï¼‰ã»ã©é«˜ã‚¹ã‚³ã‚¢ â†’ 1.0 - (avg_position / 18)
                    # 2. å®‰å®šæ€§ãƒœãƒ¼ãƒŠã‚¹: std ãŒå°ã•ã„ã»ã©é«˜è©•ä¾¡ â†’ æœ€å¤§0.2ã®ãƒœãƒ¼ãƒŠã‚¹
                    position_score = max(0, 1.0 - (avg_position / 18.0))
                    stability_bonus = max(0, 0.2 - (std_position / 10.0))
                    
                    total_score = position_score + stability_bonus
                    scores.append(min(1.0, total_score))  # æœ€å¤§1.0ã«ã‚¯ãƒªãƒƒãƒ—
                else:
                    scores.append(0.5)  # ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã¯ä¸­ç«‹å€¤
            
            return pd.Series(scores, index=group.index)
        
        df_sorted['start_index'] = df_sorted.groupby('ketto_toroku_bango', group_keys=False).apply(
            calc_start_index
        ).values
        
        df['start_index'] = df_sorted.sort_index()['start_index']
        X['start_index'] = df['start_index']
        
        print(f"[OK] ã‚¹ã‚¿ãƒ¼ãƒˆæŒ‡æ•°ã‚’è¿½åŠ ã—ã¾ã—ãŸï¼")
        print(f"  - start_index: éå»10èµ°ã®ç¬¬1ã‚³ãƒ¼ãƒŠãƒ¼é€šéé †ä½ã‹ã‚‰ç®—å‡ºï¼ˆæ—©æœŸä½ç½®å–ã‚Šèƒ½åŠ›+å®‰å®šæ€§ï¼‰")
    else:
        print("[!]  corner_1ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ãªã„ãŸã‚ã€ã‚¹ã‚¿ãƒ¼ãƒˆæŒ‡æ•°ã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
        # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã§0.5ï¼ˆä¸­ç«‹å€¤ï¼‰ã‚’è¨­å®š
        df['start_index'] = 0.5
        X['start_index'] = 0.5
    
    # çŸ­è·é›¢ç‰¹åŒ–: ã‚³ãƒ¼ãƒŠãƒ¼é€šéä½ç½®ã‚¹ã‚³ã‚¢ï¼ˆå…¨ã‚³ãƒ¼ãƒŠãƒ¼ã®å¹³å‡ï¼‰
    if all(col in df.columns for col in ['corner_1', 'corner_2', 'corner_3', 'corner_4']):
        print("[DONE] ã‚³ãƒ¼ãƒŠãƒ¼é€šéä½ç½®ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ä¸­...")
        
        def calc_corner_position_score(group):
            """
            éå»3èµ°ã®å…¨ã‚³ãƒ¼ãƒŠãƒ¼(1-4)é€šéä½ç½®ã®å¹³å‡ã¨å®‰å®šæ€§ã‚’è¨ˆç®—
            - ä½ç½®å–ã‚ŠãŒè‰¯ã„(æ•°å€¤ãŒå°ã•ã„)ã»ã©é«˜ã‚¹ã‚³ã‚¢
            - å®‰å®šæ€§ã‚‚è©•ä¾¡ â†’ é¦¬é€£ãƒ»ãƒ¯ã‚¤ãƒ‰ã®ç²¾åº¦å‘ä¸Š
            """
            scores = []
            for idx in range(len(group)):
                if idx < 1:  # æœ€ä½1èµ°åˆ†ã®ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦
                    scores.append(0.5)
                    continue
                
                # éå»3èµ°ã‚’å–å¾—
                past_3_races = group.iloc[max(0, idx-2):idx+1]
                
                if len(past_3_races) >= 1:
                    # å„ãƒ¬ãƒ¼ã‚¹ã®å…¨ã‚³ãƒ¼ãƒŠãƒ¼å¹³å‡ä½ç½®ã‚’è¨ˆç®—
                    corner_averages = []
                    for _, race in past_3_races.iterrows():
                        corners = []
                        for corner_col in ['corner_1', 'corner_2', 'corner_3', 'corner_4']:
                            corner_val = race[corner_col]
                            if pd.notna(corner_val) and corner_val > 0:
                                corners.append(corner_val)
                        if len(corners) > 0:
                            corner_averages.append(np.mean(corners))
                    
                    if len(corner_averages) > 0:
                        avg_position = np.mean(corner_averages)
                        std_position = np.std(corner_averages) if len(corner_averages) > 1 else 0
                        
                        # ã‚¹ã‚³ã‚¢è¨ˆç®—:
                        # 1. ä½ç½®å–ã‚Šã‚¹ã‚³ã‚¢: å‰æ–¹ã»ã©é«˜è©•ä¾¡
                        position_score = max(0, 1.0 - (avg_position / 18.0))
                        
                        # 2. å®‰å®šæ€§ãƒœãƒ¼ãƒŠã‚¹: stdãŒå°ã•ã„ã»ã©é«˜è©•ä¾¡ (æœ€å¤§+0.3)
                        stability_bonus = max(0, 0.3 - (std_position / 10.0))
                        
                        # åˆè¨ˆã‚¹ã‚³ã‚¢ (æœ€å¤§1.0ã«ã‚¯ãƒªãƒƒãƒ—)
                        total_score = position_score + stability_bonus
                        scores.append(min(1.0, total_score))
                    else:
                        scores.append(0.5)
                else:
                    scores.append(0.5)
            
            return pd.Series(scores, index=group.index)
        
        df_sorted['corner_position_score'] = df_sorted.groupby('ketto_toroku_bango', group_keys=False).apply(
            calc_corner_position_score
        ).values
        
        df['corner_position_score'] = df_sorted.sort_index()['corner_position_score']
        X['corner_position_score'] = df['corner_position_score']
        
        print(f"[OK] ã‚³ãƒ¼ãƒŠãƒ¼é€šéä½ç½®ã‚¹ã‚³ã‚¢ã‚’è¿½åŠ ã—ã¾ã—ãŸï¼")
        print(f"  - corner_position_score: éå»3èµ°ã®å…¨ã‚³ãƒ¼ãƒŠãƒ¼(1-4)é€šéä½ç½®å¹³å‡+å®‰å®šæ€§ï¼ˆãƒã‚¸ã‚·ãƒ§ãƒ‹ãƒ³ã‚°èƒ½åŠ›+å®‰å®šæ€§ï¼‰")
    else:
        print("[!]  corner_2~4ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ãªã„ãŸã‚ã€ã‚³ãƒ¼ãƒŠãƒ¼é€šéä½ç½®ã‚¹ã‚³ã‚¢ã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
        df['corner_position_score'] = 0.5
        X['corner_position_score'] = 0.5
    
    # æ–°æ©Ÿèƒ½: é¦¬å ´é©æ€§ã‚¹ã‚³ã‚¢ã‚’è¿½åŠ ï¼ˆ3ç¨®é¡ï¼‰
    print("é¦¬å ´é©æ€§ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ä¸­...")
    
    # é¦¬å ´æƒ…å ±ã¯æ—¢ã«df_sortedã«å«ã¾ã‚Œã¦ã„ã‚‹ã®ã§ã€ãã®ã¾ã¾ä½¿ç”¨
    # 1ï¸âƒ£ èŠ/ãƒ€ãƒ¼ãƒˆåˆ¥é©æ€§ã‚¹ã‚³ã‚¢ï¼ˆç›´è¿‘10èµ°ï¼‰
    def calc_surface_score(group):
        """å„é¦¬ã®èŠ/ãƒ€ãƒ¼ãƒˆåˆ¥é©æ€§ã‚’è¨ˆç®—"""
        scores = []
        for idx in range(len(group)):
            if idx == 0:
                scores.append(0.5)  # [OK] ä¿®æ­£: ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã¯ä¸­ç«‹å€¤
                continue
            
            current_surface = group.iloc[idx]['surface_type']
            
            # åŒã˜é¦¬å ´ã‚¿ã‚¤ãƒ—ã§ã®éå»æˆç¸¾ï¼ˆç›´è¿‘10èµ°ï¼‰
            past_same_surface = group.iloc[:idx][
                group.iloc[:idx]['surface_type'] == current_surface
            ].tail(10)
            
            if len(past_same_surface) > 0:
                avg_score = (1 - (past_same_surface['kakutei_chakujun_numeric'] / 18.0)).mean()
                scores.append(avg_score)
            else:
                scores.append(0.5)  # [OK] ä¿®æ­£: ãƒ‡ãƒ¼ã‚¿ãªã—ã¯ä¸­ç«‹å€¤
        
        return pd.Series(scores, index=group.index)
    
    df_sorted['surface_aptitude_score'] = df_sorted.groupby('ketto_toroku_bango', group_keys=False).apply(
        calc_surface_score
    ).values
    
    # 2ï¸âƒ£ é¦¬å ´çŠ¶æ…‹åˆ¥é©æ€§ã‚¹ã‚³ã‚¢ï¼ˆè‰¯/ç¨é‡/é‡/ä¸è‰¯ã€ç›´è¿‘10èµ°ï¼‰
    def calc_baba_condition_score(group):
        """å„é¦¬ã®é¦¬å ´çŠ¶æ…‹åˆ¥é©æ€§ã‚’è¨ˆç®—"""
        scores = []
        for idx in range(len(group)):
            if idx == 0:
                scores.append(0.5)  # [OK] ä¿®æ­£: ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã¯ä¸­ç«‹å€¤
                continue
            
            current_condition = group.iloc[idx]['baba_condition']
            
            # åŒã˜é¦¬å ´çŠ¶æ…‹ã§ã®éå»æˆç¸¾ï¼ˆç›´è¿‘10èµ°ï¼‰
            past_same_condition = group.iloc[:idx][
                group.iloc[:idx]['baba_condition'] == current_condition
            ].tail(10)
            
            if len(past_same_condition) > 0:
                avg_score = (1 - (past_same_condition['kakutei_chakujun_numeric'] / 18.0)).mean()
                scores.append(avg_score)
            else:
                scores.append(0.5)  # [OK] ä¿®æ­£: ãƒ‡ãƒ¼ã‚¿ãªã—ã¯ä¸­ç«‹å€¤
        
        return pd.Series(scores, index=group.index)
    
    df_sorted['baba_condition_score'] = df_sorted.groupby('ketto_toroku_bango', group_keys=False).apply(
        calc_baba_condition_score
    ).values
    
    # 3ï¸âƒ£ é¦¬å ´å¤‰åŒ–å¯¾å¿œåŠ›ï¼ˆå‰èµ°ã¨ç•°ãªã‚‹é¦¬å ´çŠ¶æ…‹ã§ã®æˆç¸¾ã€ç›´è¿‘5èµ°ï¼‰
    def calc_baba_change_adaptability(group):
        """é¦¬å ´çŠ¶æ…‹å¤‰åŒ–æ™‚ã®å¯¾å¿œåŠ›ã‚’è¨ˆç®—"""
        scores = []
        for idx in range(len(group)):
            if idx < 2:  # æœ€ä½2èµ°åˆ†ã®ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦
                scores.append(0.5)  # [OK] ä¿®æ­£: ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã¯ä¸­ç«‹å€¤
                continue
            
            # [OK] ä¿®æ­£: éå»6èµ°åˆ†ã‚’å–å¾—ï¼ˆå‰èµ°ã¨ã®å¤‰åŒ–ã‚’è¦‹ã‚‹ãŸã‚ï¼‰
            past_races = group.iloc[max(0, idx-6):idx].copy()
            
            if len(past_races) >= 3:  # [OK] ä¿®æ­£: æœ€ä½3èµ°å¿…è¦
                # é¦¬å ´çŠ¶æ…‹ã®å¤‰åŒ–ã‚’æ¤œå‡ºï¼ˆå‰èµ°ã¨ç•°ãªã‚‹é¦¬å ´çŠ¶æ…‹ï¼‰
                past_races['baba_changed'] = past_races['baba_condition'].shift(1) != past_races['baba_condition']
                
                # [OK] ä¿®æ­£: æœ€æ–°5èµ°ã®ã¿ã‚’è©•ä¾¡
                past_races_eval = past_races.tail(5)
                
                # é¦¬å ´çŠ¶æ…‹ãŒå¤‰åŒ–ã—ãŸãƒ¬ãƒ¼ã‚¹ã®ã¿æŠ½å‡º
                changed_races = past_races_eval[past_races_eval['baba_changed'] == True]
                
                if len(changed_races) > 0:
                    avg_score = (1 - (changed_races['kakutei_chakujun_numeric'] / 18.0)).mean()
                    scores.append(avg_score)
                else:
                    scores.append(0.5)  # [OK] ä¿®æ­£: å¤‰åŒ–ãªã—ã¯ä¸­ç«‹
            else:
                scores.append(0.5)  # [OK] ä¿®æ­£: ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã¯ä¸­ç«‹å€¤
        
        return pd.Series(scores, index=group.index)
    
    df_sorted['baba_change_adaptability'] = df_sorted.groupby('ketto_toroku_bango', group_keys=False).apply(
        calc_baba_change_adaptability
    ).values
    
    # å…ƒã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹é †ã«æˆ»ã™
    df['surface_aptitude_score'] = df_sorted.sort_index()['surface_aptitude_score']
    df['baba_condition_score'] = df_sorted.sort_index()['baba_condition_score']
    df['baba_change_adaptability'] = df_sorted.sort_index()['baba_change_adaptability']
    
    # ç‰¹å¾´é‡ã«è¿½åŠ 
    X['surface_aptitude_score'] = df['surface_aptitude_score']
    # X['baba_condition_score'] = df['baba_condition_score']
    X['baba_change_adaptability'] = df['baba_change_adaptability']
    
    print(f"[OK] é¦¬å ´é©æ€§ã‚¹ã‚³ã‚¢ã‚’3ç¨®é¡è¿½åŠ ã—ã¾ã—ãŸï¼")
    print(f"  - surface_aptitude_score: èŠ/ãƒ€ãƒ¼ãƒˆåˆ¥é©æ€§ï¼ˆç›´è¿‘10èµ°ï¼‰")
    print(f"  - baba_condition_score: é¦¬å ´çŠ¶æ…‹åˆ¥é©æ€§ï¼ˆè‰¯/ç¨é‡/é‡/ä¸è‰¯ã€ç›´è¿‘10èµ°ï¼‰")
    print(f"  - baba_change_adaptability: é¦¬å ´å¤‰åŒ–å¯¾å¿œåŠ›ï¼ˆç›´è¿‘5èµ°ï¼‰")

    # æ–°æ©Ÿèƒ½: é¨æ‰‹ãƒ»èª¿æ•™å¸«ã®å‹•çš„èƒ½åŠ›ã‚¹ã‚³ã‚¢ã‚’è¿½åŠ ï¼ˆ4ç¨®é¡ï¼‰
    print("[RACE] é¨æ‰‹ãƒ»èª¿æ•™å¸«ã®èƒ½åŠ›ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ä¸­...")
    
    # [OK] ä¿®æ­£: race_bangoã‚’è¿½åŠ ã—ã¦æ™‚ç³»åˆ—ãƒªãƒ¼ã‚¯ã‚’é˜²æ­¢
    df_sorted_kishu = df.sort_values(['kishu_code', 'kaisai_nen', 'kaisai_tsukihi', 'race_bango']).copy()
    
    # 1ï¸âƒ£ é¨æ‰‹ã®å®ŸåŠ›è£œæ­£ã‚¹ã‚³ã‚¢ï¼ˆæœŸå¾…ç€é †ã¨ã®å·®åˆ†ã€ç›´è¿‘3ãƒ¶æœˆï¼‰
    def calc_kishu_skill_adjusted_score(group):
        """é¨æ‰‹ã®ç´”ç²‹ãªæŠ€è¡“ã‚’è©•ä¾¡ï¼ˆé¦¬ã®å®ŸåŠ›ã‚’è£œæ­£ï¼‰"""
        scores = []
        
        for idx in range(len(group)):
            # é¨æ‰‹ã‚³ãƒ¼ãƒ‰ãŒãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            if pd.isna(group.iloc[idx]['kishu_code']) or group.iloc[idx]['kishu_code'] == '':
                scores.append(0.5)
                continue
                
            current_date = pd.to_datetime(
                str(int(group.iloc[idx]['kaisai_nen'])) + str(int(group.iloc[idx]['kaisai_tsukihi'])).zfill(4),
                format='%Y%m%d'
            )
            
            # 3ãƒ¶æœˆå‰ã®æ—¥ä»˜
            three_months_ago = current_date - pd.DateOffset(months=3)
            
            # éå»3ãƒ¶æœˆã®ãƒ¬ãƒ¼ã‚¹ã‚’æŠ½å‡ºï¼ˆæœªæ¥ã®ãƒ‡ãƒ¼ã‚¿ã¯è¦‹ãªã„ï¼ï¼‰
            past_races = group.iloc[:idx]
            
            if len(past_races) > 0:
                past_races = past_races.copy()
                past_races['kaisai_date'] = pd.to_datetime(
                    past_races['kaisai_nen'].astype(str) + past_races['kaisai_tsukihi'].astype(str).str.zfill(4),
                    format='%Y%m%d'
                )
                recent_races = past_races[past_races['kaisai_date'] >= three_months_ago]
                
                if len(recent_races) >= 3:  # æœ€ä½3ãƒ¬ãƒ¼ã‚¹å¿…è¦
                    # [OK] ä¿®æ­£: é¨æ‰‹ã®ç´”ç²‹ãªæˆç¸¾ã‚’è©•ä¾¡ï¼ˆé¦¬ã®å®ŸåŠ›è£œæ­£ã§ã¯ãªãã€é¨æ‰‹ã®å¹³å‡æˆç¸¾ï¼‰
                    # ç€é †ã‚’ã‚¹ã‚³ã‚¢åŒ–ï¼ˆ1ç€=1.0, 18ç€=0.0ï¼‰
                    recent_races['rank_score'] = 1.0 - ((18 - recent_races['kakutei_chakujun_numeric'] + 1) / 18.0)
                    
                    # é¨æ‰‹ã®å¹³å‡ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
                    avg_score = recent_races['rank_score'].mean()
                    
                    # 0-1ã®ç¯„å›²ã«ã‚¯ãƒªãƒƒãƒ—ï¼ˆæ—¢ã«ç¯„å›²å†…ã ãŒå¿µã®ãŸã‚ï¼‰
                    normalized_score = max(0.0, min(1.0, avg_score))
                    
                    scores.append(normalized_score)
                else:
                    scores.append(0.5)  # ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã¯ä¸­ç«‹
            else:
                scores.append(0.5)  # åˆå›ã¯ä¸­ç«‹
        
        return pd.Series(scores, index=group.index)
    
    df_sorted_kishu['kishu_skill_score'] = df_sorted_kishu.groupby('kishu_code', group_keys=False).apply(
        calc_kishu_skill_adjusted_score
    ).values
    
    # 2ï¸âƒ£ é¨æ‰‹ã®äººæ°—å·®ã‚¹ã‚³ã‚¢ï¼ˆã‚ªãƒƒã‚ºè£œæ­£ã€ç›´è¿‘3ãƒ¶æœˆï¼‰
    def calc_kishu_popularity_adjusted_score(group):
        """é¨æ‰‹ã®äººæ°—è£œæ­£ã‚¹ã‚³ã‚¢ï¼ˆäººæ°—ã‚ˆã‚Šä¸Šä½ã«æ¥ã‚Œã‚‹ã‹ï¼‰"""
        scores = []
        
        for idx in range(len(group)):
            if pd.isna(group.iloc[idx]['kishu_code']) or group.iloc[idx]['kishu_code'] == '':
                scores.append(0.5)
                continue
                
            current_date = pd.to_datetime(
                str(int(group.iloc[idx]['kaisai_nen'])) + str(int(group.iloc[idx]['kaisai_tsukihi'])).zfill(4),
                format='%Y%m%d'
            )
            
            three_months_ago = current_date - pd.DateOffset(months=3)
            
            past_races = group.iloc[:idx]
            
            if len(past_races) > 0:
                past_races = past_races.copy()
                past_races['kaisai_date'] = pd.to_datetime(
                    past_races['kaisai_nen'].astype(str) + past_races['kaisai_tsukihi'].astype(str).str.zfill(4),
                    format='%Y%m%d'
                )
                recent_races = past_races[past_races['kaisai_date'] >= three_months_ago]
                
                if len(recent_races) >= 3:
                    # ã‚ªãƒƒã‚ºãŒ0ã‚„ç•°å¸¸å€¤ã®å ´åˆã‚’é™¤å¤–
                    valid_races = recent_races[recent_races['tansho_odds'] > 0]
                    
                    if len(valid_races) >= 3:
                        # [OK] ä¿®æ­£: ã‚ªãƒƒã‚ºãƒ™ãƒ¼ã‚¹ã®æœŸå¾…æˆç¸¾ã¨å®Ÿéš›ã®æˆç¸¾ã‚’æ¯”è¼ƒ
                        # ã‚ªãƒƒã‚ºãŒä½ã„ = æœŸå¾…å€¤ãŒé«˜ã„ï¼ˆ1ã«è¿‘ã„ï¼‰
                        # ã‚ªãƒƒã‚ºãŒé«˜ã„ = æœŸå¾…å€¤ãŒä½ã„ï¼ˆ0ã«è¿‘ã„ï¼‰
                        max_odds = valid_races['tansho_odds'].max()
                        valid_races['odds_expectation'] = 1.0 - (valid_races['tansho_odds'] / (max_odds + 1.0))
                        
                        # å®Ÿéš›ã®æˆç¸¾ã‚¹ã‚³ã‚¢
                        valid_races['actual_score'] = 1.0 - ((18 - valid_races['kakutei_chakujun_numeric'] + 1) / 18.0)
                        
                        # æœŸå¾…ã‚’ä¸Šå›ã£ãŸåº¦åˆã„ï¼ˆãƒ—ãƒ©ã‚¹ãªã‚‰æœŸå¾…ä»¥ä¸Šï¼‰
                        valid_races['performance_diff'] = valid_races['actual_score'] - valid_races['odds_expectation']
                        
                        # å¹³å‡å·®åˆ†ã‚’ã‚¹ã‚³ã‚¢åŒ–ï¼ˆ0.5ãŒä¸­ç«‹ï¼‰
                        avg_diff = valid_races['performance_diff'].mean()
                        normalized_score = 0.5 + (avg_diff * 0.5)  # Â±0.5ã®ç¯„å›²ã«åã‚ã‚‹
                        normalized_score = max(0.0, min(1.0, normalized_score))
                        
                        scores.append(normalized_score)
                    else:
                        scores.append(0.5)
                else:
                    scores.append(0.5)
            else:
                scores.append(0.5)
        
        return pd.Series(scores, index=group.index)
    
    df_sorted_kishu['kishu_popularity_score'] = df_sorted_kishu.groupby('kishu_code', group_keys=False).apply(
        calc_kishu_popularity_adjusted_score
    ).values
    
    # 3ï¸âƒ£ é¨æ‰‹ã®èŠ/ãƒ€ãƒ¼ãƒˆåˆ¥ã‚¹ã‚³ã‚¢ï¼ˆé¦¬å ´é©æ€§è€ƒæ…®ã€ç›´è¿‘6ãƒ¶æœˆï¼‰
    def calc_kishu_surface_score(group):
        """é¨æ‰‹ã®é¦¬å ´ã‚¿ã‚¤ãƒ—åˆ¥ç›´è¿‘6ãƒ¶æœˆæˆç¸¾"""
        scores = []
        
        for idx in range(len(group)):
            if pd.isna(group.iloc[idx]['kishu_code']) or group.iloc[idx]['kishu_code'] == '':
                scores.append(0.5)
                continue
                
            current_date = pd.to_datetime(
                str(int(group.iloc[idx]['kaisai_nen'])) + str(int(group.iloc[idx]['kaisai_tsukihi'])).zfill(4),
                format='%Y%m%d'
            )
            current_surface = group.iloc[idx]['surface_type']
            
            six_months_ago = current_date - pd.DateOffset(months=6)
            
            past_races = group.iloc[:idx]
            
            if len(past_races) > 0:
                past_races = past_races.copy()
                past_races['kaisai_date'] = pd.to_datetime(
                    past_races['kaisai_nen'].astype(str) + past_races['kaisai_tsukihi'].astype(str).str.zfill(4),
                    format='%Y%m%d'
                )
                # åŒã˜é¦¬å ´ã‚¿ã‚¤ãƒ—ã§ã®ç›´è¿‘6ãƒ¶æœˆ
                recent_same_surface = past_races[
                    (past_races['kaisai_date'] >= six_months_ago) &
                    (past_races['surface_type'] == current_surface)
                ]
                
                if len(recent_same_surface) >= 5:  # æœ€ä½5ãƒ¬ãƒ¼ã‚¹å¿…è¦
                    avg_score = (1 - ((18 - recent_same_surface['kakutei_chakujun_numeric'] + 1) / 18.0)).mean()
                    scores.append(avg_score)
                else:
                    scores.append(0.5)
            else:
                scores.append(0.5)
        
        return pd.Series(scores, index=group.index)
    
    df_sorted_kishu['kishu_surface_score'] = df_sorted_kishu.groupby('kishu_code', group_keys=False).apply(
        calc_kishu_surface_score
    ).values
    
    # [OK] ä¿®æ­£: race_bangoã‚’è¿½åŠ ã—ã¦æ™‚ç³»åˆ—ãƒªãƒ¼ã‚¯ã‚’é˜²æ­¢
    df_sorted_chokyoshi = df.sort_values(['chokyoshi_code', 'kaisai_nen', 'kaisai_tsukihi', 'race_bango']).copy()
    
    # 4ï¸âƒ£ èª¿æ•™å¸«ã®ç›´è¿‘3ãƒ¶æœˆæˆç¸¾ã‚¹ã‚³ã‚¢
    def calc_chokyoshi_recent_score(group):
        """èª¿æ•™å¸«ã®ç›´è¿‘3ãƒ¶æœˆæˆç¸¾"""
        scores = []
        
        for idx in range(len(group)):
            if pd.isna(group.iloc[idx]['chokyoshi_code']) or group.iloc[idx]['chokyoshi_code'] == '':
                scores.append(0.5)
                continue
                
            current_date = pd.to_datetime(
                str(int(group.iloc[idx]['kaisai_nen'])) + str(int(group.iloc[idx]['kaisai_tsukihi'])).zfill(4),
                format='%Y%m%d'
            )
            
            three_months_ago = current_date - pd.DateOffset(months=3)
            
            past_races = group.iloc[:idx]
            
            if len(past_races) > 0:
                past_races = past_races.copy()
                past_races['kaisai_date'] = pd.to_datetime(
                    past_races['kaisai_nen'].astype(str) + past_races['kaisai_tsukihi'].astype(str).str.zfill(4),
                    format='%Y%m%d'
                )
                recent_races = past_races[past_races['kaisai_date'] >= three_months_ago]
                
                if len(recent_races) >= 5:  # [OK] ä¿®æ­£: 5ãƒ¬ãƒ¼ã‚¹ã«å¤‰æ›´ï¼ˆ10ãƒ¬ãƒ¼ã‚¹ã§ã¯å¤§éƒ¨åˆ†ãŒä¸­ç«‹å€¤ã«ãªã‚‹ï¼‰
                    avg_score = (1 - ((18 - recent_races['kakutei_chakujun_numeric'] + 1) / 18.0)).mean()
                    scores.append(avg_score)
                else:
                    scores.append(0.5)
            else:
                scores.append(0.5)
        
        return pd.Series(scores, index=group.index)
    
    df_sorted_chokyoshi['chokyoshi_recent_score'] = df_sorted_chokyoshi.groupby('chokyoshi_code', group_keys=False).apply(
        calc_chokyoshi_recent_score
    ).values
    
    # å…ƒã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹é †ã«æˆ»ã™
    df['kishu_skill_score'] = df_sorted_kishu.sort_index()['kishu_skill_score']
    df['kishu_popularity_score'] = df_sorted_kishu.sort_index()['kishu_popularity_score']
    df['kishu_surface_score'] = df_sorted_kishu.sort_index()['kishu_surface_score']
    # df['chokyoshi_recent_score'] = df_sorted_chokyoshi.sort_index()['chokyoshi_recent_score']
    
    # ç‰¹å¾´é‡ã«è¿½åŠ 
    X['kishu_skill_score'] = df['kishu_skill_score']
    X['kishu_popularity_score'] = df['kishu_popularity_score']
    X['kishu_surface_score'] = df['kishu_surface_score']
    # X['chokyoshi_recent_score'] = df['chokyoshi_recent_score']
    
    print(f"[OK] é¨æ‰‹ãƒ»èª¿æ•™å¸«ã‚¹ã‚³ã‚¢ã‚’4ç¨®é¡è¿½åŠ ã—ã¾ã—ãŸï¼")
    print(f"  - kishu_skill_score: é¨æ‰‹ã®å®ŸåŠ›è£œæ­£ã‚¹ã‚³ã‚¢ï¼ˆé¦¬ã®å®ŸåŠ›ã‚’è€ƒæ…®ã€ç›´è¿‘3ãƒ¶æœˆï¼‰")
    print(f"  - kishu_popularity_score: é¨æ‰‹ã®äººæ°—å·®ã‚¹ã‚³ã‚¢ï¼ˆã‚ªãƒƒã‚ºè£œæ­£ã€ç›´è¿‘3ãƒ¶æœˆï¼‰")
    print(f"  - kishu_surface_score: é¨æ‰‹ã®èŠ/ãƒ€ãƒ¼ãƒˆåˆ¥é©æ€§ï¼ˆç›´è¿‘6ãƒ¶æœˆï¼‰")
    print(f"  - chokyoshi_recent_score: èª¿æ•™å¸«ã®ç›´è¿‘æˆç¸¾ï¼ˆç›´è¿‘3ãƒ¶æœˆï¼‰")

    # éå»ãƒ¬ãƒ¼ã‚¹ã§ã€Œäººæ°—è–„ãªã®ã«å¥½èµ°ã—ãŸå›æ•°ã€
    # df['upset_count'] = df.groupby('ketto_toroku_bango').apply(
    #     lambda g: ((g['tansho_ninkijun_numeric'] >= 5) & (g['kakutei_chakujun_numeric'] <= 3)).sum()
    # )
    # X['upset_count'] = df['upset_count']

    # # ç ”ç©¶ç”¨ç‰¹å¾´é‡ è¿½åŠ 
    
    # ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã‚’ä½œæˆ
    # X['kyori'] = X['kyori'].astype('category')
    # X['tenko_code'] = X['tenko_code'].astype('category')
    # X['babajotai_code'] = X['babajotai_code'].astype('category')
    # X['seibetsu_code'] = X['seibetsu_code'].astype('category')
    categorical_features = []

    # [TARGET] è·¯é¢Ã—è·é›¢åˆ¥ç‰¹å¾´é‡é¸æŠï¼ˆSHAPåˆ†æçµæœã«åŸºã¥ãæœ€é©åŒ–ï¼‰
    print(f"\n[RACE] è·¯é¢Ã—è·é›¢åˆ¥ç‰¹å¾´é‡é¸æŠã‚’å®Ÿæ–½...")
    print(f"  è·¯é¢: {surface_type}, è·é›¢: {min_distance}m ã€œ {max_distance}m")
    
    # è·¯é¢ã¨è·é›¢ã®çµ„ã¿åˆã‚ã›ã§ç‰¹å¾´é‡ã‚’èª¿æ•´
    is_turf = surface_type.lower() == 'turf'
    is_short = max_distance <= 1600
    is_long = min_distance >= 1700
    
    # çŸ­è·é›¢å°‚ç”¨ç‰¹å¾´é‡ã®è¿½åŠ 
    if is_short:
        print(f"  [TARGET] çŸ­è·é›¢ãƒ¢ãƒ‡ãƒ«: çŸ­è·é›¢ç‰¹åŒ–ç‰¹å¾´é‡ã‚’è¿½åŠ ")
        # wakuban_kyori_interaction, zenso_kyori_sa, start_index, corner_position_scoreã¯æ—¢ã«dfã¨Xã«è¿½åŠ æ¸ˆã¿
        # çŸ­è·é›¢ãƒ¢ãƒ‡ãƒ«ã§ã®ã¿ä½¿ç”¨ã™ã‚‹ãŸã‚ã€é•·è·é›¢ã§ã¯å‰Šé™¤ã™ã‚‹
        features_added_short = ['wakuban_kyori_interaction', 'zenso_kyori_sa', 'start_index', 'corner_position_score']
        print(f"    [OK] çŸ­è·é›¢ç‰¹åŒ–ç‰¹å¾´é‡: {features_added_short}")
        # é•·è·é›¢ç‰¹åŒ–ç‰¹å¾´é‡ã¯çŸ­è·é›¢ã§ã¯ä¸è¦
        if 'long_distance_experience_count' in X.columns:
            X = X.drop(columns=['long_distance_experience_count'])
            print(f"    [OK] å‰Šé™¤ï¼ˆçŸ­è·é›¢ç”¨ï¼‰: long_distance_experience_count")
    else:
        # é•·è·é›¢ãƒ»ä¸­è·é›¢ãƒ¢ãƒ‡ãƒ«ã§ã¯çŸ­è·é›¢ç‰¹åŒ–ç‰¹å¾´é‡ã‚’å‰Šé™¤
        print(f"  ä¸­é•·è·é›¢ãƒ¢ãƒ‡ãƒ«: çŸ­è·é›¢ç‰¹åŒ–ç‰¹å¾´é‡ã‚’å‰Šé™¤")
        features_to_remove_for_long = ['wakuban_kyori_interaction', 'zenso_kyori_sa', 'start_index', 'corner_position_score']
        for feature in features_to_remove_for_long:
            if feature in X.columns:
                X = X.drop(columns=[feature])
                print(f"    [OK] å‰Šé™¤ï¼ˆé•·è·é›¢ç”¨ï¼‰: {feature}")
        # é•·è·é›¢(2200mä»¥ä¸Š)ã§ã¯long_distance_experience_countã‚’ä½¿ç”¨
        if min_distance >= 2200:
            print(f"  [TARGET] é•·è·é›¢ãƒ¢ãƒ‡ãƒ«: é•·è·é›¢ç‰¹åŒ–ç‰¹å¾´é‡ã‚’ä½¿ç”¨")
            print(f"    [OK] é•·è·é›¢ç‰¹åŒ–ç‰¹å¾´é‡: ['long_distance_experience_count']")
        else:
            # ä¸­è·é›¢ã§ã¯é•·è·é›¢ç‰¹åŒ–ç‰¹å¾´é‡ã¯ä¸è¦
            if 'long_distance_experience_count' in X.columns:
                X = X.drop(columns=['long_distance_experience_count'])
                print(f"    [OK] å‰Šé™¤ï¼ˆä¸­è·é›¢ç”¨ï¼‰: long_distance_experience_count")
    
    features_to_remove = []
    
    if is_turf and is_long:
        # ğŸŒ¿ èŠä¸­é•·è·é›¢ï¼ˆãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ï¼‰: å…¨ç‰¹å¾´é‡ã‚’ä½¿ç”¨
        print("  èŠä¸­é•·è·é›¢ï¼ˆãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ï¼‰: å…¨ç‰¹å¾´é‡ã‚’ä½¿ç”¨")
        print(f"  [OK] ã“ã‚ŒãŒæœ€ã‚‚æˆåŠŸã—ã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«ã§ã™!")
    
    elif is_turf and is_short:
        # ğŸŒ¿ èŠçŸ­è·é›¢: SHAPåˆ†æã§åŠ¹æœãŒä½ã„ç‰¹å¾´é‡ã‚’å‰Šé™¤
        print("  èŠçŸ­è·é›¢: ä¸è¦ãªç‰¹å¾´é‡ã‚’å‰Šé™¤")
        features_to_remove = [
            'kohan_3f_index',           # SHAP 0.030 â†’ å¾ŒåŠã®è„šã¯çŸ­è·é›¢ã§ã¯é‡è¦åº¦ä½ã„
            'surface_aptitude_score',   # SHAP 0.000 â†’ å®Œå…¨ã«ç„¡æ„å‘³
            'wakuban_ratio',            # SHAP 0.008 â†’ ã»ã¼ç„¡åŠ¹
        ]
    
    elif not is_turf and is_long:
        # ğŸœï¸ ãƒ€ãƒ¼ãƒˆä¸­é•·è·é›¢: èŠç‰¹æœ‰ã®ç‰¹å¾´é‡ã‚’èª¿æ•´
        print("  ãƒ€ãƒ¼ãƒˆä¸­é•·è·é›¢: èŠç‰¹æœ‰ã®ç‰¹å¾´é‡ã‚’èª¿æ•´")
        # ãƒ€ãƒ¼ãƒˆã§ã¯èŠã¨ç•°ãªã‚‹ç‰¹æ€§ãŒã‚ã‚‹ãŸã‚ã€å¿…è¦ã«å¿œã˜ã¦ç‰¹å¾´é‡ã‚’èª¿æ•´
        # ç¾æ™‚ç‚¹ã§ã¯å…¨ç‰¹å¾´é‡ã‚’ä½¿ç”¨ï¼ˆä»Šå¾Œã®åˆ†æã§èª¿æ•´å¯èƒ½ï¼‰
        pass
    
    elif not is_turf and is_short:
        # ğŸœï¸ ãƒ€ãƒ¼ãƒˆçŸ­è·é›¢: èŠçŸ­è·é›¢ã®èª¿æ•´ + ãƒ€ãƒ¼ãƒˆç‰¹æœ‰ã®èª¿æ•´
        print("  ãƒ€ãƒ¼ãƒˆçŸ­è·é›¢: èŠçŸ­è·é›¢+ãƒ€ãƒ¼ãƒˆç‰¹æœ‰ã®èª¿æ•´")
        features_to_remove = [
            'kohan_3f_index',           # çŸ­è·é›¢ã§ã¯å¾ŒåŠã®è„šã¯é‡è¦åº¦ä½ã„
            'surface_aptitude_score',   # èŠ/ãƒ€ãƒ¼ãƒˆé©æ€§ã‚¹ã‚³ã‚¢ã¯åŠ¹æœè–„
            'wakuban_ratio',            # ãƒ€ãƒ¼ãƒˆçŸ­è·é›¢ã§ã‚‚åŠ¹æœè–„ã„å¯èƒ½æ€§
        ]
    
    else:
        # ãƒã‚¤ãƒ«è·é›¢ãªã©ä¸­é–“
        print("  ä¸­é–“è·é›¢ãƒ¢ãƒ‡ãƒ«: å…¨ç‰¹å¾´é‡ã‚’ä½¿ç”¨")
    
    # ç‰¹å¾´é‡ã®å‰Šé™¤å®Ÿè¡Œ
    if features_to_remove:
        print(f"  å‰Šé™¤ã™ã‚‹ç‰¹å¾´é‡: {features_to_remove}")
        for feature in features_to_remove:
            if feature in X.columns:
                X = X.drop(columns=[feature])
                print(f"    [OK] å‰Šé™¤: {feature}")
    
    print(f"  æœ€çµ‚ç‰¹å¾´é‡æ•°: {len(X.columns)}å€‹")
    print(f"  ç‰¹å¾´é‡ãƒªã‚¹ãƒˆ: {list(X.columns)}")

    #ç›®çš„å¤‰æ•°ã‚’è¨­å®š
    y = df['kakutei_chakujun_numeric'].astype(int)

    # ãƒ¬ãƒ¼ã‚¹ã”ã¨ã®ã‚°ãƒ«ãƒ¼ãƒ—ã‚’æ­£ã—ãè¨ˆç®—
    df['group_id'] = df.groupby(['kaisai_nen', 'kaisai_tsukihi', 'race_bango']).ngroup()
    groups = df['group_id'].values
    print(f"ã‚°ãƒ«ãƒ¼ãƒ—æ•°: {len(set(groups))}")  # ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªã‚°ãƒ«ãƒ¼ãƒ—ã®æ•°
    print(f"ãƒ‡ãƒ¼ã‚¿æ•°: {len(groups)}")  # å…¨ãƒ‡ãƒ¼ã‚¿æ•°

    # ãƒ‡ãƒ¼ã‚¿ã¨ã‚°ãƒ«ãƒ¼ãƒ—æ•°ãŒä¸€è‡´ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª
    if len(groups) != len(X):
        raise ValueError(f"ãƒ‡ãƒ¼ã‚¿ä»¶æ•°({len(X)})ã¨ã‚°ãƒ«ãƒ¼ãƒ—ã®æ•°({len(groups)})ãŒä¸€è‡´ã—ã¾ã›ã‚“ï¼")

    # æ”¹å–„1: æ™‚ç³»åˆ—åˆ†å‰²ã‚’å¹´å˜ä½ã§æ˜ç¢ºåŒ–
    # å¹´å˜ä½ã§è¨“ç·´/ãƒ†ã‚¹ãƒˆã‚’åˆ†å‰²ï¼ˆã°ã‚‰ã¤ãå‰Šæ¸›ã®ãŸã‚ï¼‰
    # ä¾‹: 2013-2020å¹´ã‚’è¨“ç·´ã€2021-2022å¹´ã‚’ãƒ†ã‚¹ãƒˆ
    
    # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®å¹´ç¯„å›²ã‹ã‚‰è¨“ç·´/ãƒ†ã‚¹ãƒˆå¹´ã‚’è¨ˆç®—
    all_years = sorted(df['kaisai_nen'].unique())
    total_years = len(all_years)
    
    # 75%ã‚’è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«ï¼ˆå¹´å˜ä½ã§ï¼‰
    train_year_count = int(total_years * 0.75)
    train_years = all_years[:train_year_count]
    test_years = all_years[train_year_count:]
    
    print(f"[DATE] è¨“ç·´ãƒ‡ãƒ¼ã‚¿å¹´: {train_years} ({len(train_years)}å¹´é–“)")
    print(f"[DATE] ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿å¹´: {test_years} ({len(test_years)}å¹´é–“)")
    
    # å¹´å˜ä½ã§ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’åˆ†å‰²
    train_indices = df[df['kaisai_nen'].isin(train_years)].index
    test_indices = df[df['kaisai_nen'].isin(test_years)].index
    
    # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
    X_train = X.loc[train_indices]
    X_test = X.loc[test_indices]
    y_train = y.loc[train_indices]
    y_test = y.loc[test_indices]
    groups_train = groups[train_indices]
    groups_test = groups[test_indices]
    
    print(f"[OK] è¨“ç·´ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(X_train)}ä»¶")
    print(f"[OK] ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(X_test)}ä»¶")

    # Optunaã®objectiveé–¢æ•°
    def objective(trial):
        param = {
            'objective': 'lambdarank',
            'metric': 'ndcg',                                                              # ä¸Šä½ã®ä¸¦ã³é †ã‚’é‡è¦–
            'ndcg_eval_at': [1, 3, 5],
            'boosting_type': 'gbdt',
            'verbosity': -1,
            'random_state': 42,
            'num_leaves': trial.suggest_int('num_leaves', 20, 100),
            'max_depth': trial.suggest_int('max_depth', 3, 8),
            'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.3),
            'n_estimators': trial.suggest_int('n_estimators', 200, 1000),
            'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),
            'subsample': trial.suggest_uniform('subsample', 0.7, 1.0),
            'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),
            'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-4, 10.0),
            'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-4, 10.0),
            'subsample_freq': trial.suggest_int('subsample_freq', 1, 5)
        }

        # ä¿®æ­£: ã‚°ãƒ«ãƒ¼ãƒ—ã‚µã‚¤ã‚ºã‚’æ­£ã—ã„é †åºã§è¨ˆç®—ï¼ˆãƒ‡ãƒ¼ã‚¿ã®é †åºã‚’ä¿æŒï¼‰
        # sort=Falseã§å…ƒã®ãƒ‡ãƒ¼ã‚¿é †ã‚’ç¶­æŒã—ãªãŒã‚‰ã‚°ãƒ«ãƒ¼ãƒ—ã‚µã‚¤ã‚ºã‚’æŠ½å‡º
        train_df_with_group = pd.DataFrame({'group': groups_train}).reset_index(drop=True)
        train_group_sizes = train_df_with_group.groupby('group', sort=False).size().values
        
        test_df_with_group = pd.DataFrame({'group': groups_test}).reset_index(drop=True)
        test_group_sizes = test_df_with_group.groupby('group', sort=False).size().values
        
        dtrain = lgb.Dataset(X_train, label=y_train, group=train_group_sizes, categorical_feature=categorical_features)
        dvalid = lgb.Dataset(X_test, label=y_test, group=test_group_sizes, categorical_feature=categorical_features)

        tmp_model = lgb.train(
            param,
            dtrain,
            valid_sets=[dvalid],
            valid_names=['valid'],
            callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]
        )

        # ä¿®æ­£: ãƒ¬ãƒ¼ã‚¹ã”ã¨ã«NDCGã‚’è¨ˆç®—ã—ã¦å¹³å‡ã‚’è¿”ã™ï¼ˆæ­£ã—ã„è©•ä¾¡æ–¹æ³•ï¼‰
        preds = tmp_model.predict(X_test, num_iteration=tmp_model.best_iteration)
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ãƒ¬ãƒ¼ã‚¹ã”ã¨ã«åˆ†å‰²ã—ã¦NDCGã‚’è¨ˆç®—
        ndcg_scores = []
        start_idx = 0
        for group_size in test_group_sizes:
            end_idx = start_idx + group_size
            y_true_group = y_test.iloc[start_idx:end_idx].values
            y_pred_group = preds[start_idx:end_idx]
            
            # ãƒ¬ãƒ¼ã‚¹å†…ã«2é ­ä»¥ä¸Šã„ã‚‹å ´åˆã®ã¿NDCGè¨ˆç®—ï¼ˆ1é ­ã ã¨ã‚¨ãƒ©ãƒ¼ã«ãªã‚‹ï¼‰
            if len(y_true_group) > 1:
                # 2æ¬¡å…ƒé…åˆ—ã¨ã—ã¦æ¸¡ã™
                ndcg = ndcg_score([y_true_group], [y_pred_group], k=5)
                ndcg_scores.append(ndcg)
            
            start_idx = end_idx
        
        # å…¨ãƒ¬ãƒ¼ã‚¹ã®NDCGå¹³å‡ã‚’è¿”ã™
        return np.mean(ndcg_scores) if ndcg_scores else 0.0

    # [TIP]æ¨å¥¨: æœ¬ç•ªé‹ç”¨å‰ã«ä¸Šä½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¤‡æ•°ã‚·ãƒ¼ãƒ‰ï¼ˆrandom_state=42,43,44...ï¼‰ã§
    #        å†å­¦ç¿’ã—ã€NDCG/ROI/çš„ä¸­ç‡ã®å®‰å®šæ€§ã‚’ç¢ºèªã™ã‚‹ã“ã¨ã‚’å¼·ãæ¨å¥¨
    # TODO å°†æ¥ã®æ”¹å–„: lgb.Dataset ã® weight ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ã‚ªãƒƒã‚ºãƒ™ãƒ¼ã‚¹ã®é‡ã¿ã‚’å°å…¥ã—ã€
    #      ç©´é¦¬ï¼ˆé«˜ã‚ªãƒƒã‚ºé¦¬ï¼‰ã®äºˆæ¸¬ç²¾åº¦ã‚’å‘ä¸Šã•ã›ã‚‹ã“ã¨ã§ROIæœ€é©åŒ–ã‚’å›³ã‚‹
    
    # æ”¹å–„2: Optunaã®ã‚·ãƒ¼ãƒ‰å›ºå®šï¼ˆå†ç¾æ€§å‘ä¸Šã®ãŸã‚ï¼‰
    print("[TEST] ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ã‚’é–‹å§‹...")
    sampler = optuna.samplers.TPESampler(seed=42)  # ã‚·ãƒ¼ãƒ‰ã‚’å›ºå®š
    study = optuna.create_study(direction="maximize", sampler=sampler)
    study.optimize(objective, n_trials=50)

    print('Best trial:')
    print(study.best_trial.params)

    # æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½¿ã£ã¦å†å­¦ç¿’
    best_params = study.best_trial.params

    best_params.update({
        'objective': 'lambdarank',
        'metric': 'ndcg',
        'boosting_type': 'gbdt',
        'verbosity': 0,  # å­¦ç¿’ã®é€²æ—ã‚’è¡¨ç¤º
        'random_state': 42,
    })

    # ä¿®æ­£: ã‚°ãƒ«ãƒ¼ãƒ—ãƒ‡ãƒ¼ã‚¿ã‚’æ­£ã—ãæº–å‚™ï¼ˆãƒ‡ãƒ¼ã‚¿ã®é †åºã‚’ä¿æŒï¼‰
    # ãƒ¬ãƒ¼ã‚¹ã”ã¨ã®å‡ºèµ°é ­æ•°ã‚’è¨ˆç®—ï¼ˆsort=Falseã§å…ƒã®é †åºã‚’ç¶­æŒï¼‰
    train_df_with_group = pd.DataFrame({'group': groups_train}).reset_index(drop=True)
    train_group_sizes = train_df_with_group.groupby('group', sort=False).size().values
    
    test_df_with_group = pd.DataFrame({'group': groups_test}).reset_index(drop=True)
    test_group_sizes = test_df_with_group.groupby('group', sort=False).size().values
    
    print(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ãƒ¬ãƒ¼ã‚¹æ•°: {len(train_group_sizes)}")
    print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ãƒ¬ãƒ¼ã‚¹æ•°: {len(test_group_sizes)}")
    
    # LightGBMç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ
    dtrain = lgb.Dataset(X_train, label=y_train, group=train_group_sizes, categorical_feature=categorical_features)
    dvalid = lgb.Dataset(X_test, label=y_test, group=test_group_sizes, categorical_feature=categorical_features)

    # æœ€é©åŒ–ã•ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’
    print(" æœ€é©åŒ–ã•ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã™ã‚‹ã‚ˆï¼")
    model = lgb.train(
        best_params,
        dtrain,
        valid_sets=[dvalid],
        valid_names=['ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿'],
        num_boost_round=1000,  # æœ€å¤§åå¾©å›æ•°
        callbacks=[
            lgb.early_stopping(50),  # 50å›æ”¹å–„ãŒãªã‘ã‚Œã°æ—©æœŸçµ‚äº†ï¼ˆå­¦ç¿’ã®å®‰å®šåŒ–ï¼‰
            lgb.log_evaluation(10)   # 10å›ã”ã¨ã«çµæœè¡¨ç¤º
        ]
    )

    # ç‰¹å¾´é‡ã®é‡è¦åº¦ã‚’ç¢ºèªã™ã‚‹ï¼ˆãƒ¢ãƒ‡ãƒ«ãŒã©ã®æƒ…å ±ã‚’é‡è¦–ã—ã¦ã„ã‚‹ã‹ï¼‰
    importance = model.feature_importance()
    feature_names = X.columns
    feature_importance = pd.DataFrame({'feature': feature_names, 'importance': importance})
    feature_importance = feature_importance.sort_values('importance', ascending=False)
    print("ç‰¹å¾´é‡ã®é‡è¦åº¦:")
    print(feature_importance)

    # ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã™ã‚‹
    model_filepath = output_path / model_filename
    pickle.dump(model, open(model_filepath, 'wb'))
    print(f"[OK] ãƒ¢ãƒ‡ãƒ«ã‚’ {model_filepath} ã«ä¿å­˜ã—ã¾ã—ãŸ")

    # ã‚³ãƒã‚¯ã‚·ãƒ§ãƒ³ã‚’ã‚¯ãƒ­ãƒ¼ã‚º
    conn.close()


# æ—§æ¥ã®é–¢æ•°ã‚’ç¶­æŒï¼ˆäº’æ›æ€§ã®ãŸã‚ï¼‰
def make_model():
    """
    æ—§ãƒãƒ¼ã‚¸ãƒ§ãƒ³äº’æ›æ€§ã®ãŸã‚ã®é–¢æ•°
    é˜ªç¥ç«¶é¦¬å ´ã®ï¼“æ­³ä»¥ä¸ŠèŠä¸­é•·è·é›¢ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
    
    æ³¨æ„: äº’æ›æ€§ã®ãŸã‚ã€ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜ã•ã‚Œã¾ã™
    """
    create_universal_model(
        track_code='09',  # é˜ªç¥
        kyoso_shubetsu_code='13',  # 3æ­³ä»¥ä¸Š
        surface_type='turf',  # èŠ
        min_distance=1700,  # ä¸­é•·è·é›¢
        max_distance=9999,  # ä¸Šé™ãªã—
        model_filename='hanshin_shiba_3ageup_model.sav',
        output_dir='.'  # ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜ï¼ˆæ—¢å­˜ã®å‹•ä½œã‚’ç¶­æŒï¼‰
    )


if __name__ == '__main__':
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œï¼šæ—§ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¨åŒã˜ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
    make_model()