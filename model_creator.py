import psycopg2
import os
from pathlib2 import Path
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import pickle
import numpy as np
import lightgbm as lgb
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import mean_squared_error
import optuna
from sklearn.metrics import ndcg_score
from keiba_constants import get_track_name, format_model_description
from datetime import datetime


def create_universal_model(track_code, kyoso_shubetsu_code, surface_type, 
                          min_distance, max_distance, model_filename, output_dir='models',
                          year_start=2013, year_end=2022):
    """
    æ±ç”¨çš„ãªç«¶é¦¬äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ä½œæˆé–¢æ•°
    
    Args:
        track_code (str): ç«¶é¦¬å ´ã‚³ãƒ¼ãƒ‰ ('01'=æœ­å¹Œ, '02'=å‡½é¤¨, ..., '09'=é˜ªç¥, '10'=å°å€‰)
        kyoso_shubetsu_code (str): ç«¶äº‰ç¨®åˆ¥ã‚³ãƒ¼ãƒ‰ ('12'=3æ­³, '13'=3æ­³ä»¥ä¸Š)
        surface_type (str): 'turf' or 'dirt' (èŠã¾ãŸã¯ãƒ€ãƒ¼ãƒˆ)
        min_distance (int): æœ€å°è·é›¢ (ä¾‹: 1000)
        max_distance (int): æœ€å¤§è·é›¢ (ä¾‹: 3600, ä¸Šé™ãªã—ã®å ´åˆã¯9999)
        model_filename (str): ä¿å­˜ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«å (ä¾‹: 'hanshin_turf_3ageup.sav')
        output_dir (str): ãƒ¢ãƒ‡ãƒ«ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 'models')
        year_start (int): å­¦ç¿’ãƒ‡ãƒ¼ã‚¿é–‹å§‹å¹´ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 2013)
        year_end (int): å­¦ç¿’ãƒ‡ãƒ¼ã‚¿çµ‚äº†å¹´ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 2022)
    
    Returns:
        None: ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
    """
    
    # ã‚«ãƒ¬ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ã‚¹ã‚¯ãƒªãƒ—ãƒˆé…ç½®ç®‡æ‰€ã«å¤‰æ›´
    os.chdir(Path(__file__).parent)
    print(f"ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª:{os.getcwd()}")
    
    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆï¼ˆå­˜åœ¨ã—ãªã„å ´åˆï¼‰
    output_path = Path(output_dir)
    output_path.mkdir(exist_ok=True)
    print(f"ğŸ“ ãƒ¢ãƒ‡ãƒ«ä¿å­˜å…ˆ: {output_path.absolute()}")

    # PostgreSQL ã‚³ãƒã‚¯ã‚·ãƒ§ãƒ³ã®ä½œæˆ
    conn = psycopg2.connect(
        host='localhost',
        port='5432',
        user='postgres',
        password='ahtaht88',
        dbname='keiba'
    )

    # ãƒˆãƒ©ãƒƒã‚¯æ¡ä»¶ã‚’å‹•çš„ã«è¨­å®š
    if surface_type.lower() == 'turf':
        # èŠã®å ´åˆ
        # TODO èŠã¯10ï½22ã¨åºƒãç¯„å›²æŒ‡å®šã™ã‚‹ã€‚èŠã¨ãƒ€ãƒ¼ãƒˆã§ç²¾åº¦ã«é•ã„ãŒå‡ºã‚‹ã‚ˆã†ã§ã‚ã‚Œã°å¯¾è±¡ãƒˆãƒ©ãƒƒã‚¯ã‚³ãƒ¼ãƒ‰ã‚’æ¸›ã‚‰ã™ãªã©ã®å·¥å¤«ãŒå¿…è¦ã‹ã‚‚ã€‚
        track_condition = "cast(rase.track_code as integer) between 10 and 22"
        baba_condition = "ra.babajotai_code_shiba"
    else:
        # ãƒ€ãƒ¼ãƒˆã®å ´åˆ
        track_condition = "cast(rase.track_code as integer) between 23 and 24"
        baba_condition = "ra.babajotai_code_dirt"

    # è·é›¢æ¡ä»¶ã‚’è¨­å®š
    if max_distance == 9999:
        distance_condition = f"cast(rase.kyori as integer) >= {min_distance}"
    else:
        distance_condition = f"cast(rase.kyori as integer) between {min_distance} and {max_distance}"
    
    # ç«¶äº‰ç¨®åˆ¥ã‚’è¨­å®š
    if kyoso_shubetsu_code == '12':
        # 3æ­³æˆ¦
        kyoso_shubetsu_condition = "cast(rase.kyoso_shubetsu_code as integer) = 12"
    elif kyoso_shubetsu_code == '13':
        kyoso_shubetsu_condition = "cast(rase.kyoso_shubetsu_code as integer) >= 13"

    # SQLã‚¯ã‚¨ãƒªã‚’å‹•çš„ã«ç”Ÿæˆ
    sql = f"""
    select * from (
        select
        ra.kaisai_nen,
        ra.kaisai_tsukihi,
        ra.keibajo_code,
        CASE 
            WHEN ra.keibajo_code = '01' THEN 'æœ­å¹Œ' 
            WHEN ra.keibajo_code = '02' THEN 'å‡½é¤¨' 
            WHEN ra.keibajo_code = '03' THEN 'ç¦å³¶' 
            WHEN ra.keibajo_code = '04' THEN 'æ–°æ½Ÿ' 
            WHEN ra.keibajo_code = '05' THEN 'æ±äº¬' 
            WHEN ra.keibajo_code = '06' THEN 'ä¸­å±±' 
            WHEN ra.keibajo_code = '07' THEN 'ä¸­äº¬' 
            WHEN ra.keibajo_code = '08' THEN 'äº¬éƒ½' 
            WHEN ra.keibajo_code = '09' THEN 'é˜ªç¥' 
            WHEN ra.keibajo_code = '10' THEN 'å°å€‰' 
            ELSE '' 
        END keibajo_name,
        ra.race_bango,
        ra.kyori,
        ra.tenko_code,
        {baba_condition} as babajotai_code,
        ra.grade_code,
        ra.kyoso_joken_code,
        ra.kyoso_shubetsu_code,
        ra.track_code,
        ra.shusso_tosu,
        seum.ketto_toroku_bango,
        trim(seum.bamei),
        seum.wakuban,
        cast(seum.umaban as integer) as umaban_numeric,
        seum.barei,
        seum.kishu_code,
        seum.chokyoshi_code,
        seum.kishu_name,
        seum.chokyoshi_name,
        seum.futan_juryo,
        nullif(cast(seum.tansho_odds as float), 0) / 10 as tansho_odds,
        seum.seibetsu_code,
        nullif(cast(seum.tansho_ninkijun as integer), 0) as tansho_ninkijun_numeric,
        18 - cast(seum.kakutei_chakujun as integer) + 1 as kakutei_chakujun_numeric, 
        1.0 / nullif(cast(seum.kakutei_chakujun as integer), 0) as chakujun_score,  --ä¸Šä½ç€é †ã»ã©1ã«è¿‘ããªã‚‹
        AVG(
            1 - (cast(seum.kakutei_chakujun as float) / cast(ra.shusso_tosu as float))
        ) OVER (
            PARTITION BY seum.ketto_toroku_bango
            ORDER BY cast(ra.kaisai_nen as integer), cast(ra.kaisai_tsukihi as integer)
            ROWS BETWEEN 3 PRECEDING AND 1 PRECEDING
        ) AS past_avg_sotai_chakujun,
        AVG(
            cast(ra.kyori as integer) /
            NULLIF(
                FLOOR(cast(seum.soha_time as integer) / 1000) * 60 +
                FLOOR((cast(seum.soha_time as integer) % 1000) / 10) +
                (cast(seum.soha_time as integer) % 10) * 0.1,
                0
            )
        ) OVER (
            PARTITION BY seum.ketto_toroku_bango
            ORDER BY cast(ra.kaisai_nen as integer), cast(ra.kaisai_tsukihi as integer)
            ROWS BETWEEN 3 PRECEDING AND 1 PRECEDING
        ) AS time_index,
        SUM(
            CASE 
                WHEN seum.kakutei_chakujun = '01' THEN 100
                WHEN seum.kakutei_chakujun = '02' THEN 80
                WHEN seum.kakutei_chakujun = '03' THEN 60
                WHEN seum.kakutei_chakujun = '04' THEN 40
                WHEN seum.kakutei_chakujun = '05' THEN 30
                WHEN seum.kakutei_chakujun = '06' THEN 20
                WHEN seum.kakutei_chakujun = '07' THEN 10
                ELSE 5 
            END
            * CASE 
                WHEN ra.grade_code = 'A' THEN 3.00                                                                                          --G1 (1.00â†’3.00ã«å¼·åŒ–)
                WHEN ra.grade_code = 'B' THEN 2.00                                                                                          --G2 (0.80â†’2.00ã«å¼·åŒ–)
                WHEN ra.grade_code = 'C' THEN 1.50                                                                                          --G3 (0.60â†’1.50ã«å¼·åŒ–)
                WHEN ra.grade_code <> 'A' AND ra.grade_code <> 'B' AND ra.grade_code <> 'C' AND ra.kyoso_joken_code = '999' THEN 1.00       --OP (0.50â†’1.00ã«èª¿æ•´)
                WHEN ra.grade_code <> 'A' AND ra.grade_code <> 'B' AND ra.grade_code <> 'C' AND ra.kyoso_joken_code = '016' THEN 0.80       --3å‹ã‚¯ãƒ©ã‚¹ (0.40â†’0.80ã«èª¿æ•´)
                WHEN ra.grade_code <> 'A' AND ra.grade_code <> 'B' AND ra.grade_code <> 'C' AND ra.kyoso_joken_code = '010' THEN 0.60       --2å‹ã‚¯ãƒ©ã‚¹ (0.30â†’0.60ã«èª¿æ•´)
                WHEN ra.grade_code <> 'A' AND ra.grade_code <> 'B' AND ra.grade_code <> 'C' AND ra.kyoso_joken_code = '005' THEN 0.40       --1å‹ã‚¯ãƒ©ã‚¹ (0.20â†’0.40ã«èª¿æ•´)
                ELSE 0.20                                                                                                                   --æœªå‹åˆ© (0.10â†’0.20ã«èª¿æ•´)
            END
        ) OVER (
            PARTITION BY seum.ketto_toroku_bango
            ORDER BY cast(ra.kaisai_nen as integer), cast(ra.kaisai_tsukihi as integer)
            ROWS BETWEEN 3 PRECEDING AND 1 PRECEDING  
        ) AS past_score,  --ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ¥ã‚¹ã‚³ã‚¢
        CASE 
            WHEN AVG(
                CASE 
                    WHEN cast(seum.kohan_3f as integer) > 0 AND cast(seum.kohan_3f as integer) < 999 THEN
                    CAST(seum.kohan_3f AS FLOAT) / 10
                    ELSE NULL
                END
            ) OVER (
                PARTITION BY seum.ketto_toroku_bango
                ORDER BY cast(ra.kaisai_nen as integer), cast(ra.kaisai_tsukihi as integer)
                ROWS BETWEEN 3 PRECEDING AND 1 PRECEDING
            ) IS NOT NULL THEN
            AVG(
                CASE 
                    WHEN cast(seum.kohan_3f as integer) > 0 AND cast(seum.kohan_3f as integer) < 999 THEN
                    CAST(seum.kohan_3f AS FLOAT) / 10
                    ELSE NULL
                END
            ) OVER (
                PARTITION BY seum.ketto_toroku_bango
                ORDER BY cast(ra.kaisai_nen as integer), cast(ra.kaisai_tsukihi as integer)
                ROWS BETWEEN 3 PRECEDING AND 1 PRECEDING
            ) - 
            CASE
                WHEN cast(ra.kyori as integer) <= 1600 THEN 33.5
                WHEN cast(ra.kyori as integer) <= 2000 THEN 35.0
                WHEN cast(ra.kyori as integer) <= 2400 THEN 36.0
                ELSE 37.0
            END
            ELSE 0
        END AS kohan_3f_index
    from
        jvd_ra ra 
        inner join ( 
            select
                se.kaisai_nen
                , se.kaisai_tsukihi
                , se.keibajo_code
                , se.race_bango
                , se.kakutei_chakujun
                , se.ketto_toroku_bango
                , se.bamei
                , se.wakuban
                , se.umaban
                , se.barei
                , se.seibetsu_code
                , se.kishu_code
                , se.chokyoshi_code
                , trim(se.kishumei_ryakusho) as kishu_name
                , trim(se.chokyoshimei_ryakusho) as chokyoshi_name
                , se.futan_juryo
                , se.tansho_odds
                , se.tansho_ninkijun
                , se.kohan_3f
                , se.soha_time
            from
                jvd_se se
            where 
                se.kohan_3f <> '000' 
                and se.kohan_3f <> '999'
        ) seum 
            on ra.kaisai_nen = seum.kaisai_nen 
            and ra.kaisai_tsukihi = seum.kaisai_tsukihi 
            and ra.keibajo_code = seum.keibajo_code 
            and ra.race_bango = seum.race_bango 
    where
        cast(ra.kaisai_nen as integer) between {year_start} and {year_end}    --å­¦ç¿’ãƒ‡ãƒ¼ã‚¿å¹´ç¯„å›²
    ) rase 
    where 
    rase.keibajo_code = '{track_code}'                                        --ç«¶é¦¬å ´æŒ‡å®š
    and {kyoso_shubetsu_condition}                                            --ç«¶äº‰ç¨®åˆ¥
    and {track_condition}                                                     --èŠ/ãƒ€ãƒ¼ãƒˆ
    and {distance_condition}                                                  --è·é›¢æ¡ä»¶
    """

    # ãƒ¢ãƒ‡ãƒ«èª¬æ˜ã‚’ç”Ÿæˆ
    model_desc = format_model_description(track_code, kyoso_shubetsu_code, surface_type, min_distance, max_distance)
    print(f"ğŸ‡ ãƒ¢ãƒ‡ãƒ«ä½œæˆé–‹å§‹: {model_desc}")
    
    # SQLã‚’ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›ï¼ˆå¸¸ã«ä¸Šæ›¸ãï¼‰
    log_filepath = Path('sql_log.txt')
    with open(log_filepath, 'w', encoding='utf-8') as f:
        f.write(f"=== ãƒ¢ãƒ‡ãƒ«ä½œæˆSQL ===\n")
        f.write(f"ãƒ¢ãƒ‡ãƒ«: {model_desc}\n")
        f.write(f"ä½œæˆæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"\n{sql}\n")
    print(f"ğŸ“ SQLã‚’ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›: {log_filepath}")
    
    # SELECTçµæœã‚’DataFrame
    df = pd.read_sql_query(sql=sql, con=conn)
    
    if len(df) == 0:
        print("âŒ æŒ‡å®šã—ãŸæ¡ä»¶ã«åˆè‡´ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚æ¡ä»¶ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        return

    print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(df)}ä»¶")

    # ç€é †ã‚¹ã‚³ã‚¢ãŒ0ã®ãƒ‡ãƒ¼ã‚¿ã¯ç„¡åŠ¹æ‰±ã„ã«ã—ã¦é™¤å¤–
    df = df[df['chakujun_score'] > 0]

    # ğŸ”¥ä¿®æ­£: ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã‚’é©åˆ‡ã«å®Ÿæ–½
    # é¨æ‰‹ã‚³ãƒ¼ãƒ‰ãƒ»èª¿æ•™å¸«ã‚³ãƒ¼ãƒ‰ãƒ»é¦¬åãªã©ã®æ–‡å­—åˆ—åˆ—ã‚’ä¿æŒã—ãŸã¾ã¾ã€æ•°å€¤åˆ—ã®ã¿ã‚’å‡¦ç†
    print("ğŸ” ãƒ‡ãƒ¼ã‚¿å‹ç¢ºèª...")
    print(f"  kishu_codeå‹ï¼ˆä¿®æ­£å‰ï¼‰: {df['kishu_code'].dtype}")
    print(f"  kishu_codeã‚µãƒ³ãƒ—ãƒ«: {df['kishu_code'].head(5).tolist()}")
    print(f"  kishu_codeãƒ¦ãƒ‹ãƒ¼ã‚¯æ•°: {df['kishu_code'].nunique()}")
    
    # æ•°å€¤åŒ–ã™ã‚‹åˆ—ã‚’æ˜ç¤ºçš„ã«æŒ‡å®šï¼ˆæ–‡å­—åˆ—åˆ—ã¯é™¤å¤–ï¼‰
    numeric_columns = [
        'wakuban', 'umaban_numeric', 'barei', 'futan_juryo', 'tansho_odds',
        'kaisai_nen', 'kaisai_tsukihi', 'race_bango', 'kyori', 'shusso_tosu',
        'tenko_code', 'babajotai_code', 'grade_code', 'kyoso_joken_code',
        'kyoso_shubetsu_code', 'track_code', 'seibetsu_code',
        'kakutei_chakujun_numeric', 'chakujun_score', 'past_avg_sotai_chakujun',
        'time_index', 'past_score', 'kohan_3f_index'
    ]
    
    # æ•°å€¤åŒ–ã™ã‚‹åˆ—ã®ã¿å‡¦ç†ï¼ˆæ–‡å­—åˆ—åˆ—ã¯ä¿æŒï¼‰
    for col in numeric_columns:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce')
    
    # æ¬ æå€¤ã‚’0ã§åŸ‹ã‚ã‚‹ï¼ˆæ•°å€¤åˆ—ã®ã¿ï¼‰
    df[numeric_columns] = df[numeric_columns].fillna(0)
    
    # æ–‡å­—åˆ—å‹ã®åˆ—ã¯ãã®ã¾ã¾ä¿æŒï¼ˆkishu_code, chokyoshi_code, bamei ãªã©ï¼‰
    print(f"  kishu_codeå‹ï¼ˆä¿®æ­£å¾Œï¼‰: {df['kishu_code'].dtype}")
    print(f"  kishu_codeã‚µãƒ³ãƒ—ãƒ«: {df['kishu_code'].head(5).tolist()}")
    print("âœ… ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†å®Œäº†ï¼ˆæ–‡å­—åˆ—åˆ—ã‚’ä¿æŒï¼‰")

    # past_avg_sotai_chakujunã¯SQLã§è¨ˆç®—æ¸ˆã¿ã®å˜ç´”ç§»å‹•å¹³å‡ã‚’ä½¿ç”¨
    # (EWMå®Ÿé¨“ã®çµæœã€å˜ç´”å¹³å‡ã®æ–¹ãŒè¤‡å‹ãƒ»ä¸‰é€£è¤‡ã§å®‰å®šã—ãŸæ€§èƒ½ã‚’ç¤ºã—ãŸ)

    X = df.loc[:, [
        # "futan_juryo",
        "past_score",
        "kohan_3f_index",
        "past_avg_sotai_chakujun",
        "time_index",
    ]].astype(float)
    
    # é«˜æ€§èƒ½ãªæ´¾ç”Ÿç‰¹å¾´é‡ã‚’è¿½åŠ ï¼
    # æ ç•ªã¨é ­æ•°ã®æ¯”ç‡ï¼ˆå†…æ æœ‰åˆ©åº¦ï¼‰
    max_wakuban = df.groupby(['kaisai_nen', 'kaisai_tsukihi', 'race_bango'])['wakuban'].transform('max')
    df['wakuban_ratio'] = df['wakuban'] / max_wakuban
    X['wakuban_ratio'] = df['wakuban_ratio']
    
    # æ–¤é‡ã¨é¦¬é½¢ã®æ¯”ç‡ï¼ˆè‹¥é¦¬ã®è² æ‹…èƒ½åŠ›ï¼‰
    df['futan_per_barei'] = df['futan_juryo'] / df['barei'].replace(0, 1)
    X['futan_per_barei'] = df['futan_per_barei']

    # ğŸš€ é«˜ç²¾åº¦äºŒæ¬¡ç‰¹å¾´é‡ã‚’è¿½åŠ ï¼ˆäºˆæ¸¬ã‚¹ã‚³ã‚¢é‡è¤‡å›é¿ + ç²¾åº¦å‘ä¸Šï¼‰
    # ã‚·ãƒ³ãƒ—ãƒ«ãªç‰¹å¾´é‡ã‹ã‚‰å§‹ã‚ã¦éå­¦ç¿’ã‚’é˜²ã
    
    # é¦¬ç•ªÃ—è·é›¢ã®ç›¸äº’ä½œç”¨ï¼ˆå†…å¤–æ ã®è·é›¢é©æ€§ï¼‰
    df['umaban_kyori_interaction'] = df['umaban_numeric'] * df['kyori'] / 1000  # ã‚¹ã‚±ãƒ¼ãƒ«èª¿æ•´
    X['umaban_kyori_interaction'] = df['umaban_kyori_interaction']
    
    # ğŸ”¥æ”¹å–„ã•ã‚ŒãŸç‰¹å¾´é‡ğŸ”¥
    # 2. futan_per_bareiã®éç·šå½¢å¤‰æ›
    df['futan_per_barei_log'] = np.log(df['futan_per_barei'].clip(lower=0.1))
    X['futan_per_barei_log'] = df['futan_per_barei_log']
    
    # æœŸå¾…æ–¤é‡ã‹ã‚‰ã®å·®åˆ†ï¼ˆå¹´é½¢åˆ¥æœŸå¾…æ–¤é‡ã¨ã®å·®ï¼‰
    expected_weight_by_age = {2: 48, 3: 52, 4: 55, 5: 57, 6: 57, 7: 56, 8: 55}
    df['futan_deviation'] = df.apply(
        lambda row: row['futan_juryo'] - expected_weight_by_age.get(row['barei'], 55), 
        axis=1
    )
    X['futan_deviation'] = df['futan_deviation']
    
    # # 4. è¤‡æ•°ã®ãƒ”ãƒ¼ã‚¯å¹´é½¢ãƒ‘ã‚¿ãƒ¼ãƒ³
    # df['barei_peak_distance'] = abs(df['barei'] - 4)  # 4æ­³ã‚’ãƒ”ãƒ¼ã‚¯ã¨ä»®å®šï¼ˆæ—¢å­˜ï¼‰
    # X['barei_peak_distance'] = df['barei_peak_distance']
    
    # # 3æ­³çŸ­è·é›¢ãƒ”ãƒ¼ã‚¯ï¼ˆæ—©ç†Ÿå‹ï¼‰
    # df['barei_peak_short'] = abs(df['barei'] - 3)
    # X['barei_peak_short'] = df['barei_peak_short']
    
    # # 5æ­³é•·è·é›¢ãƒ”ãƒ¼ã‚¯ï¼ˆæ™©æˆå‹ï¼‰
    # df['barei_peak_long'] = abs(df['barei'] - 5)
    # X['barei_peak_long'] = df['barei_peak_long']

    # 5. æ ç•ªãƒã‚¤ã‚¢ã‚¹ã‚¹ã‚³ã‚¢ï¼ˆæ ç•ªã®æ­´å²çš„å„ªä½æ€§ã‚’æ•°å€¤åŒ–ï¼‰
    # æ ç•ªåˆ¥ã®æ­´å²çš„ç€é †åˆ†å¸ƒã‚’è¨ˆç®—
    wakuban_stats = df.groupby('wakuban').agg({
        'kakutei_chakujun_numeric': ['mean', 'std', 'count']
    }).round(4)
    wakuban_stats.columns = ['waku_avg_rank', 'waku_std_rank', 'waku_count']
    wakuban_stats = wakuban_stats.reset_index()
    
    # å…¨ä½“å¹³å‡ã‹ã‚‰ã®åå·®ã§ãƒã‚¤ã‚¢ã‚¹ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
    overall_avg_rank = df['kakutei_chakujun_numeric'].mean()
    wakuban_stats['wakuban_bias_score'] = (overall_avg_rank - wakuban_stats['waku_avg_rank']) / wakuban_stats['waku_std_rank']
    wakuban_stats['wakuban_bias_score'] = wakuban_stats['wakuban_bias_score'].fillna(0)  # NaNã‚’0ã§åŸ‹ã‚ã‚‹
    
    # DataFrameã«ãƒãƒ¼ã‚¸
    df = df.merge(wakuban_stats[['wakuban', 'wakuban_bias_score']], on='wakuban', how='left')
    # X['wakuban_bias_score'] = df['wakuban_bias_score']

    # ãƒ¬ãƒ¼ã‚¹å†…ã§ã®é¦¬ç•ªç›¸å¯¾ä½ç½®ï¼ˆé ­æ•°ã«ã‚ˆã‚‹æ­£è¦åŒ–ï¼‰
    df['umaban_percentile'] = df.groupby(['kaisai_nen', 'kaisai_tsukihi', 'race_bango'])['umaban_numeric'].transform(
        lambda x: x.rank(pct=True)
    )
    X['umaban_percentile'] = df['umaban_percentile']
    
    # ç ”ç©¶ç”¨ç‰¹å¾´é‡ è¿½åŠ 
    # æ–¤é‡åå·®å€¤ï¼ˆãƒ¬ãƒ¼ã‚¹å†…ã§æ¨™æº–åŒ–ï¼‰
    # ãƒ¬ãƒ¼ã‚¹å†…ã®å¹³å‡ã¨æ¨™æº–åå·®ã‚’è¨ˆç®—ã—ã¦ã€å„é¦¬ã®æ–¤é‡ãŒã©ã‚Œãã‚‰ã„é‡ã„/è»½ã„ã‹ã‚’è¡¨ç¾
    race_group = df.groupby(['kaisai_nen', 'kaisai_tsukihi', 'race_bango'])['futan_juryo']
    df['futan_mean'] = race_group.transform('mean')
    df['futan_std'] = race_group.transform('std')
    
    # æ¨™æº–åå·®ãŒ0ã®å ´åˆï¼ˆå…¨é ­åŒã˜æ–¤é‡ï¼‰ã¯0ã«ã™ã‚‹
    df['futan_zscore'] = np.where(
        df['futan_std'] > 0,
        (df['futan_juryo'] - df['futan_mean']) / df['futan_std'],
        0
    )
    X['futan_zscore'] = df['futan_zscore']
    
    # ãƒ¬ãƒ¼ã‚¹å†…ã§ã®æ–¤é‡é †ä½ï¼ˆãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«ï¼‰
    # 0.0=æœ€è»½é‡ã€1.0=æœ€é‡é‡
    df['futan_percentile'] = race_group.transform(lambda x: x.rank(pct=True))
    X['futan_percentile'] = df['futan_percentile']

    # ğŸ”¥æ–°æ©Ÿèƒ½: è·é›¢é©æ€§ã‚¹ã‚³ã‚¢ã‚’è¿½åŠ ï¼ˆ3ç¨®é¡ï¼‰ğŸ”¥
    print("ğŸ‡ è·é›¢é©æ€§ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ä¸­...")
    
    # è·é›¢ã‚«ãƒ†ã‚´ãƒªåˆ†é¡é–¢æ•°
    def categorize_distance(kyori):
        """è·é›¢ã‚’4ã‚«ãƒ†ã‚´ãƒªã«åˆ†é¡"""
        if kyori <= 1400:
            return 'short'  # çŸ­è·é›¢
        elif kyori <= 1800:
            return 'mile'   # ãƒã‚¤ãƒ«
        elif kyori <= 2400:
            return 'middle' # ä¸­è·é›¢
        else:
            return 'long'   # é•·è·é›¢
    
    # ä»Šå›ã®ãƒ¬ãƒ¼ã‚¹ã®è·é›¢ã‚«ãƒ†ã‚´ãƒªã‚’è¿½åŠ 
    df['distance_category'] = df['kyori'].apply(categorize_distance)
    
    # ğŸ”¥é‡è¦: é¦¬å ´æƒ…å ±ã‚‚å…ˆã«è¿½åŠ ï¼ˆdf_sortedã§ä½¿ã†ãŸã‚ï¼‰ğŸ”¥
    # èŠ/ãƒ€ãƒ¼ãƒˆåˆ†é¡é–¢æ•°
    def categorize_surface(track_code):
        """ãƒˆãƒ©ãƒƒã‚¯ã‚³ãƒ¼ãƒ‰ã‹ã‚‰èŠ/ãƒ€ãƒ¼ãƒˆã‚’åˆ¤å®š"""
        track_code_int = int(track_code)
        if 10 <= track_code_int <= 22:
            return 'turf'  # èŠ
        elif 23 <= track_code_int <= 24:
            return 'dirt'  # ãƒ€ãƒ¼ãƒˆ
        else:
            return 'unknown'
    
    # é¦¬å ´çŠ¶æ…‹åˆ†é¡é–¢æ•°
    def categorize_baba_condition(baba_code):
        """é¦¬å ´çŠ¶æ…‹ã‚³ãƒ¼ãƒ‰ã‚’åˆ†é¡"""
        if baba_code == 1:
            return 'good'      # è‰¯
        elif baba_code == 2:
            return 'slightly'  # ç¨é‡
        elif baba_code == 3:
            return 'heavy'     # é‡
        elif baba_code == 4:
            return 'bad'       # ä¸è‰¯
        else:
            return 'unknown'
    
    # ä»Šå›ã®ãƒ¬ãƒ¼ã‚¹ã®é¦¬å ´æƒ…å ±ã‚’è¿½åŠ 
    df['surface_type'] = df['track_code'].apply(categorize_surface)
    df['baba_condition'] = df['babajotai_code'].apply(categorize_baba_condition)
    
    # æ™‚ç³»åˆ—é †ã«ã‚½ãƒ¼ãƒˆï¼ˆé¦¬ã”ã¨ã«éå»ãƒ‡ãƒ¼ã‚¿ã‚’å‚ç…§ã™ã‚‹ãŸã‚ï¼‰
    df_sorted = df.sort_values(['ketto_toroku_bango', 'kaisai_nen', 'kaisai_tsukihi']).copy()
    
    # è·é›¢ã‚«ãƒ†ã‚´ãƒªåˆ¥é©æ€§ã‚¹ã‚³ã‚¢ï¼ˆåŒã˜ã‚«ãƒ†ã‚´ãƒªã§ã®éå»5èµ°ã®å¹³å‡ç›¸å¯¾ç€é †ï¼‰
    def calc_distance_category_score(group):
        """å„é¦¬ã®è·é›¢ã‚«ãƒ†ã‚´ãƒªåˆ¥é©æ€§ã‚’è¨ˆç®—"""
        scores = []
        for idx in range(len(group)):
            if idx == 0:
                scores.append(0.5)  # âœ… ä¿®æ­£: ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã¯ä¸­ç«‹å€¤
                continue
            
            # ç¾åœ¨ã®ãƒ¬ãƒ¼ã‚¹ã®è·é›¢ã‚«ãƒ†ã‚´ãƒª
            current_category = group.iloc[idx]['distance_category']
            
            # éå»ã®ãƒ¬ãƒ¼ã‚¹ï¼ˆåŒã˜ã‚«ãƒ†ã‚´ãƒªï¼‰ã‹ã‚‰ç›´è¿‘5èµ°ã‚’å–å¾—
            past_same_category = group.iloc[:idx][
                group.iloc[:idx]['distance_category'] == current_category
            ].tail(5)
            
            if len(past_same_category) > 0:
                # ç›¸å¯¾ç€é †ã®å¹³å‡ï¼ˆ1 - ç€é †/18ï¼‰
                avg_score = (1 - (past_same_category['kakutei_chakujun_numeric'] / 18.0)).mean()
                scores.append(avg_score)
            else:
                scores.append(0.5)  # âœ… ä¿®æ­£: ãƒ‡ãƒ¼ã‚¿ãªã—ã¯ä¸­ç«‹å€¤
        
        return pd.Series(scores, index=group.index)
    
    df_sorted['distance_category_score'] = df_sorted.groupby('ketto_toroku_bango', group_keys=False).apply(
        calc_distance_category_score
    ).values
    
    # è¿‘ä¼¼è·é›¢ã§ã®æˆç¸¾ï¼ˆÂ±200mä»¥å†…ã€ç›´è¿‘10èµ°ï¼‰
    def calc_similar_distance_score(group):
        """è¿‘ä¼¼è·é›¢ã§ã®é©æ€§ã‚’è¨ˆç®—"""
        scores = []
        for idx in range(len(group)):
            if idx == 0:
                scores.append(0.5)  # âœ… ä¿®æ­£: ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã¯ä¸­ç«‹å€¤
                continue
            
            current_kyori = group.iloc[idx]['kyori']
            
            # éå»ã®ãƒ¬ãƒ¼ã‚¹ï¼ˆÂ±200mä»¥å†…ï¼‰ã‹ã‚‰ç›´è¿‘10èµ°ã‚’å–å¾—
            past_similar = group.iloc[:idx][
                abs(group.iloc[:idx]['kyori'] - current_kyori) <= 200
            ].tail(10)
            
            if len(past_similar) > 0:
                avg_score = (1 - (past_similar['kakutei_chakujun_numeric'] / 18.0)).mean()
                scores.append(avg_score)
            else:
                scores.append(0.5)  # âœ… ä¿®æ­£: ãƒ‡ãƒ¼ã‚¿ãªã—ã¯ä¸­ç«‹å€¤
        
        return pd.Series(scores, index=group.index)
    
    df_sorted['similar_distance_score'] = df_sorted.groupby('ketto_toroku_bango', group_keys=False).apply(
        calc_similar_distance_score
    ).values
    
    # è·é›¢å¤‰åŒ–å¯¾å¿œåŠ›ï¼ˆå‰èµ°ã‹ã‚‰ã®è·é›¢å¤‰åŒ–Â±100mä»¥ä¸Šæ™‚ã®æˆç¸¾ã€ç›´è¿‘5èµ°ï¼‰
    def calc_distance_change_adaptability(group):
        """è·é›¢å¤‰åŒ–æ™‚ã®å¯¾å¿œåŠ›ã‚’è¨ˆç®—"""
        scores = []
        for idx in range(len(group)):
            if idx < 2:  # æœ€ä½2èµ°åˆ†ã®ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦
                scores.append(0.5)  # âœ… ä¿®æ­£: ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã¯ä¸­ç«‹å€¤
                continue
            
            # âœ… ä¿®æ­£: éå»6èµ°åˆ†ã‚’å–å¾—ï¼ˆå‰èµ°ã¨ã®å·®åˆ†ã‚’è¦‹ã‚‹ãŸã‚ï¼‰
            past_races = group.iloc[max(0, idx-6):idx].copy()
            
            if len(past_races) >= 3:  # âœ… ä¿®æ­£: æœ€ä½3èµ°å¿…è¦ï¼ˆå·®åˆ†2å€‹ï¼‰
                # è·é›¢ã®å¤‰åŒ–é‡ã‚’è¨ˆç®—
                past_races['kyori_diff'] = past_races['kyori'].diff().abs()
                
                # âœ… ä¿®æ­£: æœ€æ–°5èµ°ã®ã¿ã‚’è©•ä¾¡ï¼ˆæœ€åˆã®1è¡Œã¯NaNãªã®ã§é™¤å¤–ï¼‰
                past_races_eval = past_races.tail(5)
                
                # è·é›¢å¤‰åŒ–ãŒ100mä»¥ä¸Šã®ãƒ¬ãƒ¼ã‚¹ã‚’æŠ½å‡º
                changed_races = past_races_eval[past_races_eval['kyori_diff'] >= 100]
                
                if len(changed_races) > 0:
                    avg_score = (1 - (changed_races['kakutei_chakujun_numeric'] / 18.0)).mean()
                    scores.append(avg_score)
                else:
                    scores.append(0.5)  # âœ… ä¿®æ­£: å¤‰åŒ–ãªã—ã¯ä¸­ç«‹
            else:
                scores.append(0.5)  # âœ… ä¿®æ­£: ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã¯ä¸­ç«‹å€¤
        
        return pd.Series(scores, index=group.index)
    
    df_sorted['distance_change_adaptability'] = df_sorted.groupby('ketto_toroku_bango', group_keys=False).apply(
        calc_distance_change_adaptability
    ).values
    
    # å…ƒã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹é †ã«æˆ»ã™
    df = df.copy()
    df['distance_category_score'] = df_sorted.sort_index()['distance_category_score']
    df['similar_distance_score'] = df_sorted.sort_index()['similar_distance_score']
    df['distance_change_adaptability'] = df_sorted.sort_index()['distance_change_adaptability']
    
    # ç‰¹å¾´é‡ã«è¿½åŠ 
    X['distance_category_score'] = df['distance_category_score']
    X['similar_distance_score'] = df['similar_distance_score']
    # X['distance_change_adaptability'] = df['distance_change_adaptability']
    
    print(f"âœ… è·é›¢é©æ€§ã‚¹ã‚³ã‚¢ã‚’3ç¨®é¡è¿½åŠ ã—ã¾ã—ãŸï¼")
    print(f"  - distance_category_score: è·é›¢ã‚«ãƒ†ã‚´ãƒªåˆ¥é©æ€§ï¼ˆç›´è¿‘5èµ°ï¼‰")
    print(f"  - similar_distance_score: è¿‘ä¼¼è·é›¢ã§ã®æˆç¸¾ï¼ˆÂ±200mã€ç›´è¿‘10èµ°ï¼‰")
    print(f"  - distance_change_adaptability: è·é›¢å¤‰åŒ–å¯¾å¿œåŠ›ï¼ˆÂ±100mä»¥ä¸Šã€ç›´è¿‘5èµ°ï¼‰")
    
    # ğŸ”¥æ–°æ©Ÿèƒ½: é¦¬å ´é©æ€§ã‚¹ã‚³ã‚¢ã‚’è¿½åŠ ï¼ˆ3ç¨®é¡ï¼‰ğŸ”¥
    print("ğŸŒ¿ é¦¬å ´é©æ€§ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ä¸­...")
    
    # é¦¬å ´æƒ…å ±ã¯æ—¢ã«df_sortedã«å«ã¾ã‚Œã¦ã„ã‚‹ã®ã§ã€ãã®ã¾ã¾ä½¿ç”¨
    # 1ï¸âƒ£ èŠ/ãƒ€ãƒ¼ãƒˆåˆ¥é©æ€§ã‚¹ã‚³ã‚¢ï¼ˆç›´è¿‘10èµ°ï¼‰
    def calc_surface_score(group):
        """å„é¦¬ã®èŠ/ãƒ€ãƒ¼ãƒˆåˆ¥é©æ€§ã‚’è¨ˆç®—"""
        scores = []
        for idx in range(len(group)):
            if idx == 0:
                scores.append(0.5)  # âœ… ä¿®æ­£: ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã¯ä¸­ç«‹å€¤
                continue
            
            current_surface = group.iloc[idx]['surface_type']
            
            # åŒã˜é¦¬å ´ã‚¿ã‚¤ãƒ—ã§ã®éå»æˆç¸¾ï¼ˆç›´è¿‘10èµ°ï¼‰
            past_same_surface = group.iloc[:idx][
                group.iloc[:idx]['surface_type'] == current_surface
            ].tail(10)
            
            if len(past_same_surface) > 0:
                avg_score = (1 - (past_same_surface['kakutei_chakujun_numeric'] / 18.0)).mean()
                scores.append(avg_score)
            else:
                scores.append(0.5)  # âœ… ä¿®æ­£: ãƒ‡ãƒ¼ã‚¿ãªã—ã¯ä¸­ç«‹å€¤
        
        return pd.Series(scores, index=group.index)
    
    df_sorted['surface_aptitude_score'] = df_sorted.groupby('ketto_toroku_bango', group_keys=False).apply(
        calc_surface_score
    ).values
    
    # 2ï¸âƒ£ é¦¬å ´çŠ¶æ…‹åˆ¥é©æ€§ã‚¹ã‚³ã‚¢ï¼ˆè‰¯/ç¨é‡/é‡/ä¸è‰¯ã€ç›´è¿‘10èµ°ï¼‰
    def calc_baba_condition_score(group):
        """å„é¦¬ã®é¦¬å ´çŠ¶æ…‹åˆ¥é©æ€§ã‚’è¨ˆç®—"""
        scores = []
        for idx in range(len(group)):
            if idx == 0:
                scores.append(0.5)  # âœ… ä¿®æ­£: ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã¯ä¸­ç«‹å€¤
                continue
            
            current_condition = group.iloc[idx]['baba_condition']
            
            # åŒã˜é¦¬å ´çŠ¶æ…‹ã§ã®éå»æˆç¸¾ï¼ˆç›´è¿‘10èµ°ï¼‰
            past_same_condition = group.iloc[:idx][
                group.iloc[:idx]['baba_condition'] == current_condition
            ].tail(10)
            
            if len(past_same_condition) > 0:
                avg_score = (1 - (past_same_condition['kakutei_chakujun_numeric'] / 18.0)).mean()
                scores.append(avg_score)
            else:
                scores.append(0.5)  # âœ… ä¿®æ­£: ãƒ‡ãƒ¼ã‚¿ãªã—ã¯ä¸­ç«‹å€¤
        
        return pd.Series(scores, index=group.index)
    
    df_sorted['baba_condition_score'] = df_sorted.groupby('ketto_toroku_bango', group_keys=False).apply(
        calc_baba_condition_score
    ).values
    
    # 3ï¸âƒ£ é¦¬å ´å¤‰åŒ–å¯¾å¿œåŠ›ï¼ˆå‰èµ°ã¨ç•°ãªã‚‹é¦¬å ´çŠ¶æ…‹ã§ã®æˆç¸¾ã€ç›´è¿‘5èµ°ï¼‰
    def calc_baba_change_adaptability(group):
        """é¦¬å ´çŠ¶æ…‹å¤‰åŒ–æ™‚ã®å¯¾å¿œåŠ›ã‚’è¨ˆç®—"""
        scores = []
        for idx in range(len(group)):
            if idx < 2:  # æœ€ä½2èµ°åˆ†ã®ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦
                scores.append(0.5)  # âœ… ä¿®æ­£: ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã¯ä¸­ç«‹å€¤
                continue
            
            # âœ… ä¿®æ­£: éå»6èµ°åˆ†ã‚’å–å¾—ï¼ˆå‰èµ°ã¨ã®å¤‰åŒ–ã‚’è¦‹ã‚‹ãŸã‚ï¼‰
            past_races = group.iloc[max(0, idx-6):idx].copy()
            
            if len(past_races) >= 3:  # âœ… ä¿®æ­£: æœ€ä½3èµ°å¿…è¦
                # é¦¬å ´çŠ¶æ…‹ã®å¤‰åŒ–ã‚’æ¤œå‡ºï¼ˆå‰èµ°ã¨ç•°ãªã‚‹é¦¬å ´çŠ¶æ…‹ï¼‰
                past_races['baba_changed'] = past_races['baba_condition'].shift(1) != past_races['baba_condition']
                
                # âœ… ä¿®æ­£: æœ€æ–°5èµ°ã®ã¿ã‚’è©•ä¾¡
                past_races_eval = past_races.tail(5)
                
                # é¦¬å ´çŠ¶æ…‹ãŒå¤‰åŒ–ã—ãŸãƒ¬ãƒ¼ã‚¹ã®ã¿æŠ½å‡º
                changed_races = past_races_eval[past_races_eval['baba_changed'] == True]
                
                if len(changed_races) > 0:
                    avg_score = (1 - (changed_races['kakutei_chakujun_numeric'] / 18.0)).mean()
                    scores.append(avg_score)
                else:
                    scores.append(0.5)  # âœ… ä¿®æ­£: å¤‰åŒ–ãªã—ã¯ä¸­ç«‹
            else:
                scores.append(0.5)  # âœ… ä¿®æ­£: ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã¯ä¸­ç«‹å€¤
        
        return pd.Series(scores, index=group.index)
    
    df_sorted['baba_change_adaptability'] = df_sorted.groupby('ketto_toroku_bango', group_keys=False).apply(
        calc_baba_change_adaptability
    ).values
    
    # å…ƒã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹é †ã«æˆ»ã™
    df['surface_aptitude_score'] = df_sorted.sort_index()['surface_aptitude_score']
    df['baba_condition_score'] = df_sorted.sort_index()['baba_condition_score']
    df['baba_change_adaptability'] = df_sorted.sort_index()['baba_change_adaptability']
    
    # ç‰¹å¾´é‡ã«è¿½åŠ 
    X['surface_aptitude_score'] = df['surface_aptitude_score']
    # X['baba_condition_score'] = df['baba_condition_score']
    X['baba_change_adaptability'] = df['baba_change_adaptability']
    
    print(f"âœ… é¦¬å ´é©æ€§ã‚¹ã‚³ã‚¢ã‚’3ç¨®é¡è¿½åŠ ã—ã¾ã—ãŸï¼")
    print(f"  - surface_aptitude_score: èŠ/ãƒ€ãƒ¼ãƒˆåˆ¥é©æ€§ï¼ˆç›´è¿‘10èµ°ï¼‰")
    print(f"  - baba_condition_score: é¦¬å ´çŠ¶æ…‹åˆ¥é©æ€§ï¼ˆè‰¯/ç¨é‡/é‡/ä¸è‰¯ã€ç›´è¿‘10èµ°ï¼‰")
    print(f"  - baba_change_adaptability: é¦¬å ´å¤‰åŒ–å¯¾å¿œåŠ›ï¼ˆç›´è¿‘5èµ°ï¼‰")

    # ğŸ”¥æ–°æ©Ÿèƒ½: é¨æ‰‹ãƒ»èª¿æ•™å¸«ã®å‹•çš„èƒ½åŠ›ã‚¹ã‚³ã‚¢ã‚’è¿½åŠ ï¼ˆ4ç¨®é¡ï¼‰ğŸ”¥
    print("ğŸ‡ é¨æ‰‹ãƒ»èª¿æ•™å¸«ã®èƒ½åŠ›ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ä¸­...")
    
    # âœ… ä¿®æ­£: race_bangoã‚’è¿½åŠ ã—ã¦æ™‚ç³»åˆ—ãƒªãƒ¼ã‚¯ã‚’é˜²æ­¢
    df_sorted_kishu = df.sort_values(['kishu_code', 'kaisai_nen', 'kaisai_tsukihi', 'race_bango']).copy()
    
    # 1ï¸âƒ£ é¨æ‰‹ã®å®ŸåŠ›è£œæ­£ã‚¹ã‚³ã‚¢ï¼ˆæœŸå¾…ç€é †ã¨ã®å·®åˆ†ã€ç›´è¿‘3ãƒ¶æœˆï¼‰
    def calc_kishu_skill_adjusted_score(group):
        """é¨æ‰‹ã®ç´”ç²‹ãªæŠ€è¡“ã‚’è©•ä¾¡ï¼ˆé¦¬ã®å®ŸåŠ›ã‚’è£œæ­£ï¼‰"""
        scores = []
        
        for idx in range(len(group)):
            # é¨æ‰‹ã‚³ãƒ¼ãƒ‰ãŒãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            if pd.isna(group.iloc[idx]['kishu_code']) or group.iloc[idx]['kishu_code'] == '':
                scores.append(0.5)
                continue
                
            current_date = pd.to_datetime(
                str(int(group.iloc[idx]['kaisai_nen'])) + str(int(group.iloc[idx]['kaisai_tsukihi'])).zfill(4),
                format='%Y%m%d'
            )
            
            # 3ãƒ¶æœˆå‰ã®æ—¥ä»˜
            three_months_ago = current_date - pd.DateOffset(months=3)
            
            # éå»3ãƒ¶æœˆã®ãƒ¬ãƒ¼ã‚¹ã‚’æŠ½å‡ºï¼ˆæœªæ¥ã®ãƒ‡ãƒ¼ã‚¿ã¯è¦‹ãªã„ï¼ï¼‰
            past_races = group.iloc[:idx]
            
            if len(past_races) > 0:
                past_races = past_races.copy()
                past_races['kaisai_date'] = pd.to_datetime(
                    past_races['kaisai_nen'].astype(str) + past_races['kaisai_tsukihi'].astype(str).str.zfill(4),
                    format='%Y%m%d'
                )
                recent_races = past_races[past_races['kaisai_date'] >= three_months_ago]
                
                if len(recent_races) >= 3:  # æœ€ä½3ãƒ¬ãƒ¼ã‚¹å¿…è¦
                    # âœ… ä¿®æ­£: é¨æ‰‹ã®ç´”ç²‹ãªæˆç¸¾ã‚’è©•ä¾¡ï¼ˆé¦¬ã®å®ŸåŠ›è£œæ­£ã§ã¯ãªãã€é¨æ‰‹ã®å¹³å‡æˆç¸¾ï¼‰
                    # ç€é †ã‚’ã‚¹ã‚³ã‚¢åŒ–ï¼ˆ1ç€=1.0, 18ç€=0.0ï¼‰
                    recent_races['rank_score'] = 1.0 - ((18 - recent_races['kakutei_chakujun_numeric'] + 1) / 18.0)
                    
                    # é¨æ‰‹ã®å¹³å‡ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
                    avg_score = recent_races['rank_score'].mean()
                    
                    # 0-1ã®ç¯„å›²ã«ã‚¯ãƒªãƒƒãƒ—ï¼ˆæ—¢ã«ç¯„å›²å†…ã ãŒå¿µã®ãŸã‚ï¼‰
                    normalized_score = max(0.0, min(1.0, avg_score))
                    
                    scores.append(normalized_score)
                else:
                    scores.append(0.5)  # ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã¯ä¸­ç«‹
            else:
                scores.append(0.5)  # åˆå›ã¯ä¸­ç«‹
        
        return pd.Series(scores, index=group.index)
    
    df_sorted_kishu['kishu_skill_score'] = df_sorted_kishu.groupby('kishu_code', group_keys=False).apply(
        calc_kishu_skill_adjusted_score
    ).values
    
    # 2ï¸âƒ£ é¨æ‰‹ã®äººæ°—å·®ã‚¹ã‚³ã‚¢ï¼ˆã‚ªãƒƒã‚ºè£œæ­£ã€ç›´è¿‘3ãƒ¶æœˆï¼‰
    def calc_kishu_popularity_adjusted_score(group):
        """é¨æ‰‹ã®äººæ°—è£œæ­£ã‚¹ã‚³ã‚¢ï¼ˆäººæ°—ã‚ˆã‚Šä¸Šä½ã«æ¥ã‚Œã‚‹ã‹ï¼‰"""
        scores = []
        
        for idx in range(len(group)):
            if pd.isna(group.iloc[idx]['kishu_code']) or group.iloc[idx]['kishu_code'] == '':
                scores.append(0.5)
                continue
                
            current_date = pd.to_datetime(
                str(int(group.iloc[idx]['kaisai_nen'])) + str(int(group.iloc[idx]['kaisai_tsukihi'])).zfill(4),
                format='%Y%m%d'
            )
            
            three_months_ago = current_date - pd.DateOffset(months=3)
            
            past_races = group.iloc[:idx]
            
            if len(past_races) > 0:
                past_races = past_races.copy()
                past_races['kaisai_date'] = pd.to_datetime(
                    past_races['kaisai_nen'].astype(str) + past_races['kaisai_tsukihi'].astype(str).str.zfill(4),
                    format='%Y%m%d'
                )
                recent_races = past_races[past_races['kaisai_date'] >= three_months_ago]
                
                if len(recent_races) >= 3:
                    # ã‚ªãƒƒã‚ºãŒ0ã‚„ç•°å¸¸å€¤ã®å ´åˆã‚’é™¤å¤–
                    valid_races = recent_races[recent_races['tansho_odds'] > 0]
                    
                    if len(valid_races) >= 3:
                        # âœ… ä¿®æ­£: ã‚ªãƒƒã‚ºãƒ™ãƒ¼ã‚¹ã®æœŸå¾…æˆç¸¾ã¨å®Ÿéš›ã®æˆç¸¾ã‚’æ¯”è¼ƒ
                        # ã‚ªãƒƒã‚ºãŒä½ã„ = æœŸå¾…å€¤ãŒé«˜ã„ï¼ˆ1ã«è¿‘ã„ï¼‰
                        # ã‚ªãƒƒã‚ºãŒé«˜ã„ = æœŸå¾…å€¤ãŒä½ã„ï¼ˆ0ã«è¿‘ã„ï¼‰
                        max_odds = valid_races['tansho_odds'].max()
                        valid_races['odds_expectation'] = 1.0 - (valid_races['tansho_odds'] / (max_odds + 1.0))
                        
                        # å®Ÿéš›ã®æˆç¸¾ã‚¹ã‚³ã‚¢
                        valid_races['actual_score'] = 1.0 - ((18 - valid_races['kakutei_chakujun_numeric'] + 1) / 18.0)
                        
                        # æœŸå¾…ã‚’ä¸Šå›ã£ãŸåº¦åˆã„ï¼ˆãƒ—ãƒ©ã‚¹ãªã‚‰æœŸå¾…ä»¥ä¸Šï¼‰
                        valid_races['performance_diff'] = valid_races['actual_score'] - valid_races['odds_expectation']
                        
                        # å¹³å‡å·®åˆ†ã‚’ã‚¹ã‚³ã‚¢åŒ–ï¼ˆ0.5ãŒä¸­ç«‹ï¼‰
                        avg_diff = valid_races['performance_diff'].mean()
                        normalized_score = 0.5 + (avg_diff * 0.5)  # Â±0.5ã®ç¯„å›²ã«åã‚ã‚‹
                        normalized_score = max(0.0, min(1.0, normalized_score))
                        
                        scores.append(normalized_score)
                    else:
                        scores.append(0.5)
                else:
                    scores.append(0.5)
            else:
                scores.append(0.5)
        
        return pd.Series(scores, index=group.index)
    
    df_sorted_kishu['kishu_popularity_score'] = df_sorted_kishu.groupby('kishu_code', group_keys=False).apply(
        calc_kishu_popularity_adjusted_score
    ).values
    
    # 3ï¸âƒ£ é¨æ‰‹ã®èŠ/ãƒ€ãƒ¼ãƒˆåˆ¥ã‚¹ã‚³ã‚¢ï¼ˆé¦¬å ´é©æ€§è€ƒæ…®ã€ç›´è¿‘6ãƒ¶æœˆï¼‰
    def calc_kishu_surface_score(group):
        """é¨æ‰‹ã®é¦¬å ´ã‚¿ã‚¤ãƒ—åˆ¥ç›´è¿‘6ãƒ¶æœˆæˆç¸¾"""
        scores = []
        
        for idx in range(len(group)):
            if pd.isna(group.iloc[idx]['kishu_code']) or group.iloc[idx]['kishu_code'] == '':
                scores.append(0.5)
                continue
                
            current_date = pd.to_datetime(
                str(int(group.iloc[idx]['kaisai_nen'])) + str(int(group.iloc[idx]['kaisai_tsukihi'])).zfill(4),
                format='%Y%m%d'
            )
            current_surface = group.iloc[idx]['surface_type']
            
            six_months_ago = current_date - pd.DateOffset(months=6)
            
            past_races = group.iloc[:idx]
            
            if len(past_races) > 0:
                past_races = past_races.copy()
                past_races['kaisai_date'] = pd.to_datetime(
                    past_races['kaisai_nen'].astype(str) + past_races['kaisai_tsukihi'].astype(str).str.zfill(4),
                    format='%Y%m%d'
                )
                # åŒã˜é¦¬å ´ã‚¿ã‚¤ãƒ—ã§ã®ç›´è¿‘6ãƒ¶æœˆ
                recent_same_surface = past_races[
                    (past_races['kaisai_date'] >= six_months_ago) &
                    (past_races['surface_type'] == current_surface)
                ]
                
                if len(recent_same_surface) >= 5:  # æœ€ä½5ãƒ¬ãƒ¼ã‚¹å¿…è¦
                    avg_score = (1 - ((18 - recent_same_surface['kakutei_chakujun_numeric'] + 1) / 18.0)).mean()
                    scores.append(avg_score)
                else:
                    scores.append(0.5)
            else:
                scores.append(0.5)
        
        return pd.Series(scores, index=group.index)
    
    df_sorted_kishu['kishu_surface_score'] = df_sorted_kishu.groupby('kishu_code', group_keys=False).apply(
        calc_kishu_surface_score
    ).values
    
    # âœ… ä¿®æ­£: race_bangoã‚’è¿½åŠ ã—ã¦æ™‚ç³»åˆ—ãƒªãƒ¼ã‚¯ã‚’é˜²æ­¢
    df_sorted_chokyoshi = df.sort_values(['chokyoshi_code', 'kaisai_nen', 'kaisai_tsukihi', 'race_bango']).copy()
    
    # 4ï¸âƒ£ èª¿æ•™å¸«ã®ç›´è¿‘3ãƒ¶æœˆæˆç¸¾ã‚¹ã‚³ã‚¢
    def calc_chokyoshi_recent_score(group):
        """èª¿æ•™å¸«ã®ç›´è¿‘3ãƒ¶æœˆæˆç¸¾"""
        scores = []
        
        for idx in range(len(group)):
            if pd.isna(group.iloc[idx]['chokyoshi_code']) or group.iloc[idx]['chokyoshi_code'] == '':
                scores.append(0.5)
                continue
                
            current_date = pd.to_datetime(
                str(int(group.iloc[idx]['kaisai_nen'])) + str(int(group.iloc[idx]['kaisai_tsukihi'])).zfill(4),
                format='%Y%m%d'
            )
            
            three_months_ago = current_date - pd.DateOffset(months=3)
            
            past_races = group.iloc[:idx]
            
            if len(past_races) > 0:
                past_races = past_races.copy()
                past_races['kaisai_date'] = pd.to_datetime(
                    past_races['kaisai_nen'].astype(str) + past_races['kaisai_tsukihi'].astype(str).str.zfill(4),
                    format='%Y%m%d'
                )
                recent_races = past_races[past_races['kaisai_date'] >= three_months_ago]
                
                if len(recent_races) >= 5:  # âœ… ä¿®æ­£: 5ãƒ¬ãƒ¼ã‚¹ã«å¤‰æ›´ï¼ˆ10ãƒ¬ãƒ¼ã‚¹ã§ã¯å¤§éƒ¨åˆ†ãŒä¸­ç«‹å€¤ã«ãªã‚‹ï¼‰
                    avg_score = (1 - ((18 - recent_races['kakutei_chakujun_numeric'] + 1) / 18.0)).mean()
                    scores.append(avg_score)
                else:
                    scores.append(0.5)
            else:
                scores.append(0.5)
        
        return pd.Series(scores, index=group.index)
    
    df_sorted_chokyoshi['chokyoshi_recent_score'] = df_sorted_chokyoshi.groupby('chokyoshi_code', group_keys=False).apply(
        calc_chokyoshi_recent_score
    ).values
    
    # å…ƒã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹é †ã«æˆ»ã™
    df['kishu_skill_score'] = df_sorted_kishu.sort_index()['kishu_skill_score']
    df['kishu_popularity_score'] = df_sorted_kishu.sort_index()['kishu_popularity_score']
    df['kishu_surface_score'] = df_sorted_kishu.sort_index()['kishu_surface_score']
    df['chokyoshi_recent_score'] = df_sorted_chokyoshi.sort_index()['chokyoshi_recent_score']
    
    # ç‰¹å¾´é‡ã«è¿½åŠ 
    X['kishu_skill_score'] = df['kishu_skill_score']
    X['kishu_popularity_score'] = df['kishu_popularity_score']
    X['kishu_surface_score'] = df['kishu_surface_score']
    X['chokyoshi_recent_score'] = df['chokyoshi_recent_score']
    
    print(f"âœ… é¨æ‰‹ãƒ»èª¿æ•™å¸«ã‚¹ã‚³ã‚¢ã‚’4ç¨®é¡è¿½åŠ ã—ã¾ã—ãŸï¼")
    print(f"  - kishu_skill_score: é¨æ‰‹ã®å®ŸåŠ›è£œæ­£ã‚¹ã‚³ã‚¢ï¼ˆé¦¬ã®å®ŸåŠ›ã‚’è€ƒæ…®ã€ç›´è¿‘3ãƒ¶æœˆï¼‰")
    print(f"  - kishu_popularity_score: é¨æ‰‹ã®äººæ°—å·®ã‚¹ã‚³ã‚¢ï¼ˆã‚ªãƒƒã‚ºè£œæ­£ã€ç›´è¿‘3ãƒ¶æœˆï¼‰")
    print(f"  - kishu_surface_score: é¨æ‰‹ã®èŠ/ãƒ€ãƒ¼ãƒˆåˆ¥é©æ€§ï¼ˆç›´è¿‘6ãƒ¶æœˆï¼‰")
    print(f"  - chokyoshi_recent_score: èª¿æ•™å¸«ã®ç›´è¿‘æˆç¸¾ï¼ˆç›´è¿‘3ãƒ¶æœˆï¼‰")

    # éå»ãƒ¬ãƒ¼ã‚¹ã§ã€Œäººæ°—è–„ãªã®ã«å¥½èµ°ã—ãŸå›æ•°ã€
    # df['upset_count'] = df.groupby('ketto_toroku_bango').apply(
    #     lambda g: ((g['tansho_ninkijun_numeric'] >= 5) & (g['kakutei_chakujun_numeric'] <= 3)).sum()
    # )
    # X['upset_count'] = df['upset_count']

    # # ç ”ç©¶ç”¨ç‰¹å¾´é‡ è¿½åŠ 
    
    # ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã‚’ä½œæˆ
    # X['kyori'] = X['kyori'].astype('category')
    # X['tenko_code'] = X['tenko_code'].astype('category')
    # X['babajotai_code'] = X['babajotai_code'].astype('category')
    # X['seibetsu_code'] = X['seibetsu_code'].astype('category')
    categorical_features = []

    #ç›®çš„å¤‰æ•°ã‚’è¨­å®š
    y = df['kakutei_chakujun_numeric'].astype(int)

    # ãƒ¬ãƒ¼ã‚¹ã”ã¨ã®ã‚°ãƒ«ãƒ¼ãƒ—ã‚’æ­£ã—ãè¨ˆç®—
    df['group_id'] = df.groupby(['kaisai_nen', 'kaisai_tsukihi', 'race_bango']).ngroup()
    groups = df['group_id'].values
    print(f"ã‚°ãƒ«ãƒ¼ãƒ—æ•°: {len(set(groups))}")  # ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªã‚°ãƒ«ãƒ¼ãƒ—ã®æ•°
    print(f"ãƒ‡ãƒ¼ã‚¿æ•°: {len(groups)}")  # å…¨ãƒ‡ãƒ¼ã‚¿æ•°

    # ãƒ‡ãƒ¼ã‚¿ã¨ã‚°ãƒ«ãƒ¼ãƒ—æ•°ãŒä¸€è‡´ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª
    if len(groups) != len(X):
        raise ValueError(f"ãƒ‡ãƒ¼ã‚¿ä»¶æ•°({len(X)})ã¨ã‚°ãƒ«ãƒ¼ãƒ—ã®æ•°({len(groups)})ãŒä¸€è‡´ã—ã¾ã›ã‚“ï¼")

    # ğŸ”¥æ”¹å–„1: æ™‚ç³»åˆ—åˆ†å‰²ã‚’å¹´å˜ä½ã§æ˜ç¢ºåŒ–ğŸ”¥
    # å¹´å˜ä½ã§è¨“ç·´/ãƒ†ã‚¹ãƒˆã‚’åˆ†å‰²ï¼ˆã°ã‚‰ã¤ãå‰Šæ¸›ã®ãŸã‚ï¼‰
    # ä¾‹: 2013-2020å¹´ã‚’è¨“ç·´ã€2021-2022å¹´ã‚’ãƒ†ã‚¹ãƒˆ
    
    # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®å¹´ç¯„å›²ã‹ã‚‰è¨“ç·´/ãƒ†ã‚¹ãƒˆå¹´ã‚’è¨ˆç®—
    all_years = sorted(df['kaisai_nen'].unique())
    total_years = len(all_years)
    
    # 75%ã‚’è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã«ï¼ˆå¹´å˜ä½ã§ï¼‰
    train_year_count = int(total_years * 0.75)
    train_years = all_years[:train_year_count]
    test_years = all_years[train_year_count:]
    
    print(f"ğŸ“… è¨“ç·´ãƒ‡ãƒ¼ã‚¿å¹´: {train_years} ({len(train_years)}å¹´é–“)")
    print(f"ğŸ“… ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿å¹´: {test_years} ({len(test_years)}å¹´é–“)")
    
    # å¹´å˜ä½ã§ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’åˆ†å‰²
    train_indices = df[df['kaisai_nen'].isin(train_years)].index
    test_indices = df[df['kaisai_nen'].isin(test_years)].index
    
    # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
    X_train = X.loc[train_indices]
    X_test = X.loc[test_indices]
    y_train = y.loc[train_indices]
    y_test = y.loc[test_indices]
    groups_train = groups[train_indices]
    groups_test = groups[test_indices]
    
    print(f"âœ… è¨“ç·´ãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(X_train)}ä»¶")
    print(f"âœ… ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(X_test)}ä»¶")

    # Optunaã®objectiveé–¢æ•°
    def objective(trial):
        param = {
            'objective': 'lambdarank',
            'metric': 'ndcg',                                                              # ä¸Šä½ã®ä¸¦ã³é †ã‚’é‡è¦–
            'ndcg_eval_at': [1, 3, 5],
            'boosting_type': 'gbdt',
            'verbosity': -1,
            'random_state': 42,
            'num_leaves': trial.suggest_int('num_leaves', 20, 100),
            'max_depth': trial.suggest_int('max_depth', 3, 8),
            'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.3),
            'n_estimators': trial.suggest_int('n_estimators', 200, 1000),
            'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),
            'subsample': trial.suggest_uniform('subsample', 0.7, 1.0),
            'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),
            'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-4, 10.0),
            'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-4, 10.0),
            'subsample_freq': trial.suggest_int('subsample_freq', 1, 5)
        }

        # ğŸ”¥ä¿®æ­£: ã‚°ãƒ«ãƒ¼ãƒ—ã‚µã‚¤ã‚ºã‚’æ­£ã—ã„é †åºã§è¨ˆç®—ï¼ˆãƒ‡ãƒ¼ã‚¿ã®é †åºã‚’ä¿æŒï¼‰
        # sort=Falseã§å…ƒã®ãƒ‡ãƒ¼ã‚¿é †ã‚’ç¶­æŒã—ãªãŒã‚‰ã‚°ãƒ«ãƒ¼ãƒ—ã‚µã‚¤ã‚ºã‚’æŠ½å‡º
        train_df_with_group = pd.DataFrame({'group': groups_train}).reset_index(drop=True)
        train_group_sizes = train_df_with_group.groupby('group', sort=False).size().values
        
        test_df_with_group = pd.DataFrame({'group': groups_test}).reset_index(drop=True)
        test_group_sizes = test_df_with_group.groupby('group', sort=False).size().values
        
        dtrain = lgb.Dataset(X_train, label=y_train, group=train_group_sizes, categorical_feature=categorical_features)
        dvalid = lgb.Dataset(X_test, label=y_test, group=test_group_sizes, categorical_feature=categorical_features)

        tmp_model = lgb.train(
            param,
            dtrain,
            valid_sets=[dvalid],
            valid_names=['valid'],
            callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)]
        )

        # ğŸ”¥ä¿®æ­£: ãƒ¬ãƒ¼ã‚¹ã”ã¨ã«NDCGã‚’è¨ˆç®—ã—ã¦å¹³å‡ã‚’è¿”ã™ï¼ˆæ­£ã—ã„è©•ä¾¡æ–¹æ³•ï¼‰
        preds = tmp_model.predict(X_test, num_iteration=tmp_model.best_iteration)
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ãƒ¬ãƒ¼ã‚¹ã”ã¨ã«åˆ†å‰²ã—ã¦NDCGã‚’è¨ˆç®—
        ndcg_scores = []
        start_idx = 0
        for group_size in test_group_sizes:
            end_idx = start_idx + group_size
            y_true_group = y_test.iloc[start_idx:end_idx].values
            y_pred_group = preds[start_idx:end_idx]
            
            # ãƒ¬ãƒ¼ã‚¹å†…ã«2é ­ä»¥ä¸Šã„ã‚‹å ´åˆã®ã¿NDCGè¨ˆç®—ï¼ˆ1é ­ã ã¨ã‚¨ãƒ©ãƒ¼ã«ãªã‚‹ï¼‰
            if len(y_true_group) > 1:
                # 2æ¬¡å…ƒé…åˆ—ã¨ã—ã¦æ¸¡ã™
                ndcg = ndcg_score([y_true_group], [y_pred_group], k=5)
                ndcg_scores.append(ndcg)
            
            start_idx = end_idx
        
        # å…¨ãƒ¬ãƒ¼ã‚¹ã®NDCGå¹³å‡ã‚’è¿”ã™
        return np.mean(ndcg_scores) if ndcg_scores else 0.0

    # ğŸ’¡æ¨å¥¨: æœ¬ç•ªé‹ç”¨å‰ã«ä¸Šä½ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¤‡æ•°ã‚·ãƒ¼ãƒ‰ï¼ˆrandom_state=42,43,44...ï¼‰ã§
    #        å†å­¦ç¿’ã—ã€NDCG/ROI/çš„ä¸­ç‡ã®å®‰å®šæ€§ã‚’ç¢ºèªã™ã‚‹ã“ã¨ã‚’å¼·ãæ¨å¥¨
    # TODO å°†æ¥ã®æ”¹å–„: lgb.Dataset ã® weight ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ã‚ªãƒƒã‚ºãƒ™ãƒ¼ã‚¹ã®é‡ã¿ã‚’å°å…¥ã—ã€
    #      ç©´é¦¬ï¼ˆé«˜ã‚ªãƒƒã‚ºé¦¬ï¼‰ã®äºˆæ¸¬ç²¾åº¦ã‚’å‘ä¸Šã•ã›ã‚‹ã“ã¨ã§ROIæœ€é©åŒ–ã‚’å›³ã‚‹
    
    # ğŸ”¥æ”¹å–„2: Optunaã®ã‚·ãƒ¼ãƒ‰å›ºå®šï¼ˆå†ç¾æ€§å‘ä¸Šã®ãŸã‚ï¼‰ğŸ”¥
    print("ğŸ” ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–ã‚’é–‹å§‹...")
    sampler = optuna.samplers.TPESampler(seed=42)  # ã‚·ãƒ¼ãƒ‰ã‚’å›ºå®š
    study = optuna.create_study(direction="maximize", sampler=sampler)
    study.optimize(objective, n_trials=50)

    print('Best trial:')
    print(study.best_trial.params)

    # æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½¿ã£ã¦å†å­¦ç¿’
    best_params = study.best_trial.params

    best_params.update({
        'objective': 'lambdarank',
        'metric': 'ndcg',
        'boosting_type': 'gbdt',
        'verbosity': 0,  # å­¦ç¿’ã®é€²æ—ã‚’è¡¨ç¤º
        'random_state': 42,
    })

    # ğŸ”¥ä¿®æ­£: ã‚°ãƒ«ãƒ¼ãƒ—ãƒ‡ãƒ¼ã‚¿ã‚’æ­£ã—ãæº–å‚™ï¼ˆãƒ‡ãƒ¼ã‚¿ã®é †åºã‚’ä¿æŒï¼‰
    # ãƒ¬ãƒ¼ã‚¹ã”ã¨ã®å‡ºèµ°é ­æ•°ã‚’è¨ˆç®—ï¼ˆsort=Falseã§å…ƒã®é †åºã‚’ç¶­æŒï¼‰
    train_df_with_group = pd.DataFrame({'group': groups_train}).reset_index(drop=True)
    train_group_sizes = train_df_with_group.groupby('group', sort=False).size().values
    
    test_df_with_group = pd.DataFrame({'group': groups_test}).reset_index(drop=True)
    test_group_sizes = test_df_with_group.groupby('group', sort=False).size().values
    
    print(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ãƒ¬ãƒ¼ã‚¹æ•°: {len(train_group_sizes)}")
    print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ãƒ¬ãƒ¼ã‚¹æ•°: {len(test_group_sizes)}")
    
    # LightGBMç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ
    dtrain = lgb.Dataset(X_train, label=y_train, group=train_group_sizes, categorical_feature=categorical_features)
    dvalid = lgb.Dataset(X_test, label=y_test, group=test_group_sizes, categorical_feature=categorical_features)

    # æœ€é©åŒ–ã•ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’
    print("ğŸ”¥ æœ€é©åŒ–ã•ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã™ã‚‹ã‚ˆï¼")
    model = lgb.train(
        best_params,
        dtrain,
        valid_sets=[dvalid],
        valid_names=['ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿'],
        num_boost_round=1000,  # æœ€å¤§åå¾©å›æ•°
        callbacks=[
            lgb.early_stopping(50),  # 50å›æ”¹å–„ãŒãªã‘ã‚Œã°æ—©æœŸçµ‚äº†ï¼ˆå­¦ç¿’ã®å®‰å®šåŒ–ï¼‰
            lgb.log_evaluation(10)   # 10å›ã”ã¨ã«çµæœè¡¨ç¤º
        ]
    )

    # ç‰¹å¾´é‡ã®é‡è¦åº¦ã‚’ç¢ºèªã™ã‚‹ï¼ˆãƒ¢ãƒ‡ãƒ«ãŒã©ã®æƒ…å ±ã‚’é‡è¦–ã—ã¦ã„ã‚‹ã‹ï¼‰
    importance = model.feature_importance()
    feature_names = X.columns
    feature_importance = pd.DataFrame({'feature': feature_names, 'importance': importance})
    feature_importance = feature_importance.sort_values('importance', ascending=False)
    print("ç‰¹å¾´é‡ã®é‡è¦åº¦:")
    print(feature_importance)

    # ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã™ã‚‹
    model_filepath = output_path / model_filename
    pickle.dump(model, open(model_filepath, 'wb'))
    print(f"âœ… ãƒ¢ãƒ‡ãƒ«ã‚’ {model_filepath} ã«ä¿å­˜ã—ã¾ã—ãŸ")

    # ã‚³ãƒã‚¯ã‚·ãƒ§ãƒ³ã‚’ã‚¯ãƒ­ãƒ¼ã‚º
    conn.close()


# æ—§æ¥ã®é–¢æ•°ã‚’ç¶­æŒï¼ˆäº’æ›æ€§ã®ãŸã‚ï¼‰
def make_model():
    """
    æ—§ãƒãƒ¼ã‚¸ãƒ§ãƒ³äº’æ›æ€§ã®ãŸã‚ã®é–¢æ•°
    é˜ªç¥ç«¶é¦¬å ´ã®ï¼“æ­³ä»¥ä¸ŠèŠä¸­é•·è·é›¢ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
    
    æ³¨æ„: äº’æ›æ€§ã®ãŸã‚ã€ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜ã•ã‚Œã¾ã™
    """
    create_universal_model(
        track_code='09',  # é˜ªç¥
        kyoso_shubetsu_code='13',  # 3æ­³ä»¥ä¸Š
        surface_type='turf',  # èŠ
        min_distance=1700,  # ä¸­é•·è·é›¢
        max_distance=9999,  # ä¸Šé™ãªã—
        model_filename='hanshin_shiba_3ageup_model.sav',
        output_dir='.'  # ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜ï¼ˆæ—¢å­˜ã®å‹•ä½œã‚’ç¶­æŒï¼‰
    )


if __name__ == '__main__':
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œï¼šæ—§ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¨åŒã˜ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
    make_model()