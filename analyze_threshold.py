"""
ã‚¹ã‚³ã‚¢å·®é–¾å€¤ã®æœ€é©åŒ–åˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path

def analyze_score_diff_distribution():
    """ã‚¹ã‚³ã‚¢å·®ã®åˆ†å¸ƒã‚’åˆ†æ"""
    print("=" * 80)
    print("[ANALYZE] ã‚¹ã‚³ã‚¢å·®ã®åˆ†å¸ƒåˆ†æ")
    print("=" * 80)
    
    # predicted_results_skipped.tsvã‹ã‚‰ã‚¹ã‚³ã‚¢å·®ã‚’å–å¾—
    skipped_file = Path("results/predicted_results_skipped.tsv")
    
    if not skipped_file.exists():
        print(f"[ERROR] ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {skipped_file}")
        return
    
    df = pd.read_csv(skipped_file, sep='\t', encoding='utf-8-sig')
    
    print(f"\n[DATA] ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(df)}")
    print(f"[DATA] ã‚«ãƒ©ãƒ æ•°: {len(df.columns)}")
    
    # ã‚¹ã‚³ã‚¢å·®åˆ—ã®ç¢ºèª
    score_diff_col = None
    for col in ['ã‚¹ã‚³ã‚¢å·®', 'score_diff']:
        if col in df.columns:
            score_diff_col = col
            break
    
    if score_diff_col is None:
        print("[ERROR] ã‚¹ã‚³ã‚¢å·®åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return
    
    # ãƒ¬ãƒ¼ã‚¹å˜ä½ã§ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªã‚¹ã‚³ã‚¢å·®ã‚’å–å¾—
    # ï¼ˆåŒã˜ãƒ¬ãƒ¼ã‚¹ã®å…¨é¦¬ãŒåŒã˜ã‚¹ã‚³ã‚¢å·®ã‚’æŒã¤ã®ã§ã€ãƒ¬ãƒ¼ã‚¹IDã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ï¼‰
    race_id_cols = ['ç«¶é¦¬å ´', 'é–‹å‚¬å¹´', 'é–‹å‚¬æ—¥', 'ãƒ¬ãƒ¼ã‚¹ç•ªå·']
    df_races = df.groupby(race_id_cols)[score_diff_col].first().reset_index()
    
    score_diffs = df_races[score_diff_col].dropna()
    
    print(f"\n[STATS] ã‚¹ã‚³ã‚¢å·®ã®åŸºæœ¬çµ±è¨ˆ")
    print(f"  - ãƒ¬ãƒ¼ã‚¹æ•°: {len(score_diffs)}")
    print(f"  - å¹³å‡: {score_diffs.mean():.6f}")
    print(f"  - ä¸­å¤®å€¤: {score_diffs.median():.6f}")
    print(f"  - æ¨™æº–åå·®: {score_diffs.std():.6f}")
    print(f"  - æœ€å°å€¤: {score_diffs.min():.6f}")
    print(f"  - æœ€å¤§å€¤: {score_diffs.max():.6f}")
    
    # ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«
    print(f"\n[PERCENTILE] ã‚¹ã‚³ã‚¢å·®ã®åˆ†ä½ç‚¹")
    for p in [10, 25, 50, 75, 90, 95, 99]:
        val = score_diffs.quantile(p/100)
        print(f"  - {p:2d}%ç‚¹: {val:.6f}")
    
    # ç¾åœ¨ã®é–¾å€¤ã§ã©ã‚Œã ã‘ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã•ã‚Œã‚‹ã‹
    thresholds = [0.01, 0.02, 0.03, 0.05, 0.08, 0.10]
    print(f"\n[FILTER] å„é–¾å€¤ã§ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ç‡")
    print(f"{'é–¾å€¤':>8s} {'ã‚¹ã‚­ãƒƒãƒ—æ•°':>10s} {'æ®‹å­˜æ•°':>10s} {'ã‚¹ã‚­ãƒƒãƒ—ç‡':>10s}")
    print("-" * 45)
    for threshold in thresholds:
        skipped = (score_diffs < threshold).sum()
        remained = (score_diffs >= threshold).sum()
        skip_rate = skipped / len(score_diffs) * 100
        marker = " â† ç¾åœ¨" if threshold == 0.05 else ""
        print(f"{threshold:8.2f} {skipped:10d} {remained:10d} {skip_rate:9.1f}%{marker}")
    
    # ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ä½œæˆ
    plt.figure(figsize=(12, 6))
    plt.hist(score_diffs, bins=50, edgecolor='black', alpha=0.7)
    plt.axvline(x=0.05, color='red', linestyle='--', linewidth=2, label='ç¾åœ¨ã®é–¾å€¤ (0.05)')
    plt.xlabel('ã‚¹ã‚³ã‚¢å·®ï¼ˆ1ä½ - 2ä½ï¼‰', fontsize=12)
    plt.ylabel('ãƒ¬ãƒ¼ã‚¹æ•°', fontsize=12)
    plt.title('äºˆæ¸¬ã‚¹ã‚³ã‚¢å·®ã®åˆ†å¸ƒ', fontsize=14)
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    output_dir = Path('results')
    output_dir.mkdir(exist_ok=True)
    output_file = output_dir / 'score_diff_distribution.png'
    plt.savefig(output_file, dpi=150, bbox_inches='tight')
    print(f"\n[FILE] ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ã‚’ä¿å­˜: {output_file}")
    plt.close()
    
    return df_races


def analyze_threshold_vs_accuracy(all_results_file="results/predicted_results_all.tsv"):
    """å„é–¾å€¤ã§ã®çš„ä¸­ç‡ãƒ»å›åç‡ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"""
    print("\n" + "=" * 80)
    print("[OPTIMIZE] é–¾å€¤ã¨çš„ä¸­ç‡ã®é–¢ä¿‚åˆ†æ")
    print("=" * 80)
    
    results_file = Path(all_results_file)
    
    if not results_file.exists():
        print(f"[ERROR] ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {results_file}")
        print("[INFO] å…ˆã«python universal_test.py multi 2023ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
        return
    
    df = pd.read_csv(results_file, sep='\t', encoding='utf-8-sig')
    
    print(f"\n[DATA] ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(df)}")
    
    # å¿…è¦ãªåˆ—ã®ç¢ºèª
    required_cols = ['ç«¶é¦¬å ´', 'é–‹å‚¬å¹´', 'é–‹å‚¬æ—¥', 'ãƒ¬ãƒ¼ã‚¹ç•ªå·', 'äºˆæ¸¬é †ä½', 'ç¢ºå®šç€é †', 'äºˆæ¸¬ã‚¹ã‚³ã‚¢']
    missing_cols = [col for col in required_cols if col not in df.columns]
    
    if missing_cols:
        print(f"[ERROR] å¿…è¦ãªåˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {missing_cols}")
        return
    
    # ãƒ¬ãƒ¼ã‚¹ã”ã¨ã«ã‚¹ã‚³ã‚¢å·®ã‚’è¨ˆç®—
    race_id_cols = ['ç«¶é¦¬å ´', 'é–‹å‚¬å¹´', 'é–‹å‚¬æ—¥', 'ãƒ¬ãƒ¼ã‚¹ç•ªå·']
    
    def calc_score_diff(race_df):
        sorted_df = race_df.sort_values('äºˆæ¸¬ã‚¹ã‚³ã‚¢', ascending=False)
        if len(sorted_df) >= 2:
            return sorted_df.iloc[0]['äºˆæ¸¬ã‚¹ã‚³ã‚¢'] - sorted_df.iloc[1]['äºˆæ¸¬ã‚¹ã‚³ã‚¢']
        return 0.0
    
    df['ã‚¹ã‚³ã‚¢å·®'] = df.groupby(race_id_cols, group_keys=False).apply(
        lambda x: pd.Series(calc_score_diff(x), index=x.index)
    )
    
    # äºˆæ¸¬1ä½ã®é¦¬ã®ã¿ã‚’æŠ½å‡º
    df_top1 = df[df['äºˆæ¸¬é †ä½'] == 1].copy()
    
    print(f"\n[DATA] äºˆæ¸¬1ä½ã®é¦¬: {len(df_top1)}ãƒ¬ãƒ¼ã‚¹")
    
    # å„é–¾å€¤ã§ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    thresholds = [0.00, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.08, 0.10]
    results = []
    
    print(f"\n[SIMULATE] å„é–¾å€¤ã§ã®çš„ä¸­ç‡ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³")
    print(f"{'é–¾å€¤':>8s} {'å¯¾è±¡æ•°':>8s} {'çš„ä¸­æ•°':>8s} {'çš„ä¸­ç‡':>10s} {'åˆ¤å®š':>10s}")
    print("-" * 55)
    
    for threshold in thresholds:
        # é–¾å€¤ä»¥ä¸Šã®ãƒ¬ãƒ¼ã‚¹ã®ã¿ã‚’å¯¾è±¡
        df_filtered = df_top1[df_top1['ã‚¹ã‚³ã‚¢å·®'] >= threshold]
        
        if len(df_filtered) == 0:
            continue
        
        # çš„ä¸­åˆ¤å®šï¼ˆäºˆæ¸¬1ä½ãŒå®Ÿéš›ã«1ç€ï¼‰
        hits = (df_filtered['ç¢ºå®šç€é †'] == 1).sum()
        total = len(df_filtered)
        accuracy = hits / total * 100 if total > 0 else 0
        
        # åˆ¤å®š
        if accuracy >= 30:
            judgment = "âœ… å„ªç§€"
        elif accuracy >= 25:
            judgment = "â­• è‰¯å¥½"
        else:
            judgment = "âš ï¸  è¦æ”¹å–„"
        
        marker = " â† ç¾åœ¨" if threshold == 0.05 else ""
        print(f"{threshold:8.2f} {total:8d} {hits:8d} {accuracy:9.1f}% {judgment}{marker}")
        
        results.append({
            'threshold': threshold,
            'total_races': total,
            'hits': hits,
            'accuracy': accuracy
        })
    
    # ã‚°ãƒ©ãƒ•ä½œæˆ
    results_df = pd.DataFrame(results)
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
    
    # çš„ä¸­ç‡ã®ã‚°ãƒ©ãƒ•
    ax1.plot(results_df['threshold'], results_df['accuracy'], marker='o', linewidth=2, markersize=8)
    ax1.axvline(x=0.05, color='red', linestyle='--', linewidth=2, label='ç¾åœ¨ã®é–¾å€¤ (0.05)')
    ax1.axhline(y=25, color='green', linestyle=':', linewidth=1, label='ç›®æ¨™: 25%')
    ax1.set_xlabel('ã‚¹ã‚³ã‚¢å·®é–¾å€¤', fontsize=12)
    ax1.set_ylabel('çš„ä¸­ç‡ (%)', fontsize=12)
    ax1.set_title('é–¾å€¤ã¨çš„ä¸­ç‡ã®é–¢ä¿‚', fontsize=14)
    ax1.grid(True, alpha=0.3)
    ax1.legend()
    
    # å¯¾è±¡ãƒ¬ãƒ¼ã‚¹æ•°ã®ã‚°ãƒ©ãƒ•
    ax2.plot(results_df['threshold'], results_df['total_races'], marker='s', linewidth=2, markersize=8, color='orange')
    ax2.axvline(x=0.05, color='red', linestyle='--', linewidth=2, label='ç¾åœ¨ã®é–¾å€¤ (0.05)')
    ax2.set_xlabel('ã‚¹ã‚³ã‚¢å·®é–¾å€¤', fontsize=12)
    ax2.set_ylabel('å¯¾è±¡ãƒ¬ãƒ¼ã‚¹æ•°', fontsize=12)
    ax2.set_title('é–¾å€¤ã¨å¯¾è±¡ãƒ¬ãƒ¼ã‚¹æ•°ã®é–¢ä¿‚', fontsize=14)
    ax2.grid(True, alpha=0.3)
    ax2.legend()
    
    plt.tight_layout()
    
    output_dir = Path('results')
    output_file = output_dir / 'threshold_optimization.png'
    plt.savefig(output_file, dpi=150, bbox_inches='tight')
    print(f"\n[FILE] æœ€é©åŒ–ã‚°ãƒ©ãƒ•ã‚’ä¿å­˜: {output_file}")
    plt.close()
    
    # æ¨å¥¨é–¾å€¤ã‚’ææ¡ˆ
    print("\n" + "=" * 80)
    print("[RECOMMEND] æ¨å¥¨é–¾å€¤")
    print("=" * 80)
    
    # çš„ä¸­ç‡ãŒæœ€å¤§ã®é–¾å€¤
    best_accuracy_idx = results_df['accuracy'].idxmax()
    best_threshold = results_df.loc[best_accuracy_idx, 'threshold']
    best_accuracy = results_df.loc[best_accuracy_idx, 'accuracy']
    best_races = results_df.loc[best_accuracy_idx, 'total_races']
    
    print(f"\nâœ… çš„ä¸­ç‡æœ€å¤§: é–¾å€¤={best_threshold:.2f}, çš„ä¸­ç‡={best_accuracy:.1f}%, å¯¾è±¡={best_races}ãƒ¬ãƒ¼ã‚¹")
    
    # çš„ä¸­ç‡25%ä»¥ä¸Šã§æœ€ã‚‚ãƒ¬ãƒ¼ã‚¹æ•°ãŒå¤šã„é–¾å€¤
    good_results = results_df[results_df['accuracy'] >= 25]
    if len(good_results) > 0:
        best_balance_idx = good_results['total_races'].idxmax()
        balance_threshold = good_results.loc[best_balance_idx, 'threshold']
        balance_accuracy = good_results.loc[best_balance_idx, 'accuracy']
        balance_races = good_results.loc[best_balance_idx, 'total_races']
        
        print(f"â­• ãƒãƒ©ãƒ³ã‚¹å‹: é–¾å€¤={balance_threshold:.2f}, çš„ä¸­ç‡={balance_accuracy:.1f}%, å¯¾è±¡={balance_races}ãƒ¬ãƒ¼ã‚¹")
    
    # ç¾åœ¨ã®é–¾å€¤(0.05)ã®è©•ä¾¡
    current_result = results_df[results_df['threshold'] == 0.05]
    if len(current_result) > 0:
        current_accuracy = current_result.iloc[0]['accuracy']
        current_races = current_result.iloc[0]['total_races']
        
        print(f"\nğŸ“Š ç¾åœ¨ã®é–¾å€¤(0.05): çš„ä¸­ç‡={current_accuracy:.1f}%, å¯¾è±¡={current_races}ãƒ¬ãƒ¼ã‚¹")
        
        if current_accuracy < best_accuracy - 2:
            print(f"âš ï¸  ç¾åœ¨ã®é–¾å€¤ã¯æœ€é©ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚é–¾å€¤ã‚’{best_threshold:.2f}ã«å¤‰æ›´ã™ã‚‹ã“ã¨ã§çš„ä¸­ç‡ãŒ{best_accuracy - current_accuracy:.1f}%å‘ä¸Šã—ã¾ã™ã€‚")
        else:
            print(f"âœ… ç¾åœ¨ã®é–¾å€¤ã¯æ¦‚ã­é©åˆ‡ã§ã™ã€‚")
    
    return results_df


if __name__ == "__main__":
    # Phase 1: ã‚¹ã‚³ã‚¢å·®ã®åˆ†å¸ƒã‚’åˆ†æ
    df_races = analyze_score_diff_distribution()
    
    # Phase 2: é–¾å€¤ã¨çš„ä¸­ç‡ã®é–¢ä¿‚ã‚’åˆ†æ
    results_df = analyze_threshold_vs_accuracy()
    
    print("\n" + "=" * 80)
    print("[DONE] åˆ†æå®Œäº†ï¼")
    print("=" * 80)
    print("\næ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
    print("1. results/score_diff_distribution.png ã‚’ç¢ºèª")
    print("2. results/threshold_optimization.png ã‚’ç¢ºèª")
    print("3. æ¨å¥¨é–¾å€¤ã‚’universal_test.pyã®min_score_diffã«è¨­å®š")
    print("4. å†åº¦python universal_test.py multi 2023ã‚’å®Ÿè¡Œã—ã¦åŠ¹æœã‚’æ¤œè¨¼")
