#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SHAPåˆ†æã«ã‚ˆã‚‹ç«¶é¦¬äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«èª¬æ˜ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ç†ç”±ã‚’SHAPã§å¯è¦–åŒ–ãƒ»åˆ†æã—ã¾ã™ã€‚
- å€‹åˆ¥ãƒ¬ãƒ¼ã‚¹ã®äºˆæ¸¬ç†ç”±ã‚’è©³ç´°è¡¨ç¤º
- ç‰¹å¾´é‡ã®å…¨ä½“çš„ãªå½±éŸ¿åº¦ã‚’å¯è¦–åŒ–
- ç‰¹å¾´é‡é–“ã®ç›¸äº’ä½œç”¨ã‚’åˆ†æ
"""

import psycopg2
import pandas as pd
import pickle
import lightgbm as lgb
import numpy as np
import os
import json
import argparse
from pathlib import Path
import shap
import matplotlib.pyplot as plt
from matplotlib import rcParams
from model_config_loader import get_all_models
from keiba_constants import format_model_description
from db_query_builder import build_race_data_query
from feature_engineering import create_features, add_advanced_features

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
rcParams['font.sans-serif'] = ['MS Gothic', 'Yu Gothic', 'Meiryo']
rcParams['axes.unicode_minus'] = False

# ãƒ—ãƒ­ãƒƒãƒˆä¿å­˜ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
PLOT_DIR = Path('shap_analysis')
PLOT_DIR.mkdir(exist_ok=True)


def load_model_and_data(model_filename, track_code, kyoso_shubetsu_code, surface_type, 
                        min_distance, max_distance, test_year=2022, sample_size=None):
    """
    ãƒ¢ãƒ‡ãƒ«ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€
    
    Args:
        model_filename (str): ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«å
        track_code (str): ç«¶é¦¬å ´ã‚³ãƒ¼ãƒ‰
        kyoso_shubetsu_code (str): ç«¶äº‰ç¨®åˆ¥ã‚³ãƒ¼ãƒ‰
        surface_type (str): 'turf' or 'dirt'
        min_distance (int): æœ€å°è·é›¢
        max_distance (int): æœ€å¤§è·é›¢
        test_year (int): ãƒ†ã‚¹ãƒˆå¯¾è±¡å¹´ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 2022)
        sample_size (int): ã‚µãƒ³ãƒ—ãƒ«æ•°åˆ¶é™ (None=å…¨ä»¶)
        
    Returns:
        tuple: (model, X_test, y_test, test_df_full)
    """
    
    # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
    model_path = Path('models') / model_filename
    if not model_path.exists():
        model_path = Path(model_filename)
    
    print(f"ğŸ“¦ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿: {model_path}")
    with open(model_path, 'rb') as f:
        model = pickle.load(f)
    
    # PostgreSQLæ¥ç¶šï¼ˆdb_config.jsonã‹ã‚‰èª­ã¿è¾¼ã¿ï¼‰
    with open('db_config.json', 'r', encoding='utf-8') as f:
        config = json.load(f)
    
    conn = psycopg2.connect(**config['database'])
    
    # SQLã‚¯ã‚¨ãƒªï¼ˆdb_query_builder.pyã‚’ä½¿ç”¨ï¼‰
    print(f"ğŸ” ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­: {track_code}ç«¶é¦¬å ´ {test_year}å¹´")
    sql = build_race_data_query(
        track_code=track_code,
        year_start=test_year,
        year_end=test_year,
        surface_type=surface_type,
        distance_min=min_distance,
        distance_max=max_distance,
        kyoso_shubetsu_code=kyoso_shubetsu_code,
        include_payout=False
    )
    
    print(f"[+] ãƒ‡ãƒ¼ã‚¿å–å¾—: {test_year}å¹´")
    df_raw = pd.read_sql(sql, conn)
    conn.close()
    
    print(f"å–å¾—ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(df_raw)}")
    
    if len(df_raw) == 0:
        print("[ERROR] ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
        return None, None, None, None
    
    # ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
    df = df_raw.copy()
    
    # æ–‡å­—åˆ—ã¨ã—ã¦ä¿æŒã™ã¹ãã‚«ãƒ©ãƒ 
    string_columns = ['kishu_code', 'chokyoshi_code', 'bamei']
    
    # æ•°å€¤ã‚«ãƒ©ãƒ ã‚’æ˜ç¤ºçš„ã«å®šç¾©ï¼ˆstring_columnsã‚’é™¤ãï¼‰
    numeric_columns = [col for col in df.columns if col not in string_columns + 
                      ['kaisai_nen', 'kaisai_tsukihi', 'keibajo_code', 'race_bango', 
                       'keibajo_name', 'ketto_toroku_bango', 'seibetsu_code', 
                       'kyoso_joken_code', 'kyoso_shubetsu_code', 
                       'grade_code', 'track_code']]
    
    # æ•°å€¤ã‚«ãƒ©ãƒ ã®ã¿ã‚’æ•°å€¤å‹ã«å¤‰æ›
    for col in numeric_columns:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce')
    
    # ã‚°ãƒ«ãƒ¼ãƒ—ã‚­ãƒ¼ä½œæˆ
    df['group_key'] = (df['kaisai_nen'].astype(str) + '_' + 
                       df['kaisai_tsukihi'].astype(str) + '_' + 
                       df['keibajo_code'].astype(str) + '_' + 
                       df['race_bango'].astype(str))
    
    # ç‰¹å¾´é‡è¨ˆç®—ï¼ˆfeature_engineering.pyã‚’ä½¿ç”¨ï¼‰
    print("ğŸ”„ feature_engineering.pyã§ç‰¹å¾´é‡ã‚’è¨ˆç®—ä¸­...")
    
    # åŸºæœ¬ç‰¹å¾´é‡ã‚’ä½œæˆ
    X = create_features(df)
    
    # é«˜åº¦ãªç‰¹å¾´é‡ã‚’è¿½åŠ 
    X = add_advanced_features(
        df=df,
        X=X,
        surface_type=surface_type,
        min_distance=min_distance,
        max_distance=max_distance,
        logger=None,
        inverse_rank=False  # SHAPåˆ†æã§ã¯åè»¢ä¸è¦
    )
    
    print(f"[OK] ç‰¹å¾´é‡è¨ˆç®—å®Œäº†: {len(X.columns)}å€‹")
    
    # ãƒ¢ãƒ‡ãƒ«ã®å®Ÿéš›ã®ç‰¹å¾´é‡åã‚’å–å¾—ã—ã¦é †åºã‚’åˆã‚ã›ã‚‹
    if hasattr(model, 'feature_name'):
        actual_features = model.feature_name()
        print(f"[LIST] ãƒ¢ãƒ‡ãƒ«ã®å®Ÿéš›ã®ç‰¹å¾´é‡: {len(actual_features)}å€‹")
        
        # ä¸è¶³ã—ã¦ã„ã‚‹ç‰¹å¾´é‡ã‚’ãƒã‚§ãƒƒã‚¯
        missing = [f for f in actual_features if f not in X.columns]
        if missing:
            print(f"[WARNING] ä¸€éƒ¨ç‰¹å¾´é‡ãŒä¸è¶³ã—ã¦ã„ã¾ã™: {missing}")
            print(f"[INFO] ä¸è¶³ç‰¹å¾´é‡ã‚’ä¸­ç«‹å€¤(0.5)ã§è£œå®Œã—ã¾ã™")
            # ä¸è¶³ç‰¹å¾´é‡ã‚’0.5(ä¸­ç«‹å€¤)ã§åŸ‹ã‚ã‚‹
            for feat in missing:
                X[feat] = 0.5
        
        # ç‰¹å¾´é‡ã®é †åºã‚’ãƒ¢ãƒ‡ãƒ«ã¨åˆã‚ã›ã‚‹
        X = X[actual_features]
    else:
        print("[ERROR] ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ç‰¹å¾´é‡åã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
        return None, None, None, None
    
    # ç€é †ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆSQLå´ã§ã¯ kakutei_chakujun_numeric ã¨ã—ã¦è¨ˆç®—æ¸ˆã¿ï¼‰
    y = df['kakutei_chakujun_numeric'].values
    
    # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    if sample_size and len(X) > sample_size:
        indices = np.random.choice(len(X), sample_size, replace=False)
        X = X.iloc[indices]
        y = y[indices]
        df = df.iloc[indices]
    
    print(f"[OK] ãƒ‡ãƒ¼ã‚¿æº–å‚™å®Œäº†: {len(X)}ä»¶")
    
    return model, X, y, df


def analyze_shap_global(model, X, feature_names, output_prefix):
    """
    SHAPå…¨ä½“åˆ†æï¼ˆç‰¹å¾´é‡é‡è¦åº¦ã€ä¾å­˜æ€§ãƒ—ãƒ­ãƒƒãƒˆï¼‰
    
    Args:
        model: LightGBMãƒ¢ãƒ‡ãƒ«
        X: ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿
        feature_names: ç‰¹å¾´é‡åãƒªã‚¹ãƒˆ
        output_prefix: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹
    """
    print("\n[+] SHAPå…¨ä½“åˆ†æã‚’å®Ÿè¡Œä¸­...")
    
    # SHAPå€¤è¨ˆç®—
    explainer = shap.TreeExplainer(model)
    shap_values = explainer.shap_values(X)
    
    # 1. Summary Plotï¼ˆç‰¹å¾´é‡é‡è¦åº¦ã¨åˆ†å¸ƒï¼‰
    print("  - Summary Plotä½œæˆä¸­...")
    plt.figure(figsize=(12, 8))
    shap.summary_plot(shap_values, X, feature_names=feature_names, show=False)
    plt.title('SHAP Summary Plot - ç‰¹å¾´é‡ã®å½±éŸ¿åº¦ã¨åˆ†å¸ƒ', fontsize=14, pad=20)
    plt.tight_layout()
    plt.savefig(PLOT_DIR / f'{output_prefix}_summary.png', dpi=150, bbox_inches='tight')
    plt.close()
    print(f"    [OK] ä¿å­˜: {PLOT_DIR / f'{output_prefix}_summary.png'}")
    
    # 2. Bar Plotï¼ˆå¹³å‡çµ¶å¯¾SHAPå€¤ï¼‰
    print("  - Bar Plotä½œæˆä¸­...")
    plt.figure(figsize=(10, 8))
    shap.summary_plot(shap_values, X, feature_names=feature_names, plot_type="bar", show=False)
    plt.title('SHAP Bar Plot - ç‰¹å¾´é‡ã®å¹³å‡å½±éŸ¿åº¦', fontsize=14, pad=20)
    plt.tight_layout()
    plt.savefig(PLOT_DIR / f'{output_prefix}_bar.png', dpi=150, bbox_inches='tight')
    plt.close()
    print(f"    [OK] ä¿å­˜: {PLOT_DIR / f'{output_prefix}_bar.png'}")
    
    # 3. ä¸Šä½5ç‰¹å¾´é‡ã®ä¾å­˜æ€§ãƒ—ãƒ­ãƒƒãƒˆ
    print("  - Dependence Plotä½œæˆä¸­...")
    mean_abs_shap = np.abs(shap_values).mean(axis=0)
    top_features_idx = np.argsort(mean_abs_shap)[-5:][::-1]
    
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    axes = axes.flatten()
    
    for i, idx in enumerate(top_features_idx):
        shap.dependence_plot(idx, shap_values, X, feature_names=feature_names, 
                            ax=axes[i], show=False)
        axes[i].set_title(f'{feature_names[idx]} ã®ä¾å­˜æ€§', fontsize=12)
    
    # æœ€å¾Œã®ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆã‚’éè¡¨ç¤º
    axes[-1].axis('off')
    
    plt.tight_layout()
    plt.savefig(PLOT_DIR / f'{output_prefix}_dependence.png', dpi=150, bbox_inches='tight')
    plt.close()
    print(f"    [OK] ä¿å­˜: {PLOT_DIR / f'{output_prefix}_dependence.png'}")
    
    # 4. ç‰¹å¾´é‡é‡è¦åº¦ã‚’CSVå‡ºåŠ›
    feature_importance_df = pd.DataFrame({
        'feature': feature_names,
        'mean_abs_shap': mean_abs_shap,
        'lgb_gain': model.feature_importance(importance_type='gain')
    }).sort_values('mean_abs_shap', ascending=False)
    
    csv_path = PLOT_DIR / f'{output_prefix}_importance.csv'
    feature_importance_df.to_csv(csv_path, index=False, encoding='utf-8-sig')
    print(f"    [OK] ç‰¹å¾´é‡é‡è¦åº¦ä¿å­˜: {csv_path}")
    
    print("\n[LIST] ç‰¹å¾´é‡é‡è¦åº¦ãƒˆãƒƒãƒ—10:")
    print(feature_importance_df.head(10).to_string(index=False))
    
    return shap_values, explainer


def analyze_shap_individual(shap_values, explainer, X, df_full, feature_names, 
                            output_prefix, num_samples=5):
    """
    å€‹åˆ¥ãƒ¬ãƒ¼ã‚¹ã®SHAPåˆ†æ
    
    Args:
        shap_values: SHAPå€¤é…åˆ—
        explainer: SHAPExplainer
        X: ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿
        df_full: å…ƒãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ï¼ˆé¦¬åãªã©ã®æƒ…å ±å«ã‚€ï¼‰
        feature_names: ç‰¹å¾´é‡åãƒªã‚¹ãƒˆ
        output_prefix: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹
        num_samples: åˆ†æã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«æ•°
    """
    print(f"\n[TEST] å€‹åˆ¥ãƒ¬ãƒ¼ã‚¹åˆ†æï¼ˆã‚µãƒ³ãƒ—ãƒ«{num_samples}ä»¶ï¼‰...")
    
    # ãƒ©ãƒ³ãƒ€ãƒ ã«ã‚µãƒ³ãƒ—ãƒ«é¸æŠ
    sample_indices = np.random.choice(len(X), min(num_samples, len(X)), replace=False)
    
    for i, idx in enumerate(sample_indices):
        print(f"\n--- ã‚µãƒ³ãƒ—ãƒ« {i+1}/{num_samples} ---")
        
        # ãƒ¬ãƒ¼ã‚¹æƒ…å ±
        race_info = df_full.iloc[idx]
        print(f"æ—¥ä»˜: {race_info['kaisai_nen']}/{race_info['kaisai_tsukihi']}")
        print(f"ç«¶é¦¬å ´: {race_info['keibajo_name']} R{race_info['race_bango']}")
        print(f"é¦¬å: {race_info['bamei']}")
        print(f"å®Ÿéš›ã®ç€é †: {race_info['kakutei_chakujun']:.0f}ç€")
        print(f"äººæ°—: {race_info['tansho_ninkijun_numeric']:.0f}ç•ªäººæ°—")
        
        # Force Plot
        shap.force_plot(
            explainer.expected_value, 
            shap_values[idx], 
            X.iloc[idx],
            feature_names=feature_names,
            matplotlib=True,
            show=False
        )
        plt.title(f"{race_info['bamei']} - SHAP Force Plot", fontsize=12, pad=10)
        plt.tight_layout()
        plt.savefig(PLOT_DIR / f'{output_prefix}_force_{i+1}.png', dpi=150, bbox_inches='tight')
        plt.close()
        
        # è²¢çŒ®åº¦ãƒˆãƒƒãƒ—10ã‚’è¡¨ç¤º
        shap_contributions = pd.DataFrame({
            'feature': feature_names,
            'value': X.iloc[idx].values,
            'shap_value': shap_values[idx]
        })
        shap_contributions['abs_shap'] = np.abs(shap_contributions['shap_value'])
        shap_contributions = shap_contributions.sort_values('abs_shap', ascending=False)
        
        print("\nè²¢çŒ®åº¦ãƒˆãƒƒãƒ—10:")
        for _, row in shap_contributions.head(10).iterrows():
            direction = "â†‘" if row['shap_value'] > 0 else "â†“"
            print(f"  {row['feature']:30s}: {row['value']:8.2f} â†’ SHAP={row['shap_value']:+8.4f} {direction}")
        
        print(f"  [OK] Force Plotä¿å­˜: {PLOT_DIR / f'{output_prefix}_force_{i+1}.png'}")


def main():
    """
    ãƒ¡ã‚¤ãƒ³å‡¦ç†
    """
    # argparseã§ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‚’è§£æ
    parser = argparse.ArgumentParser(
        description='SHAPåˆ†æã«ã‚ˆã‚‹ç«¶é¦¬äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«èª¬æ˜',
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    parser.add_argument(
        '--model',
        type=str,
        help='ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆä¾‹: models/tokyo_turf_3ageup_long.savï¼‰'
    )
    
    parser.add_argument(
        '--test-year',
        type=int,
        default=2023,
        help='ãƒ†ã‚¹ãƒˆå¯¾è±¡å¹´ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 2023ï¼‰'
    )
    
    parser.add_argument(
        '--track-code',
        type=str,
        help='ç«¶é¦¬å ´ã‚³ãƒ¼ãƒ‰ï¼ˆä¾‹: 05=æ±äº¬ï¼‰'
    )
    
    parser.add_argument(
        '--surface-type',
        type=str,
        choices=['turf', 'dirt'],
        help='è·¯é¢ã‚¿ã‚¤ãƒ—ï¼ˆturf or dirtï¼‰'
    )
    
    parser.add_argument(
        '--min-distance',
        type=int,
        help='æœ€å°è·é›¢ï¼ˆä¾‹: 1000ï¼‰'
    )
    
    parser.add_argument(
        '--max-distance',
        type=int,
        help='æœ€å¤§è·é›¢ï¼ˆä¾‹: 1600ï¼‰'
    )
    
    parser.add_argument(
        '--kyoso-shubetsu-code',
        type=str,
        help='ç«¶äº‰ç¨®åˆ¥ã‚³ãƒ¼ãƒ‰ï¼ˆä¾‹: 13=3æ­³ä»¥ä¸Šï¼‰'
    )
    
    parser.add_argument(
        '--sample-size',
        type=int,
        default=500,
        help='SHAPåˆ†æã®ã‚µãƒ³ãƒ—ãƒ«æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 500ï¼‰'
    )
    
    args = parser.parse_args()
    
    print("=" * 80)
    print("[TARGET] SHAPåˆ†æã«ã‚ˆã‚‹ç«¶é¦¬äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«èª¬æ˜")
    print("=" * 80)
    
    # åˆ†æå¯¾è±¡ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ
    models = get_all_models()
    
    if not models:
        print("[ERROR] model_configs.jsonã«ãƒ¢ãƒ‡ãƒ«ãŒå®šç¾©ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return
    
    print("\nåˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«:")
    for i, model_info in enumerate(models, 1):
        desc = format_model_description(
            model_info['track_code'],
            model_info['kyoso_shubetsu_code'],
            model_info['surface_type'],
            model_info['min_distance'],
            model_info['max_distance']
        )
        print(f"  {i}. {desc}")
    
    # ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’æ±ºå®š
    model_info = None
    
    if args.model:
        # --modelãŒæŒ‡å®šã•ã‚ŒãŸå ´åˆ
        model_path = Path(args.model)
        model_filename = model_path.name
        
        # model_configs.jsonã‹ã‚‰è©²å½“ãƒ¢ãƒ‡ãƒ«ã‚’æ¤œç´¢
        for m in models:
            if m['model_filename'] == model_filename:
                model_info = m.copy()
                break
        
        if not model_info:
            print(f"[WARNING] ãƒ¢ãƒ‡ãƒ« {model_filename} ãŒmodel_configs.jsonã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            
            # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‹ã‚‰ç›´æ¥ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å–å¾—
            if all([args.track_code, args.surface_type, args.min_distance, args.max_distance, args.kyoso_shubetsu_code]):
                model_info = {
                    'model_filename': model_filename,
                    'track_code': args.track_code,
                    'surface_type': args.surface_type,
                    'min_distance': args.min_distance,
                    'max_distance': args.max_distance,
                    'kyoso_shubetsu_code': args.kyoso_shubetsu_code
                }
                print("[INFO] ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’æ§‹ç¯‰ã—ã¾ã—ãŸ")
            else:
                print("[ERROR] ãƒ¢ãƒ‡ãƒ«æƒ…å ±ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚--track-code, --surface-type, --min-distance, --max-distance, --kyoso-shubetsu-codeã‚’æŒ‡å®šã—ã¦ãã ã•ã„")
                return
    else:
        # --modelãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆã€æœ€åˆã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
        model_info = models[0]
        print("[INFO] ãƒ¢ãƒ‡ãƒ«ãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„ãŸã‚ã€æœ€åˆã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™")
    
    print(f"\n[PIN] åˆ†æå¯¾è±¡: {format_model_description(model_info['track_code'], model_info['kyoso_shubetsu_code'], model_info['surface_type'], model_info['min_distance'], model_info['max_distance'])}")
    print(f"[PIN] å¯¾è±¡å¹´: {args.test_year}å¹´")
    
    # ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    model, X, y, df_full = load_model_and_data(
        model_filename=model_info['model_filename'],
        track_code=model_info['track_code'],
        kyoso_shubetsu_code=model_info['kyoso_shubetsu_code'],
        surface_type=model_info['surface_type'],
        min_distance=model_info['min_distance'],
        max_distance=model_info['max_distance'],
        test_year=args.test_year,
        sample_size=args.sample_size
    )
    
    if model is None:
        print("[ERROR] ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
        return
    
    # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åã®ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹
    output_prefix = Path(model_info['model_filename']).stem
    
    # SHAPå…¨ä½“åˆ†æ
    shap_values, explainer = analyze_shap_global(
        model=model,
        X=X,
        feature_names=X.columns.tolist(),
        output_prefix=output_prefix
    )
    
    # å€‹åˆ¥ãƒ¬ãƒ¼ã‚¹åˆ†æ
    analyze_shap_individual(
        shap_values=shap_values,
        explainer=explainer,
        X=X,
        df_full=df_full,
        feature_names=X.columns.tolist(),
        output_prefix=output_prefix,
        num_samples=5
    )
    
    print("\n" + "=" * 80)
    print("[OK] SHAPåˆ†æå®Œäº†!")
    print(f"[FILE] çµæœä¿å­˜å…ˆ: {PLOT_DIR.absolute()}")
    print("=" * 80)


if __name__ == '__main__':
    main()
