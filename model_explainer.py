#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SHAPåˆ†æã«ã‚ˆã‚‹ç«¶é¦¬äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«èª¬æ˜ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ç†ç”±ã‚’SHAPã§å¯è¦–åŒ–ãƒ»åˆ†æã—ã¾ã™ã€‚
- å€‹åˆ¥ãƒ¬ãƒ¼ã‚¹ã®äºˆæ¸¬ç†ç”±ã‚’è©³ç´°è¡¨ç¤º
- ç‰¹å¾´é‡ã®å…¨ä½“çš„ãªå½±éŸ¿åº¦ã‚’å¯è¦–åŒ–
- ç‰¹å¾´é‡é–“ã®ç›¸äº’ä½œç”¨ã‚’åˆ†æ
"""

import psycopg2
import pandas as pd
import pickle
import lightgbm as lgb
import numpy as np
import os
from pathlib import Path
import shap
import matplotlib.pyplot as plt
from matplotlib import rcParams
from model_config_loader import get_all_models
from keiba_constants import format_model_description

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
rcParams['font.sans-serif'] = ['MS Gothic', 'Yu Gothic', 'Meiryo']
rcParams['axes.unicode_minus'] = False

# ãƒ—ãƒ­ãƒƒãƒˆä¿å­˜ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
PLOT_DIR = Path('shap_analysis')
PLOT_DIR.mkdir(exist_ok=True)


def load_model_and_data(model_filename, track_code, kyoso_shubetsu_code, surface_type, 
                        min_distance, max_distance, test_year=2022, sample_size=None):
    """
    ãƒ¢ãƒ‡ãƒ«ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€
    
    Args:
        model_filename (str): ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«å
        track_code (str): ç«¶é¦¬å ´ã‚³ãƒ¼ãƒ‰
        kyoso_shubetsu_code (str): ç«¶äº‰ç¨®åˆ¥ã‚³ãƒ¼ãƒ‰
        surface_type (str): 'turf' or 'dirt'
        min_distance (int): æœ€å°è·é›¢
        max_distance (int): æœ€å¤§è·é›¢
        test_year (int): ãƒ†ã‚¹ãƒˆå¯¾è±¡å¹´ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 2022)
        sample_size (int): ã‚µãƒ³ãƒ—ãƒ«æ•°åˆ¶é™ (None=å…¨ä»¶)
        
    Returns:
        tuple: (model, X_test, y_test, test_df_full)
    """
    
    # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
    model_path = Path('models') / model_filename
    if not model_path.exists():
        model_path = Path(model_filename)
    
    print(f"ğŸ“¦ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿: {model_path}")
    with open(model_path, 'rb') as f:
        model = pickle.load(f)
    
    # PostgreSQLæ¥ç¶š
    conn = psycopg2.connect(
        host='localhost',
        port='5432',
        user='postgres',
        password='ahtaht88',
        dbname='keiba'
    )
    
    # ãƒˆãƒ©ãƒƒã‚¯æ¡ä»¶ã‚’å‹•çš„ã«è¨­å®š
    if surface_type.lower() == 'turf':
        track_condition = "cast(rase.track_code as integer) between 10 and 22"
        baba_condition = "ra.babajotai_code_shiba"
    else:
        track_condition = "cast(rase.track_code as integer) between 23 and 29"
        baba_condition = "ra.babajotai_code_dirt"

    # è·é›¢æ¡ä»¶ã‚’è¨­å®š
    if max_distance == 9999:
        distance_condition = f"cast(rase.kyori as integer) >= {min_distance}"
    else:
        distance_condition = f"cast(rase.kyori as integer) between {min_distance} and {max_distance}"

    # ç«¶äº‰ç¨®åˆ¥ã‚’è¨­å®š
    if kyoso_shubetsu_code == '12':
        kyoso_shubetsu_condition = "cast(rase.kyoso_shubetsu_code as integer) = 12"
    elif kyoso_shubetsu_code == '13':
        kyoso_shubetsu_condition = "cast(rase.kyoso_shubetsu_code as integer) >= 13"

    # SQLã‚¯ã‚¨ãƒªï¼ˆmodel_creator.pyã¨å®Œå…¨ã«åŒã˜æ§‹é€ ï¼‰
    sql = f"""
    select * from (
        select
        ra.kaisai_nen,
        ra.kaisai_tsukihi,
        ra.race_bango,
        seum.umaban,
        seum.bamei,
        ra.keibajo_code,
        CASE 
            WHEN ra.keibajo_code = '01' THEN 'æœ­å¹Œ' 
            WHEN ra.keibajo_code = '02' THEN 'å‡½é¤¨' 
            WHEN ra.keibajo_code = '03' THEN 'ç¦å³¶' 
            WHEN ra.keibajo_code = '04' THEN 'æ–°æ½Ÿ' 
            WHEN ra.keibajo_code = '05' THEN 'æ±äº¬' 
            WHEN ra.keibajo_code = '06' THEN 'ä¸­å±±' 
            WHEN ra.keibajo_code = '07' THEN 'ä¸­äº¬' 
            WHEN ra.keibajo_code = '08' THEN 'äº¬éƒ½' 
            WHEN ra.keibajo_code = '09' THEN 'é˜ªç¥' 
            WHEN ra.keibajo_code = '10' THEN 'å°å€‰' 
            ELSE '' 
        END keibajo_name,
        ra.kyori,
        ra.shusso_tosu,
        ra.tenko_code,
        {baba_condition} as babajotai_code,
        ra.grade_code,
        ra.kyoso_joken_code,
        ra.kyoso_shubetsu_code,
        ra.track_code,
        seum.ketto_toroku_bango,
        seum.wakuban,
        cast(seum.umaban as integer) as umaban_numeric,
        seum.barei,
        seum.kishu_code,
        seum.chokyoshi_code,
        seum.kishu_name,
        seum.chokyoshi_name,
        seum.futan_juryo,
        seum.seibetsu_code,
        nullif(cast(seum.tansho_odds as float), 0) / 10 as tansho_odds,
        nullif(cast(seum.tansho_ninkijun as integer), 0) as tansho_ninkijun_numeric,
        18 - cast(seum.kakutei_chakujun as integer) + 1 as kakutei_chakujun_numeric,
        1.0 / nullif(cast(seum.kakutei_chakujun as integer), 0) as chakujun_score,
        AVG(
            1 - (cast(seum.kakutei_chakujun as float) / cast(ra.shusso_tosu as float))
        ) OVER (
            PARTITION BY seum.ketto_toroku_bango
            ORDER BY cast(ra.kaisai_nen as integer), cast(ra.kaisai_tsukihi as integer)
            ROWS BETWEEN 3 PRECEDING AND 1 PRECEDING
        ) AS past_avg_sotai_chakujun,
        AVG(
            cast(ra.kyori as integer) /
            NULLIF(
                FLOOR(cast(seum.soha_time as integer) / 1000) * 60 +
                FLOOR((cast(seum.soha_time as integer) % 1000) / 10) +
                (cast(seum.soha_time as integer) % 10) * 0.1,
                0
            )
        ) OVER (
            PARTITION BY seum.ketto_toroku_bango
            ORDER BY cast(ra.kaisai_nen as integer), cast(ra.kaisai_tsukihi as integer)
            ROWS BETWEEN 3 PRECEDING AND 1 PRECEDING
        ) AS time_index,
        SUM(
            CASE 
                WHEN seum.kakutei_chakujun = '01' THEN 100
                WHEN seum.kakutei_chakujun = '02' THEN 80
                WHEN seum.kakutei_chakujun = '03' THEN 60
                WHEN seum.kakutei_chakujun = '04' THEN 40
                WHEN seum.kakutei_chakujun = '05' THEN 30
                WHEN seum.kakutei_chakujun = '06' THEN 20
                WHEN seum.kakutei_chakujun = '07' THEN 10
                ELSE 5 
            END
            * CASE 
                WHEN ra.grade_code = 'A' THEN 1.00
                WHEN ra.grade_code = 'B' THEN 0.80
                WHEN ra.grade_code = 'C' THEN 0.60
                WHEN ra.grade_code <> 'A' AND ra.grade_code <> 'B' AND ra.grade_code <> 'C' AND ra.kyoso_joken_code = '999' THEN 0.50
                WHEN ra.grade_code <> 'A' AND ra.grade_code <> 'B' AND ra.grade_code <> 'C' AND ra.kyoso_joken_code = '016' THEN 0.40
                WHEN ra.grade_code <> 'A' AND ra.grade_code <> 'B' AND ra.grade_code <> 'C' AND ra.kyoso_joken_code = '010' THEN 0.30
                WHEN ra.grade_code <> 'A' AND ra.grade_code <> 'B' AND ra.grade_code <> 'C' AND ra.kyoso_joken_code = '005' THEN 0.20
                ELSE 0.10
            END
        ) OVER (
            PARTITION BY seum.ketto_toroku_bango
            ORDER BY cast(ra.kaisai_nen as integer), cast(ra.kaisai_tsukihi as integer)
            ROWS BETWEEN 3 PRECEDING AND 1 PRECEDING  
        ) AS past_score,
        CASE 
            WHEN AVG(
                CASE 
                    WHEN cast(seum.kohan_3f as integer) > 0 AND cast(seum.kohan_3f as integer) < 999 THEN
                    CAST(seum.kohan_3f AS FLOAT) / 10
                    ELSE NULL
                END
            ) OVER (
                PARTITION BY seum.ketto_toroku_bango
                ORDER BY cast(ra.kaisai_nen as integer), cast(ra.kaisai_tsukihi as integer)
                ROWS BETWEEN 3 PRECEDING AND 1 PRECEDING
            ) IS NOT NULL THEN
            AVG(
                CASE 
                    WHEN cast(seum.kohan_3f as integer) > 0 AND cast(seum.kohan_3f as integer) < 999 THEN
                    CAST(seum.kohan_3f AS FLOAT) / 10
                    ELSE NULL
                END
            ) OVER (
                PARTITION BY seum.ketto_toroku_bango
                ORDER BY cast(ra.kaisai_nen as integer), cast(ra.kaisai_tsukihi as integer)
                ROWS BETWEEN 3 PRECEDING AND 1 PRECEDING
            ) - 
            CASE
                WHEN cast(ra.kyori as integer) <= 1600 THEN 33.5
                WHEN cast(ra.kyori as integer) <= 2000 THEN 35.0
                WHEN cast(ra.kyori as integer) <= 2400 THEN 36.0
                ELSE 37.0
            END
            ELSE 0
        END AS kohan_3f_index,
        seum.kakutei_chakujun,
        seum.kohan_3f
    from
        jvd_ra ra 
        inner join ( 
            select
                se.kaisai_nen
                , se.kaisai_tsukihi
                , se.keibajo_code
                , se.race_bango
                , se.kakutei_chakujun
                , se.ketto_toroku_bango
                , se.bamei
                , se.wakuban
                , se.umaban
                , se.barei
                , se.seibetsu_code
                , se.kishu_code
                , se.chokyoshi_code
                , trim(se.kishumei_ryakusho) as kishu_name
                , trim(se.chokyoshimei_ryakusho) as chokyoshi_name
                , se.futan_juryo
                , se.tansho_odds
                , se.tansho_ninkijun
                , se.kohan_3f
                , se.soha_time
            from
                jvd_se se
            where 
                se.kohan_3f <> '000' 
                and se.kohan_3f <> '999'
        ) seum 
            on ra.kaisai_nen = seum.kaisai_nen 
            and ra.kaisai_tsukihi = seum.kaisai_tsukihi 
            and ra.keibajo_code = seum.keibajo_code 
            and ra.race_bango = seum.race_bango 
    where
        cast(ra.kaisai_nen as integer) = {test_year}
    ) rase 
    where 
    rase.keibajo_code = '{track_code}'
    and {kyoso_shubetsu_condition}
    and {track_condition}
    and {distance_condition}
    """
    
    print(f"[+] ãƒ‡ãƒ¼ã‚¿å–å¾—: {test_year}å¹´")
    df_raw = pd.read_sql(sql, conn)
    conn.close()
    
    print(f"å–å¾—ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(df_raw)}")
    
    if len(df_raw) == 0:
        print("[ERROR] ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
        return None, None, None, None
    
    # ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
    df = df_raw.copy()
    
    # æ–‡å­—åˆ—ã¨ã—ã¦ä¿æŒã™ã¹ãã‚«ãƒ©ãƒ 
    string_columns = ['kishu_code', 'chokyoshi_code', 'bamei']
    
    # æ•°å€¤ã‚«ãƒ©ãƒ ã‚’æ˜ç¤ºçš„ã«å®šç¾©ï¼ˆstring_columnsã‚’é™¤ãï¼‰
    numeric_columns = [col for col in df.columns if col not in string_columns + 
                      ['kaisai_nen', 'kaisai_tsukihi', 'keibajo_code', 'race_bango', 
                       'keibajo_name', 'ketto_toroku_bango', 'seibetsu_code', 
                       'kyoso_joken_code', 'kyoso_shubetsu_code', 
                       'grade_code', 'track_code']]
    
    # æ•°å€¤ã‚«ãƒ©ãƒ ã®ã¿ã‚’æ•°å€¤å‹ã«å¤‰æ›
    for col in numeric_columns:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce')
    
    # ã‚°ãƒ«ãƒ¼ãƒ—ã‚­ãƒ¼ä½œæˆ
    df['group_key'] = (df['kaisai_nen'].astype(str) + '_' + 
                       df['kaisai_tsukihi'].astype(str) + '_' + 
                       df['keibajo_code'].astype(str) + '_' + 
                       df['race_bango'].astype(str))
    
    # ç‰¹å¾´é‡è¨ˆç®—
    X = calculate_features(df, model)
    
    # ãƒ¢ãƒ‡ãƒ«ã®å®Ÿéš›ã®ç‰¹å¾´é‡åã‚’å–å¾—ã—ã¦é †åºã‚’åˆã‚ã›ã‚‹
    if hasattr(model, 'feature_name'):
        actual_features = model.feature_name()
        print(f"[LIST] ãƒ¢ãƒ‡ãƒ«ã®å®Ÿéš›ã®ç‰¹å¾´é‡: {len(actual_features)}å€‹")
        
        # ä¸è¶³ã—ã¦ã„ã‚‹ç‰¹å¾´é‡ã‚’ãƒã‚§ãƒƒã‚¯
        missing = [f for f in actual_features if f not in X.columns]
        if missing:
            raise ValueError(f"[ERROR] å¿…é ˆç‰¹å¾´é‡ãŒä¸è¶³ã—ã¦ã„ã¾ã™: {missing}")
        
        # ç‰¹å¾´é‡ã®é †åºã‚’ãƒ¢ãƒ‡ãƒ«ã¨åˆã‚ã›ã‚‹
        X = X[actual_features]
    else:
        print("[ERROR] ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ç‰¹å¾´é‡åã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
        return None, None, None, None
    
    y = df['kakutei_chakujun'].values
    
    # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    if sample_size and len(X) > sample_size:
        indices = np.random.choice(len(X), sample_size, replace=False)
        X = X.iloc[indices]
        y = y[indices]
        df = df.iloc[indices]
    
    print(f"[OK] ãƒ‡ãƒ¼ã‚¿æº–å‚™å®Œäº†: {len(X)}ä»¶")
    
    return model, X, y, df


def calculate_features(df, model):
    """
    model_creator.pyã¨åŒã˜ç‰¹å¾´é‡ã‚’è¨ˆç®—
    """
    print("ğŸ”„ model_creator.pyã¨åŒã˜ç‰¹å¾´é‡è¨ˆç®—ã‚’å®Ÿè¡Œä¸­...")
    
    # past_avg_sotai_chakujunã¯SQLã§è¨ˆç®—æ¸ˆã¿ã®å˜ç´”ç§»å‹•å¹³å‡ã‚’ä½¿ç”¨
    
    # åŸºæœ¬ç‰¹å¾´é‡ï¼ˆSQLã§è¨ˆç®—æ¸ˆã¿ï¼‰
    base_features = ["futan_juryo", "past_score", "kohan_3f_index", "past_avg_sotai_chakujun", "time_index"]
    
    # ä¸è¶³ãƒã‚§ãƒƒã‚¯
    missing = [feat for feat in base_features if feat not in df.columns]
    if missing:
        raise ValueError(f"[ERROR] å¿…é ˆç‰¹å¾´é‡ãŒä¸è¶³ã—ã¦ã„ã¾ã™: {missing}")
    
    X = df.loc[:, base_features].astype(float).copy()
    
    # æ´¾ç”Ÿç‰¹å¾´é‡ã®è¨ˆç®—
    # æ ç•ªã¨é ­æ•°ã®æ¯”ç‡
    max_wakuban = df.groupby(['kaisai_nen', 'kaisai_tsukihi', 'race_bango'])['wakuban'].transform('max')
    X['wakuban_ratio'] = df['wakuban'] / max_wakuban
    
    # æ–¤é‡ã¨é¦¬é½¢ã®æ¯”ç‡
    df['futan_per_barei'] = df['futan_juryo'] / df['barei'].replace(0, 1)
    X['futan_per_barei'] = df['futan_per_barei']
    
    # é¦¬ç•ªÃ—è·é›¢ã®ç›¸äº’ä½œç”¨
    df['umaban_kyori_interaction'] = df['umaban_numeric'] * df['kyori'] / 1000
    X['umaban_kyori_interaction'] = df['umaban_kyori_interaction']
    
    # futan_per_bareiã®éç·šå½¢å¤‰æ›
    df['futan_per_barei_log'] = np.log(df['futan_per_barei'].clip(lower=0.1))
    X['futan_per_barei_log'] = df['futan_per_barei_log']
    
    # æœŸå¾…æ–¤é‡ã‹ã‚‰ã®å·®åˆ†
    expected_weight_by_age = {2: 48, 3: 52, 4: 55, 5: 57, 6: 57, 7: 56, 8: 55}
    df['futan_deviation'] = df.apply(
        lambda row: row['futan_juryo'] - expected_weight_by_age.get(row['barei'], 55), 
        axis=1
    )
    X['futan_deviation'] = df['futan_deviation']
    
    # ãƒ”ãƒ¼ã‚¯å¹´é½¢ãƒ‘ã‚¿ãƒ¼ãƒ³
    X['barei_peak_distance'] = abs(df['barei'] - 4)
    X['barei_peak_short'] = abs(df['barei'] - 3)
    
    # æ ç•ªãƒã‚¤ã‚¢ã‚¹ã‚¹ã‚³ã‚¢
    wakuban_stats = df.groupby('wakuban').agg({
        'kakutei_chakujun_numeric': ['mean', 'std', 'count']
    }).round(4)
    wakuban_stats.columns = ['waku_avg_rank', 'waku_std_rank', 'waku_count']
    wakuban_stats = wakuban_stats.reset_index()
    
    overall_avg_rank = df['kakutei_chakujun_numeric'].mean()
    wakuban_stats['wakuban_bias_score'] = (overall_avg_rank - wakuban_stats['waku_avg_rank']) / wakuban_stats['waku_std_rank']
    wakuban_stats['wakuban_bias_score'] = wakuban_stats['wakuban_bias_score'].fillna(0)
    
    df = df.merge(wakuban_stats[['wakuban', 'wakuban_bias_score']], on='wakuban', how='left')
    X['wakuban_bias_score'] = df['wakuban_bias_score']
    
    # é¦¬ç•ªç›¸å¯¾ä½ç½®
    df['umaban_percentile'] = df.groupby(['kaisai_nen', 'kaisai_tsukihi', 'race_bango'])['umaban_numeric'].transform(
        lambda x: x.rank(pct=True)
    )
    X['umaban_percentile'] = df['umaban_percentile']
    
    # æ–¤é‡åå·®å€¤
    race_group = df.groupby(['kaisai_nen', 'kaisai_tsukihi', 'race_bango'])['futan_juryo']
    df['futan_mean'] = race_group.transform('mean')
    df['futan_std'] = race_group.transform('std')
    
    df['futan_zscore'] = np.where(
        df['futan_std'] > 0,
        (df['futan_juryo'] - df['futan_mean']) / df['futan_std'],
        0
    )
    X['futan_zscore'] = df['futan_zscore']
    X['futan_percentile'] = race_group.transform(lambda x: x.rank(pct=True))
    
    # è·é›¢ãƒ»é¦¬å ´ã‚«ãƒ†ã‚´ãƒªåˆ†é¡
    def categorize_distance(kyori):
        if kyori <= 1400: return 'short'
        elif kyori <= 1800: return 'mile'
        elif kyori <= 2400: return 'middle'
        else: return 'long'
    
    def categorize_surface(track_code):
        track_code_int = int(track_code)
        if 10 <= track_code_int <= 22: return 'turf'
        elif 23 <= track_code_int <= 24: return 'dirt'
        else: return 'unknown'
    
    def categorize_baba_condition(baba_code):
        if baba_code == 1: return 'good'
        elif baba_code == 2: return 'slightly'
        elif baba_code == 3: return 'heavy'
        elif baba_code == 4: return 'bad'
        else: return 'unknown'
    
    df['distance_category'] = df['kyori'].apply(categorize_distance)
    df['surface_type'] = df['track_code'].apply(categorize_surface)
    df['baba_condition'] = df['babajotai_code'].apply(categorize_baba_condition)
    
    # æ™‚ç³»åˆ—ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆè·é›¢é©æ€§ï¼‰
    df_sorted = df.sort_values(['ketto_toroku_bango', 'kaisai_nen', 'kaisai_tsukihi']).copy()
    
    def calc_distance_category_score(group):
        scores = []
        for idx in range(len(group)):
            if idx == 0:
                scores.append(0.5)
                continue
            current_category = group.iloc[idx]['distance_category']
            past_same = group.iloc[:idx][group.iloc[:idx]['distance_category'] == current_category].tail(5)
            if len(past_same) > 0:
                scores.append((1 - (past_same['kakutei_chakujun_numeric'] / 18.0)).mean())
            else:
                scores.append(0.5)
        return pd.Series(scores, index=group.index)
    
    def calc_similar_distance_score(group):
        scores = []
        for idx in range(len(group)):
            if idx == 0:
                scores.append(0.5)
                continue
            current_kyori = group.iloc[idx]['kyori']
            past_similar = group.iloc[:idx][abs(group.iloc[:idx]['kyori'] - current_kyori) <= 200].tail(10)
            if len(past_similar) > 0:
                scores.append((1 - (past_similar['kakutei_chakujun_numeric'] / 18.0)).mean())
            else:
                scores.append(0.5)
        return pd.Series(scores, index=group.index)
    
    def calc_surface_score(group):
        scores = []
        for idx in range(len(group)):
            if idx == 0:
                scores.append(0.5)
                continue
            current_surface = group.iloc[idx]['surface_type']
            past_same = group.iloc[:idx][group.iloc[:idx]['surface_type'] == current_surface].tail(10)
            if len(past_same) > 0:
                scores.append((1 - (past_same['kakutei_chakujun_numeric'] / 18.0)).mean())
            else:
                scores.append(0.5)
        return pd.Series(scores, index=group.index)
    
    print("  - è·é›¢é©æ€§ã‚¹ã‚³ã‚¢è¨ˆç®—ä¸­...")
    df_sorted['distance_category_score'] = df_sorted.groupby('ketto_toroku_bango', group_keys=False).apply(
        calc_distance_category_score
    ).values
    
    df_sorted['similar_distance_score'] = df_sorted.groupby('ketto_toroku_bango', group_keys=False).apply(
        calc_similar_distance_score
    ).values
    
    print("  - é¦¬å ´é©æ€§ã‚¹ã‚³ã‚¢è¨ˆç®—ä¸­...")
    df_sorted['surface_aptitude_score'] = df_sorted.groupby('ketto_toroku_bango', group_keys=False).apply(
        calc_surface_score
    ).values
    
    # å…ƒã®é †åºã«æˆ»ã™
    df['distance_category_score'] = df_sorted.sort_index()['distance_category_score']
    df['similar_distance_score'] = df_sorted.sort_index()['similar_distance_score']
    df['surface_aptitude_score'] = df_sorted.sort_index()['surface_aptitude_score']
    
    # distance_change_adaptabilityè¿½åŠ 
    def calc_distance_change_adaptability(group):
        scores = []
        for idx in range(len(group)):
            if idx < 2:
                scores.append(0.5)
                continue
            past_races = group.iloc[max(0, idx-6):idx].copy()
            if len(past_races) >= 3:
                past_races['kyori_diff'] = past_races['kyori'].diff().abs()
                past_races_eval = past_races.tail(5)
                changed_races = past_races_eval[past_races_eval['kyori_diff'] >= 100]
                if len(changed_races) > 0:
                    scores.append((1 - (changed_races['kakutei_chakujun_numeric'] / 18.0)).mean())
                else:
                    scores.append(0.5)
            else:
                scores.append(0.5)
        return pd.Series(scores, index=group.index)
    
    df_sorted['distance_change_adaptability'] = df_sorted.groupby('ketto_toroku_bango', group_keys=False).apply(
        calc_distance_change_adaptability
    ).values
    df['distance_change_adaptability'] = df_sorted.sort_index()['distance_change_adaptability']
    
    # baba_condition_scoreè¿½åŠ 
    def calc_baba_condition_score(group):
        scores = []
        for idx in range(len(group)):
            if idx == 0:
                scores.append(0.5)
                continue
            current_condition = group.iloc[idx]['baba_condition']
            past_same = group.iloc[:idx][group.iloc[:idx]['baba_condition'] == current_condition].tail(10)
            if len(past_same) > 0:
                scores.append((1 - (past_same['kakutei_chakujun_numeric'] / 18.0)).mean())
            else:
                scores.append(0.5)
        return pd.Series(scores, index=group.index)
    
    df_sorted['baba_condition_score'] = df_sorted.groupby('ketto_toroku_bango', group_keys=False).apply(
        calc_baba_condition_score
    ).values
    df['baba_condition_score'] = df_sorted.sort_index()['baba_condition_score']
    
    # baba_change_adaptabilityè¿½åŠ 
    def calc_baba_change_adaptability(group):
        scores = []
        for idx in range(len(group)):
            if idx < 2:
                scores.append(0.5)
                continue
            past_races = group.iloc[max(0, idx-6):idx].copy()
            if len(past_races) >= 3:
                past_races['baba_changed'] = past_races['baba_condition'].shift(1) != past_races['baba_condition']
                past_races_eval = past_races.tail(5)
                changed_races = past_races_eval[past_races_eval['baba_changed'] == True]
                if len(changed_races) > 0:
                    scores.append((1 - (changed_races['kakutei_chakujun_numeric'] / 18.0)).mean())
                else:
                    scores.append(0.5)
            else:
                scores.append(0.5)
        return pd.Series(scores, index=group.index)
    
    df_sorted['baba_change_adaptability'] = df_sorted.groupby('ketto_toroku_bango', group_keys=False).apply(
        calc_baba_change_adaptability
    ).values
    df['baba_change_adaptability'] = df_sorted.sort_index()['baba_change_adaptability']
    
    X['distance_category_score'] = df['distance_category_score']
    X['similar_distance_score'] = df['similar_distance_score']
    X['surface_aptitude_score'] = df['surface_aptitude_score']
    X['distance_change_adaptability'] = df['distance_change_adaptability']
    X['baba_condition_score'] = df['baba_condition_score']
    X['baba_change_adaptability'] = df['baba_change_adaptability']
    
    # é¨æ‰‹ãƒ»èª¿æ•™å¸«ã‚¹ã‚³ã‚¢ï¼ˆç°¡æ˜“ç‰ˆ - å…¨ä½“çµ±è¨ˆãƒ™ãƒ¼ã‚¹ï¼‰
    print("  - é¨æ‰‹ãƒ»èª¿æ•™å¸«ã‚¹ã‚³ã‚¢è¨ˆç®—ä¸­...")
    df_sorted_kishu = df.sort_values(['kishu_code', 'kaisai_nen', 'kaisai_tsukihi', 'race_bango']).copy()
    
    def calc_kishu_skill_score(group):
        scores = []
        for idx in range(len(group)):
            if pd.isna(group.iloc[idx]['kishu_code']) or group.iloc[idx]['kishu_code'] == '':
                scores.append(0.5)
                continue
            past_races = group.iloc[:idx]
            if len(past_races) >= 3:
                avg_score = (1.0 - ((18 - past_races['kakutei_chakujun_numeric'] + 1) / 18.0)).mean()
                scores.append(max(0.0, min(1.0, avg_score)))
            else:
                scores.append(0.5)
        return pd.Series(scores, index=group.index)
    
    def calc_kishu_surface_score(group):
        scores = []
        for idx in range(len(group)):
            if pd.isna(group.iloc[idx]['kishu_code']) or group.iloc[idx]['kishu_code'] == '':
                scores.append(0.5)
                continue
            current_surface = group.iloc[idx]['surface_type']
            past_races = group.iloc[:idx]
            past_same_surface = past_races[past_races['surface_type'] == current_surface]
            if len(past_same_surface) >= 5:
                avg_score = (1 - ((18 - past_same_surface['kakutei_chakujun_numeric'] + 1) / 18.0)).mean()
                scores.append(avg_score)
            else:
                scores.append(0.5)
        return pd.Series(scores, index=group.index)
    
    df_sorted_kishu['kishu_skill_score'] = df_sorted_kishu.groupby('kishu_code', group_keys=False).apply(
        calc_kishu_skill_score
    ).values
    
    df_sorted_kishu['kishu_surface_score'] = df_sorted_kishu.groupby('kishu_code', group_keys=False).apply(
        calc_kishu_surface_score
    ).values
    
    # kishu_popularity_scoreè¿½åŠ 
    def calc_kishu_popularity_score(group):
        scores = []
        for idx in range(len(group)):
            if pd.isna(group.iloc[idx]['kishu_code']) or group.iloc[idx]['kishu_code'] == '':
                scores.append(0.5)
                continue
            past_races = group.iloc[:idx]
            if len(past_races) >= 3:
                valid_races = past_races[past_races['tansho_odds'] > 0]
                if len(valid_races) >= 3:
                    max_odds = valid_races['tansho_odds'].max()
                    valid_races = valid_races.copy()
                    valid_races['odds_expectation'] = 1.0 - (valid_races['tansho_odds'] / (max_odds + 1.0))
                    valid_races['actual_score'] = 1.0 - ((18 - valid_races['kakutei_chakujun_numeric'] + 1) / 18.0)
                    valid_races['performance_diff'] = valid_races['actual_score'] - valid_races['odds_expectation']
                    avg_diff = valid_races['performance_diff'].mean()
                    normalized_score = 0.5 + (avg_diff * 0.5)
                    scores.append(max(0.0, min(1.0, normalized_score)))
                else:
                    scores.append(0.5)
            else:
                scores.append(0.5)
        return pd.Series(scores, index=group.index)
    
    df_sorted_kishu['kishu_popularity_score'] = df_sorted_kishu.groupby('kishu_code', group_keys=False).apply(
        calc_kishu_popularity_score
    ).values
    
    df['kishu_skill_score'] = df_sorted_kishu.sort_index()['kishu_skill_score']
    df['kishu_surface_score'] = df_sorted_kishu.sort_index()['kishu_surface_score']
    df['kishu_popularity_score'] = df_sorted_kishu.sort_index()['kishu_popularity_score']
    
    # chokyoshi_recent_scoreè¿½åŠ 
    df_sorted_chokyoshi = df.sort_values(['chokyoshi_code', 'kaisai_nen', 'kaisai_tsukihi', 'race_bango']).copy()
    
    def calc_chokyoshi_recent_score(group):
        scores = []
        for idx in range(len(group)):
            if pd.isna(group.iloc[idx]['chokyoshi_code']) or group.iloc[idx]['chokyoshi_code'] == '':
                scores.append(0.5)
                continue
            past_races = group.iloc[:idx]
            if len(past_races) >= 5:
                avg_score = (1 - ((18 - past_races['kakutei_chakujun_numeric'] + 1) / 18.0)).mean()
                scores.append(avg_score)
            else:
                scores.append(0.5)
        return pd.Series(scores, index=group.index)
    
    df_sorted_chokyoshi['chokyoshi_recent_score'] = df_sorted_chokyoshi.groupby('chokyoshi_code', group_keys=False).apply(
        calc_chokyoshi_recent_score
    ).values
    
    df['chokyoshi_recent_score'] = df_sorted_chokyoshi.sort_index()['chokyoshi_recent_score']
    
    X['kishu_skill_score'] = df['kishu_skill_score']
    X['kishu_surface_score'] = df['kishu_surface_score']
    X['kishu_popularity_score'] = df['kishu_popularity_score']
    X['chokyoshi_recent_score'] = df['chokyoshi_recent_score']
    
    print(f"[OK] ç‰¹å¾´é‡è¨ˆç®—å®Œäº†: {len(X.columns)}å€‹")
    
    return X


def analyze_shap_global(model, X, feature_names, output_prefix):
    """
    SHAPå…¨ä½“åˆ†æï¼ˆç‰¹å¾´é‡é‡è¦åº¦ã€ä¾å­˜æ€§ãƒ—ãƒ­ãƒƒãƒˆï¼‰
    
    Args:
        model: LightGBMãƒ¢ãƒ‡ãƒ«
        X: ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿
        feature_names: ç‰¹å¾´é‡åãƒªã‚¹ãƒˆ
        output_prefix: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹
    """
    print("\n[+] SHAPå…¨ä½“åˆ†æã‚’å®Ÿè¡Œä¸­...")
    
    # SHAPå€¤è¨ˆç®—
    explainer = shap.TreeExplainer(model)
    shap_values = explainer.shap_values(X)
    
    # 1. Summary Plotï¼ˆç‰¹å¾´é‡é‡è¦åº¦ã¨åˆ†å¸ƒï¼‰
    print("  - Summary Plotä½œæˆä¸­...")
    plt.figure(figsize=(12, 8))
    shap.summary_plot(shap_values, X, feature_names=feature_names, show=False)
    plt.title('SHAP Summary Plot - ç‰¹å¾´é‡ã®å½±éŸ¿åº¦ã¨åˆ†å¸ƒ', fontsize=14, pad=20)
    plt.tight_layout()
    plt.savefig(PLOT_DIR / f'{output_prefix}_summary.png', dpi=150, bbox_inches='tight')
    plt.close()
    print(f"    [OK] ä¿å­˜: {PLOT_DIR / f'{output_prefix}_summary.png'}")
    
    # 2. Bar Plotï¼ˆå¹³å‡çµ¶å¯¾SHAPå€¤ï¼‰
    print("  - Bar Plotä½œæˆä¸­...")
    plt.figure(figsize=(10, 8))
    shap.summary_plot(shap_values, X, feature_names=feature_names, plot_type="bar", show=False)
    plt.title('SHAP Bar Plot - ç‰¹å¾´é‡ã®å¹³å‡å½±éŸ¿åº¦', fontsize=14, pad=20)
    plt.tight_layout()
    plt.savefig(PLOT_DIR / f'{output_prefix}_bar.png', dpi=150, bbox_inches='tight')
    plt.close()
    print(f"    [OK] ä¿å­˜: {PLOT_DIR / f'{output_prefix}_bar.png'}")
    
    # 3. ä¸Šä½5ç‰¹å¾´é‡ã®ä¾å­˜æ€§ãƒ—ãƒ­ãƒƒãƒˆ
    print("  - Dependence Plotä½œæˆä¸­...")
    mean_abs_shap = np.abs(shap_values).mean(axis=0)
    top_features_idx = np.argsort(mean_abs_shap)[-5:][::-1]
    
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    axes = axes.flatten()
    
    for i, idx in enumerate(top_features_idx):
        shap.dependence_plot(idx, shap_values, X, feature_names=feature_names, 
                            ax=axes[i], show=False)
        axes[i].set_title(f'{feature_names[idx]} ã®ä¾å­˜æ€§', fontsize=12)
    
    # æœ€å¾Œã®ã‚µãƒ–ãƒ—ãƒ­ãƒƒãƒˆã‚’éè¡¨ç¤º
    axes[-1].axis('off')
    
    plt.tight_layout()
    plt.savefig(PLOT_DIR / f'{output_prefix}_dependence.png', dpi=150, bbox_inches='tight')
    plt.close()
    print(f"    [OK] ä¿å­˜: {PLOT_DIR / f'{output_prefix}_dependence.png'}")
    
    # 4. ç‰¹å¾´é‡é‡è¦åº¦ã‚’CSVå‡ºåŠ›
    feature_importance_df = pd.DataFrame({
        'feature': feature_names,
        'mean_abs_shap': mean_abs_shap,
        'lgb_gain': model.feature_importance(importance_type='gain')
    }).sort_values('mean_abs_shap', ascending=False)
    
    csv_path = PLOT_DIR / f'{output_prefix}_importance.csv'
    feature_importance_df.to_csv(csv_path, index=False, encoding='utf-8-sig')
    print(f"    [OK] ç‰¹å¾´é‡é‡è¦åº¦ä¿å­˜: {csv_path}")
    
    print("\n[LIST] ç‰¹å¾´é‡é‡è¦åº¦ãƒˆãƒƒãƒ—10:")
    print(feature_importance_df.head(10).to_string(index=False))
    
    return shap_values, explainer


def analyze_shap_individual(shap_values, explainer, X, df_full, feature_names, 
                            output_prefix, num_samples=5):
    """
    å€‹åˆ¥ãƒ¬ãƒ¼ã‚¹ã®SHAPåˆ†æ
    
    Args:
        shap_values: SHAPå€¤é…åˆ—
        explainer: SHAPExplainer
        X: ç‰¹å¾´é‡ãƒ‡ãƒ¼ã‚¿
        df_full: å…ƒãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ï¼ˆé¦¬åãªã©ã®æƒ…å ±å«ã‚€ï¼‰
        feature_names: ç‰¹å¾´é‡åãƒªã‚¹ãƒˆ
        output_prefix: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹
        num_samples: åˆ†æã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«æ•°
    """
    print(f"\n[TEST] å€‹åˆ¥ãƒ¬ãƒ¼ã‚¹åˆ†æï¼ˆã‚µãƒ³ãƒ—ãƒ«{num_samples}ä»¶ï¼‰...")
    
    # ãƒ©ãƒ³ãƒ€ãƒ ã«ã‚µãƒ³ãƒ—ãƒ«é¸æŠ
    sample_indices = np.random.choice(len(X), min(num_samples, len(X)), replace=False)
    
    for i, idx in enumerate(sample_indices):
        print(f"\n--- ã‚µãƒ³ãƒ—ãƒ« {i+1}/{num_samples} ---")
        
        # ãƒ¬ãƒ¼ã‚¹æƒ…å ±
        race_info = df_full.iloc[idx]
        print(f"æ—¥ä»˜: {race_info['kaisai_nen']}/{race_info['kaisai_tsukihi']}")
        print(f"ç«¶é¦¬å ´: {race_info['keibajo_name']} R{race_info['race_bango']}")
        print(f"é¦¬å: {race_info['bamei']}")
        print(f"å®Ÿéš›ã®ç€é †: {race_info['kakutei_chakujun']:.0f}ç€")
        print(f"äººæ°—: {race_info['tansho_ninkijun_numeric']:.0f}ç•ªäººæ°—")
        
        # Force Plot
        shap.force_plot(
            explainer.expected_value, 
            shap_values[idx], 
            X.iloc[idx],
            feature_names=feature_names,
            matplotlib=True,
            show=False
        )
        plt.title(f"{race_info['bamei']} - SHAP Force Plot", fontsize=12, pad=10)
        plt.tight_layout()
        plt.savefig(PLOT_DIR / f'{output_prefix}_force_{i+1}.png', dpi=150, bbox_inches='tight')
        plt.close()
        
        # è²¢çŒ®åº¦ãƒˆãƒƒãƒ—10ã‚’è¡¨ç¤º
        shap_contributions = pd.DataFrame({
            'feature': feature_names,
            'value': X.iloc[idx].values,
            'shap_value': shap_values[idx]
        })
        shap_contributions['abs_shap'] = np.abs(shap_contributions['shap_value'])
        shap_contributions = shap_contributions.sort_values('abs_shap', ascending=False)
        
        print("\nè²¢çŒ®åº¦ãƒˆãƒƒãƒ—10:")
        for _, row in shap_contributions.head(10).iterrows():
            direction = "â†‘" if row['shap_value'] > 0 else "â†“"
            print(f"  {row['feature']:30s}: {row['value']:8.2f} â†’ SHAP={row['shap_value']:+8.4f} {direction}")
        
        print(f"  [OK] Force Plotä¿å­˜: {PLOT_DIR / f'{output_prefix}_force_{i+1}.png'}")


def main():
    """
    ãƒ¡ã‚¤ãƒ³å‡¦ç†
    """
    import sys
    
    print("=" * 80)
    print("[TARGET] SHAPåˆ†æã«ã‚ˆã‚‹ç«¶é¦¬äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«èª¬æ˜")
    print("=" * 80)
    
    # åˆ†æå¯¾è±¡ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ
    models = get_all_models()
    
    if not models:
        print("[ERROR] model_configs.jsonã«ãƒ¢ãƒ‡ãƒ«ãŒå®šç¾©ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return
    
    print("\nåˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«:")
    for i, model_info in enumerate(models, 1):
        desc = format_model_description(
            model_info['track_code'],
            model_info['kyoso_shubetsu_code'],
            model_info['surface_type'],
            model_info['min_distance'],
            model_info['max_distance']
        )
        print(f"  {i}. {desc}")
    
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«åã¨å¯¾è±¡å¹´ã‚’å–å¾—
    target_model_filename = None
    test_year = 2023  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯2023å¹´
    
    if len(sys.argv) >= 2:
        target_model_filename = sys.argv[1]
    if len(sys.argv) >= 3:
        try:
            test_year = int(sys.argv[2])
        except ValueError:
            print(f"[WARNING] å¹´ã®æŒ‡å®šãŒä¸æ­£ã§ã™: {sys.argv[2]}. ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ2023å¹´ã‚’ä½¿ç”¨ã—ã¾ã™")
    
    # æŒ‡å®šã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’æ¤œç´¢ï¼ˆæŒ‡å®šãªã—ã®å ´åˆã¯æœ€åˆã®ãƒ¢ãƒ‡ãƒ«ï¼‰
    model_info = None
    if target_model_filename:
        for m in models:
            if m['model_filename'] == target_model_filename:
                model_info = m
                break
        if not model_info:
            print(f"[WARNING] ãƒ¢ãƒ‡ãƒ« {target_model_filename} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚æœ€åˆã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™")
            model_info = models[0]
    else:
        model_info = models[0]
    
    print(f"\n[PIN] åˆ†æå¯¾è±¡: {format_model_description(model_info['track_code'], model_info['kyoso_shubetsu_code'], model_info['surface_type'], model_info['min_distance'], model_info['max_distance'])}")
    print(f"[PIN] å¯¾è±¡å¹´: {test_year}å¹´")
    
    # ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    model, X, y, df_full = load_model_and_data(
        model_filename=model_info['model_filename'],
        track_code=model_info['track_code'],
        kyoso_shubetsu_code=model_info['kyoso_shubetsu_code'],
        surface_type=model_info['surface_type'],
        min_distance=model_info['min_distance'],
        max_distance=model_info['max_distance'],
        test_year=test_year,
        sample_size=500  # è¨ˆç®—æ™‚é–“çŸ­ç¸®ã®ãŸã‚500ä»¶ã«åˆ¶é™
    )
    
    if model is None:
        print("[ERROR] ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
        return
    
    # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åã®ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹
    output_prefix = Path(model_info['model_filename']).stem
    
    # SHAPå…¨ä½“åˆ†æ
    shap_values, explainer = analyze_shap_global(
        model=model,
        X=X,
        feature_names=X.columns.tolist(),
        output_prefix=output_prefix
    )
    
    # å€‹åˆ¥ãƒ¬ãƒ¼ã‚¹åˆ†æ
    analyze_shap_individual(
        shap_values=shap_values,
        explainer=explainer,
        X=X,
        df_full=df_full,
        feature_names=X.columns.tolist(),
        output_prefix=output_prefix,
        num_samples=5
    )
    
    print("\n" + "=" * 80)
    print("[OK] SHAPåˆ†æå®Œäº†!")
    print(f"[FILE] çµæœä¿å­˜å…ˆ: {PLOT_DIR.absolute()}")
    print("=" * 80)


if __name__ == '__main__':
    main()
