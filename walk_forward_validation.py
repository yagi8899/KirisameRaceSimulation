"""
Walk-Forward Validation ã‚·ã‚¹ãƒ†ãƒ 

ç«¶é¦¬äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã®Walk-Forward Validationã‚’è‡ªå‹•åŒ–ã—ã€
æœ€é©ãªå­¦ç¿’æœŸé–“ã®æ±ºå®šã¨ãƒ¢ãƒ‡ãƒ«ã®æ±åŒ–æ€§èƒ½è©•ä¾¡ã‚’å®Ÿç¾ã™ã‚‹ã€‚

Phase 2.5å¯¾å¿œ: Rankerã¨ç©´é¦¬åˆ†é¡å™¨ã‚’åŒæ™‚ã«å­¦ç¿’ãƒ»è©•ä¾¡

ä½¿ç”¨æ–¹æ³•:
    python walk_forward_validation.py
    python walk_forward_validation.py --config my_config.json
    python walk_forward_validation.py --resume
    python walk_forward_validation.py --dry-run
"""

import json
import os
import sys
import logging
import argparse
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
import pandas as pd
import traceback
import multiprocessing
import threading
from concurrent.futures import ProcessPoolExecutor, as_completed

# æ—¢å­˜ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from model_creator import create_universal_model
from model_config_loader import load_model_configs
import universal_test

# Phase 2.5: ç©´é¦¬åˆ†é¡å™¨é–¢é€£ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from upset_classifier_creator import (
    prepare_features,
    train_with_class_weights
)
from analyze_upset_patterns import (
    get_data_with_predictions,
    create_training_dataset
)


class WalkForwardValidator:
    """Walk-Forward Validationã‚’å®Ÿè¡Œã™ã‚‹ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, config_path: str = "walk_forward_config.json"):
        """
        åˆæœŸåŒ–
        
        Args:
            config_path: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        """
        self.config_path = config_path
        self.config = self._load_config()
        self.wfv_config = self.config['walk_forward_validation']
        self.model_configs = self._load_model_configs()
        self.progress_file = None
        self.progress_data = {}
        self.logger = None
        
        # Phase 2: progress.jsonæ’ä»–åˆ¶å¾¡ç”¨ãƒ­ãƒƒã‚¯
        self.progress_lock = threading.Lock()
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®è¨­å®š
        self.output_dir = Path(self.wfv_config['output_dir'])
        
        # ãƒ­ã‚°è¨­å®š
        self._setup_logging()
        
    def _load_config(self) -> Dict:
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
        if not os.path.exists(self.config_path):
            raise FileNotFoundError(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.config_path}")
        
        with open(self.config_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def _load_model_configs(self) -> Dict:
        """model_configs.jsonã‚’èª­ã¿è¾¼ã‚€"""
        return load_model_configs()
    
    def _setup_logging(self):
        """ãƒ­ã‚®ãƒ³ã‚°ã‚’è¨­å®š"""
        log_config = self.wfv_config.get('logging', {})
        log_level = getattr(logging, log_config.get('level', 'INFO'))
        
        # ãƒ­ã‚¬ãƒ¼ã®ä½œæˆ
        self.logger = logging.getLogger('WalkForwardValidation')
        self.logger.setLevel(log_level)
        
        # ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’ã‚¯ãƒªã‚¢
        self.logger.handlers.clear()
        
        # ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ãƒ¼
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
        
        # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
        if log_config.get('console', True):
            console_handler = logging.StreamHandler()
            console_handler.setFormatter(formatter)
            self.logger.addHandler(console_handler)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
        if 'file' in log_config:
            log_file = Path(log_config['file'])
            log_file.parent.mkdir(parents=True, exist_ok=True)
            file_handler = logging.FileHandler(log_file, encoding='utf-8')
            file_handler.setFormatter(formatter)
            self.logger.addHandler(file_handler)
    
    def _get_model_filename(self, base_name: str, train_start: int, train_end: int) -> str:
        """
        ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«åã‚’çµ±ä¸€çš„ã«ç”Ÿæˆ
        
        Args:
            base_name: ãƒ¢ãƒ‡ãƒ«ã®ãƒ™ãƒ¼ã‚¹å
            train_start: å­¦ç¿’é–‹å§‹å¹´
            train_end: å­¦ç¿’çµ‚äº†å¹´
            
        Returns:
            ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«å
        """
        return f"{base_name}_{train_start}-{train_end}.sav"
    
    def _filter_models(self, models_setting: Any) -> List[str]:
        """
        modelè¨­å®šã«åŸºã¥ã„ã¦ãƒ¢ãƒ‡ãƒ«ãƒªã‚¹ãƒˆã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        
        Args:
            models_setting: "all", "standard", "custom", ã¾ãŸã¯ãƒ¢ãƒ‡ãƒ«åã®ãƒªã‚¹ãƒˆ
            
        Returns:
            å¯¾è±¡ãƒ¢ãƒ‡ãƒ«åã®ãƒªã‚¹ãƒˆ
        """
        if models_setting == "all":
            # æ¨™æº–ãƒ¢ãƒ‡ãƒ«
            standard_list = self.model_configs.get('standard_models', [])
            standard_names = [m['model_filename'].replace('.sav', '') for m in standard_list]
            # ã‚«ã‚¹ã‚¿ãƒ ãƒ¢ãƒ‡ãƒ«
            custom_list = self.model_configs.get('custom_models', [])
            custom_names = [m['model_filename'].replace('.sav', '') for m in custom_list]
            return standard_names + custom_names
        
        elif models_setting == "standard":
            standard_list = self.model_configs.get('standard_models', [])
            return [m['model_filename'].replace('.sav', '') for m in standard_list]
        
        elif models_setting == "custom":
            custom_list = self.model_configs.get('custom_models', [])
            return [m['model_filename'].replace('.sav', '') for m in custom_list]
        
        elif isinstance(models_setting, list):
            return models_setting
        
        else:
            self.logger.warning(f"ä¸æ˜ãªmodelsè¨­å®š: {models_setting}ã€‚æ¨™æº–ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
            standard_list = self.model_configs.get('standard_models', [])
            return [m['model_filename'].replace('.sav', '') for m in standard_list]
    
    def _get_model_config(self, model_name: str) -> Optional[Dict]:
        """
        ãƒ¢ãƒ‡ãƒ«åã‹ã‚‰è¨­å®šã‚’å–å¾—
        
        Args:
            model_name: ãƒ¢ãƒ‡ãƒ«åï¼ˆæ‹¡å¼µå­ãªã—ï¼‰
            
        Returns:
            ãƒ¢ãƒ‡ãƒ«è¨­å®šè¾æ›¸ã€è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯None
        """
        # æ¨™æº–ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰æ¤œç´¢
        for model in self.model_configs.get('standard_models', []):
            if model['model_filename'].replace('.sav', '') == model_name:
                return model
        
        # ã‚«ã‚¹ã‚¿ãƒ ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰æ¤œç´¢
        for model in self.model_configs.get('custom_models', []):
            if model['model_filename'].replace('.sav', '') == model_name:
                return model
        
        self.logger.error(f"ãƒ¢ãƒ‡ãƒ«è¨­å®šãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {model_name}")
        return None
    
    def _get_universal_model_config(self) -> Optional[Dict]:
        """
        Universal Rankerï¼ˆç©´é¦¬åˆ†é¡å™¨ç”¨ï¼‰ã®è¨­å®šã‚’å–å¾—
        
        Returns:
            ãƒ¢ãƒ‡ãƒ«è¨­å®šè¾æ›¸ã€è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯None
        """
        upset_models = self.model_configs.get('upset_classifier_models', [])
        if len(upset_models) > 0:
            return upset_models[0]  # æœ€åˆã®1ã¤ã‚’ä½¿ç”¨
        
        self.logger.error("upset_classifier_modelsè¨­å®šãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return None
        return None
    
    def _load_progress(self, progress_file: Path) -> Dict:
        """é€²æ—ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
        if progress_file.exists():
            with open(progress_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {}
    
    def _save_progress(self):
        """é€²æ—ã‚’ä¿å­˜ (Phase 2: ä¸¦åˆ—å‡¦ç†å¯¾å¿œã§ãƒ­ãƒƒã‚¯ä»˜ã)"""
        if self.progress_file:
            with self.progress_lock:
                self.progress_data['last_updated'] = datetime.now().isoformat()
                with open(self.progress_file, 'w', encoding='utf-8') as f:
                    json.dump(self.progress_data, f, indent=2, ensure_ascii=False)
    
    def _calculate_betting_results(self, buy_horses: pd.DataFrame, full_df: pd.DataFrame) -> Dict:
        """
        é¦¬åˆ¸ç¨®åˆ¥ã”ã¨ã®çš„ä¸­ç‡ãƒ»å›åç‡ã‚’è¨ˆç®—ï¼ˆPhase 2.5å¯¾å¿œ: ç©´é¦¬äºˆæ¸¬å«ã‚€ï¼‰
        
        Args:
            buy_horses: è³¼å…¥æ¨å¥¨é¦¬ã®DataFrame
            full_df: å…¨é¦¬ã®DataFrame
            
        Returns:
            é›†è¨ˆçµæœã®è¾æ›¸
        """
        results = {}
        buy_count = len(buy_horses)
        
        if buy_count == 0:
            return {
                'tansho_hit': 0, 'tansho_rate': 0, 'tansho_return': 0,
                'fukusho_hit': 0, 'fukusho_rate': 0, 'fukusho_return': 0,
                'upset_candidates': 0, 'upset_hits': 0, 'upset_precision': 0, 'upset_roi': 0
            }
        
        # å˜å‹
        tansho_hit = len(buy_horses[buy_horses['ç¢ºå®šç€é †'] == 1])
        tansho_rate = tansho_hit / buy_count
        tansho_return = (buy_horses[buy_horses['ç¢ºå®šç€é †'] == 1]['å˜å‹ã‚ªãƒƒã‚º'].sum()) / buy_count
        
        results['tansho_hit'] = tansho_hit
        results['tansho_rate'] = tansho_rate
        results['tansho_return'] = tansho_return
        
        # è¤‡å‹ï¼ˆ1-3ç€ï¼‰
        fukusho_hit = len(buy_horses[buy_horses['ç¢ºå®šç€é †'] <= 3])
        fukusho_rate = fukusho_hit / buy_count
        
        # è¤‡å‹ã‚ªãƒƒã‚ºã®è¨ˆç®—ï¼ˆè¤‡å‹1ç€ï½3ç€ã®ã‚ªãƒƒã‚ºã‹ã‚‰è©²å½“ã™ã‚‹ã‚‚ã®ã‚’å–å¾—ï¼‰
        fukusho_return_total = 0
        for _, horse in buy_horses.iterrows():
            chakujun = horse['ç¢ºå®šç€é †']
            if chakujun <= 3:
                # è¤‡å‹ã‚ªãƒƒã‚ºã‚’å–å¾—
                if chakujun == 1 and 'è¤‡å‹1ç€ã‚ªãƒƒã‚º' in horse and pd.notna(horse['è¤‡å‹1ç€ã‚ªãƒƒã‚º']):
                    fukusho_return_total += horse['è¤‡å‹1ç€ã‚ªãƒƒã‚º']
                elif chakujun == 2 and 'è¤‡å‹2ç€ã‚ªãƒƒã‚º' in horse and pd.notna(horse['è¤‡å‹2ç€ã‚ªãƒƒã‚º']):
                    fukusho_return_total += horse['è¤‡å‹2ç€ã‚ªãƒƒã‚º']
                elif chakujun == 3 and 'è¤‡å‹3ç€ã‚ªãƒƒã‚º' in horse and pd.notna(horse['è¤‡å‹3ç€ã‚ªãƒƒã‚º']):
                    fukusho_return_total += horse['è¤‡å‹3ç€ã‚ªãƒƒã‚º']
        
        fukusho_return = fukusho_return_total / buy_count
        
        results['fukusho_hit'] = fukusho_hit
        results['fukusho_rate'] = fukusho_rate
        results['fukusho_return'] = fukusho_return
        
        # é¦¬é€£ãƒ»ãƒ¯ã‚¤ãƒ‰ï¼ˆãƒ¬ãƒ¼ã‚¹ã”ã¨ã«è³¼å…¥æ¨å¥¨é¦¬ãŒ2é ­ä»¥ä¸Šã„ã‚‹å ´åˆã®ã¿ï¼‰
        race_groups = buy_horses.groupby(['é–‹å‚¬å¹´', 'é–‹å‚¬æ—¥', 'ç«¶é¦¬å ´', 'ãƒ¬ãƒ¼ã‚¹ç•ªå·'])
        
        umaren_hit = 0
        umaren_bets = 0
        umaren_return_total = 0
        
        wide_hit = 0
        wide_bets = 0
        wide_return_total = 0
        
        for race_id, race_buy_horses in race_groups:
            if len(race_buy_horses) >= 2:
                # è³¼å…¥æ¨å¥¨é¦¬ã®çµ„ã¿åˆã‚ã›æ•°
                from itertools import combinations
                combos = list(combinations(race_buy_horses['é¦¬ç•ª'].tolist(), 2))
                umaren_bets += len(combos)
                wide_bets += len(combos)
                
                # ã“ã®ãƒ¬ãƒ¼ã‚¹ã®å…¨é¦¬æƒ…å ±ã‚’å–å¾—
                race_full = full_df[
                    (full_df['é–‹å‚¬å¹´'] == race_id[0]) &
                    (full_df['é–‹å‚¬æ—¥'] == race_id[1]) &
                    (full_df['ç«¶é¦¬å ´'] == race_id[2]) &
                    (full_df['ãƒ¬ãƒ¼ã‚¹ç•ªå·'] == race_id[3])
                ]
                
                if len(race_full) == 0:
                    continue
                
                # é¦¬é€£ãƒ»ãƒ¯ã‚¤ãƒ‰ã®çš„ä¸­åˆ¤å®š
                race_sample = race_full.iloc[0]
                
                # é¦¬é€£
                if 'é¦¬é€£é¦¬ç•ª1' in race_sample and pd.notna(race_sample['é¦¬é€£é¦¬ç•ª1']):
                    umaren_winning = (int(race_sample['é¦¬é€£é¦¬ç•ª1']), int(race_sample['é¦¬é€£é¦¬ç•ª2']))
                    for combo in combos:
                        if set(combo) == set(umaren_winning):
                            umaren_hit += 1
                            if 'é¦¬é€£ã‚ªãƒƒã‚º' in race_sample and pd.notna(race_sample['é¦¬é€£ã‚ªãƒƒã‚º']):
                                umaren_return_total += race_sample['é¦¬é€£ã‚ªãƒƒã‚º']
                            break
                
                # ãƒ¯ã‚¤ãƒ‰ï¼ˆ1-2ç€ã€2-3ç€ã€1-3ç€ã®3é€šã‚Šï¼‰
                wide_winning_pairs = []
                if 'ãƒ¯ã‚¤ãƒ‰1_2é¦¬ç•ª1' in race_sample and pd.notna(race_sample['ãƒ¯ã‚¤ãƒ‰1_2é¦¬ç•ª1']):
                    wide_winning_pairs.append((
                        (int(race_sample['ãƒ¯ã‚¤ãƒ‰1_2é¦¬ç•ª1']), int(race_sample['ãƒ¯ã‚¤ãƒ‰1_2é¦¬ç•ª2'])),
                        race_sample.get('ãƒ¯ã‚¤ãƒ‰1_2ã‚ªãƒƒã‚º', 0)
                    ))
                if 'ãƒ¯ã‚¤ãƒ‰2_3ç€é¦¬ç•ª1' in race_sample and pd.notna(race_sample['ãƒ¯ã‚¤ãƒ‰2_3ç€é¦¬ç•ª1']):
                    wide_winning_pairs.append((
                        (int(race_sample['ãƒ¯ã‚¤ãƒ‰2_3ç€é¦¬ç•ª1']), int(race_sample['ãƒ¯ã‚¤ãƒ‰2_3ç€é¦¬ç•ª2'])),
                        race_sample.get('ãƒ¯ã‚¤ãƒ‰2_3ã‚ªãƒƒã‚º', 0)
                    ))
                if 'ãƒ¯ã‚¤ãƒ‰1_3ç€é¦¬ç•ª1' in race_sample and pd.notna(race_sample['ãƒ¯ã‚¤ãƒ‰1_3ç€é¦¬ç•ª1']):
                    wide_winning_pairs.append((
                        (int(race_sample['ãƒ¯ã‚¤ãƒ‰1_3ç€é¦¬ç•ª1']), int(race_sample['ãƒ¯ã‚¤ãƒ‰1_3ç€é¦¬ç•ª2'])),
                        race_sample.get('ãƒ¯ã‚¤ãƒ‰1_3ã‚ªãƒƒã‚º', 0)
                    ))
                
                for combo in combos:
                    for winning_pair, odds in wide_winning_pairs:
                        if set(combo) == set(winning_pair):
                            wide_hit += 1
                            if pd.notna(odds):
                                wide_return_total += odds
                            break
        
        # é¦¬é€£
        results['umaren_hit'] = umaren_hit
        results['umaren_rate'] = umaren_hit / umaren_bets if umaren_bets > 0 else 0
        results['umaren_return'] = umaren_return_total / umaren_bets if umaren_bets > 0 else 0
        
        # ãƒ¯ã‚¤ãƒ‰
        results['wide_hit'] = wide_hit
        results['wide_rate'] = wide_hit / wide_bets if wide_bets > 0 else 0
        results['wide_return'] = wide_return_total / wide_bets if wide_bets > 0 else 0
        
        # Phase 2.5: ç©´é¦¬äºˆæ¸¬ã®åˆ†æ
        if 'ç©´é¦¬å€™è£œ' in full_df.columns and 'å®Ÿéš›ã®ç©´é¦¬' in full_df.columns:
            upset_candidates = full_df[full_df['ç©´é¦¬å€™è£œ'] == 1]
            upset_actual = full_df[full_df['å®Ÿéš›ã®ç©´é¦¬'] == 1]
            upset_hits = upset_candidates[upset_candidates['å®Ÿéš›ã®ç©´é¦¬'] == 1]
            
            upset_count = len(upset_candidates)
            upset_hit_count = len(upset_hits)
            upset_precision = (upset_hit_count / upset_count * 100) if upset_count > 0 else 0
            upset_recall = (upset_hit_count / len(upset_actual) * 100) if len(upset_actual) > 0 else 0
            
            # ç©´é¦¬ROIè¨ˆç®—ï¼ˆå˜å‹è³¼å…¥æƒ³å®šï¼‰
            total_bet = upset_count * 100
            total_return = (upset_hits['å˜å‹ã‚ªãƒƒã‚º'].sum() * 100) if len(upset_hits) > 0 else 0
            upset_roi = (total_return / total_bet * 100) if total_bet > 0 else 0
            
            results['upset_candidates'] = upset_count
            results['upset_hits'] = upset_hit_count
            results['upset_precision'] = upset_precision
            results['upset_recall'] = upset_recall
            results['upset_roi'] = upset_roi
        else:
            results['upset_candidates'] = 0
            results['upset_hits'] = 0
            results['upset_precision'] = 0
            results['upset_recall'] = 0
            results['upset_roi'] = 0
        
        return results
    
    def _initialize_progress(self, execution_mode: str, periods: List[int], test_years: List[int], models: List[str]):
        """é€²æ—ãƒ‡ãƒ¼ã‚¿ã‚’åˆæœŸåŒ–"""
        if not self.progress_data:
            self.progress_data = {
                'execution_mode': execution_mode,
                'test_years': test_years,
                'started_at': datetime.now().isoformat(),
                'progress': {}
            }
            
            if execution_mode == 'single_period':
                self.progress_data['training_period'] = periods[0]
                period_key = f"period_{periods[0]}"
                self.progress_data['progress'][period_key] = {}
                for year in test_years:
                    self.progress_data['progress'][period_key][str(year)] = {}
            else:  # compare_periods
                self.progress_data['training_periods'] = periods
                for period in periods:
                    period_key = f"period_{period}"
                    self.progress_data['progress'][period_key] = {}
                    for year in test_years:
                        self.progress_data['progress'][period_key][str(year)] = {}
    
    def _is_model_created(self, period_key: str, year: int, model_name: str) -> bool:
        """ãƒ¢ãƒ‡ãƒ«ãŒæ—¢ã«ä½œæˆæ¸ˆã¿ã‹ç¢ºèª"""
        year_str = str(year)
        if period_key in self.progress_data.get('progress', {}):
            if year_str in self.progress_data['progress'][period_key]:
                if model_name in self.progress_data['progress'][period_key][year_str]:
                    return self.progress_data['progress'][period_key][year_str][model_name].get('model_created', False)
        return False
    
    def _is_model_tested(self, period_key: str, year: int, model_name: str) -> bool:
        """ãƒ¢ãƒ‡ãƒ«ãŒæ—¢ã«ãƒ†ã‚¹ãƒˆæ¸ˆã¿ã‹ç¢ºèª"""
        year_str = str(year)
        if period_key in self.progress_data.get('progress', {}):
            if year_str in self.progress_data['progress'][period_key]:
                if model_name in self.progress_data['progress'][period_key][year_str]:
                    return self.progress_data['progress'][period_key][year_str][model_name].get('model_tested', False)
        return False
    
    def _mark_model_created(self, period_key: str, year: int, model_name: str, model_path: str, success: bool = True):
        """ãƒ¢ãƒ‡ãƒ«ä½œæˆå®Œäº†ã‚’ãƒãƒ¼ã‚¯"""
        year_str = str(year)
        if period_key not in self.progress_data['progress']:
            self.progress_data['progress'][period_key] = {}
        if year_str not in self.progress_data['progress'][period_key]:
            self.progress_data['progress'][period_key][year_str] = {}
        if model_name not in self.progress_data['progress'][period_key][year_str]:
            self.progress_data['progress'][period_key][year_str][model_name] = {}
        
        self.progress_data['progress'][period_key][year_str][model_name]['model_created'] = success
        self.progress_data['progress'][period_key][year_str][model_name]['model_path'] = model_path
        self._save_progress()
    
    def _mark_model_tested(self, period_key: str, year: int, model_name: str, success: bool = True):
        """ãƒ¢ãƒ‡ãƒ«ãƒ†ã‚¹ãƒˆå®Œäº†ã‚’ãƒãƒ¼ã‚¯"""
        year_str = str(year)
        if period_key not in self.progress_data['progress']:
            self.progress_data['progress'][period_key] = {}
        if year_str not in self.progress_data['progress'][period_key]:
            self.progress_data['progress'][period_key][year_str] = {}
        if model_name not in self.progress_data['progress'][period_key][year_str]:
            self.progress_data['progress'][period_key][year_str][model_name] = {}
        
        self.progress_data['progress'][period_key][year_str][model_name]['model_tested'] = success
        self._save_progress()
    
    @staticmethod
    def _create_model_worker(args: Tuple) -> Tuple[str, bool, Optional[str]]:
        """
        Phase 2: ä¸¦åˆ—ãƒ¢ãƒ‡ãƒ«ä½œæˆç”¨ãƒ¯ãƒ¼ã‚«ãƒ¼é–¢æ•°
        
        å„ãƒ—ãƒ­ã‚»ã‚¹ã§ç‹¬ç«‹ã—ã¦ãƒ¢ãƒ‡ãƒ«ä½œæˆã‚’å®Ÿè¡Œã™ã‚‹ã€‚
        DBæ¥ç¶šã¯å„ãƒ—ãƒ­ã‚»ã‚¹ã§ç‹¬ç«‹ã«ä½œæˆã•ã‚Œã‚‹ã€‚
        
        Args:
            args: (model_name, model_config, train_start, train_end, output_dir_str)
            
        Returns:
            (model_name, success, model_path)
        """
        model_name, model_config, train_start, train_end, output_dir_str = args
        output_dir = Path(output_dir_str)
        
        # å„ãƒ—ãƒ­ã‚»ã‚¹ã§ç‹¬ç«‹ã—ãŸãƒ­ã‚¬ãƒ¼ã‚’ä½œæˆ
        logger = logging.getLogger(f'Worker-{model_name}')
        logger.setLevel(logging.INFO)
        
        try:
            # ãƒ¢ãƒ‡ãƒ«è¨­å®šã‹ã‚‰å¿…è¦ãªæƒ…å ±ã‚’å–å¾—ï¼ˆå¸¸ã«å–å¾—ã™ã‚‹ï¼‰
            track_code = model_config.get('track_code')
            surface_type = model_config.get('surface_type')
            kyoso_shubetsu_code = model_config.get('kyoso_shubetsu_code')
            min_distance = model_config.get('min_distance')
            max_distance = model_config.get('max_distance')
            
            # ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆï¼ˆmodel_config ã® model_filename ã‚’ãƒ™ãƒ¼ã‚¹ã«ä½¿ç”¨ï¼‰
            base_filename = model_config.get('model_filename', '').replace('.sav', '')
            
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: model_filename ãŒç„¡ã„å ´åˆã¯è‡ªå‹•ç”Ÿæˆ
            if not base_filename:
                parts = []
                if track_code:
                    parts.append(str(track_code))
                if surface_type:
                    parts.append(surface_type)
                if kyoso_shubetsu_code:
                    parts.append(str(kyoso_shubetsu_code))
                if min_distance or max_distance:
                    dist_parts = []
                    if min_distance:
                        dist_parts.append(f"{min_distance}m")
                    if max_distance:
                        dist_parts.append(f"{max_distance}m")
                    parts.append('-'.join(dist_parts))
                base_filename = '_'.join(parts)
            
            model_filename = f'{base_filename}_{train_start}-{train_end}.sav'
            model_path = output_dir / model_filename
            
            logger.info(f"ãƒ¢ãƒ‡ãƒ«ä½œæˆé–‹å§‹: {model_name} (å­¦ç¿’æœŸé–“: {train_start}-{train_end})")
            
            # Rankerãƒ¢ãƒ‡ãƒ«ä½œæˆï¼ˆå„ãƒ—ãƒ­ã‚»ã‚¹ã§DBæ¥ç¶šãŒä½œæˆã•ã‚Œã‚‹ï¼‰
            create_universal_model(
                track_code=track_code,
                kyoso_shubetsu_code=kyoso_shubetsu_code,
                surface_type=surface_type,
                min_distance=min_distance,
                max_distance=max_distance,
                model_filename=model_filename,
                output_dir=str(output_dir),
                year_start=train_start,
                year_end=train_end
            )
            
            if not model_path.exists():
                logger.error(f"Rankerãƒ¢ãƒ‡ãƒ«ä½œæˆå¤±æ•—: {model_filename}")
                return model_name, False, None
            
            logger.info(f"Rankerãƒ¢ãƒ‡ãƒ«ä½œæˆå®Œäº†: {model_filename}")
            return model_name, True, str(model_path)
            
        except Exception as e:
            logger.error(f"ãƒ¢ãƒ‡ãƒ«ä½œæˆã‚¨ãƒ©ãƒ¼: {model_name}")
            logger.error(f"ã‚¨ãƒ©ãƒ¼è©³ç´°: {str(e)}")
            logger.error(traceback.format_exc())
            return model_name, False, None
    
    @staticmethod
    def _test_model_worker(args: Tuple) -> Tuple[str, bool, Optional[str]]:
        """
        ä¸¦åˆ—ãƒ†ã‚¹ãƒˆå®Ÿè¡Œç”¨ãƒ¯ãƒ¼ã‚«ãƒ¼é–¢æ•°
        
        å„ãƒ—ãƒ­ã‚»ã‚¹ã§ç‹¬ç«‹ã—ã¦ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã™ã‚‹ã€‚
        DBæ¥ç¶šã¯å„ãƒ—ãƒ­ã‚»ã‚¹ã§ç‹¬ç«‹ã«ä½œæˆã•ã‚Œã‚‹ã€‚
        
        Args:
            args: (model_name, model_config, model_path, test_year, output_dir_str, upset_classifier_path_str)
            
        Returns:
            (model_name, success, result_filename)
        """
        model_name, model_config, model_path, test_year, output_dir_str, upset_classifier_path_str = args
        output_dir = Path(output_dir_str)
        
        # å„ãƒ—ãƒ­ã‚»ã‚¹ã§ç‹¬ç«‹ã—ãŸãƒ­ã‚¬ãƒ¼ã‚’ä½œæˆ
        logger = logging.getLogger(f'TestWorker-{model_name}')
        logger.setLevel(logging.INFO)
        
        try:
            logger.info(f"ãƒ†ã‚¹ãƒˆå®Ÿè¡Œé–‹å§‹: {model_name} (ãƒ†ã‚¹ãƒˆå¹´: {test_year})")
            
            # ãƒ†ã‚¹ãƒˆçµæœãƒ•ã‚¡ã‚¤ãƒ«å
            train_period = Path(model_path).stem.split('_')[-1]  # ä¾‹: "2018-2022"
            result_filename = f"predicted_results_{model_name}_{train_period}_test{test_year}.tsv"
            
            # ç©´é¦¬åˆ†é¡å™¨ãƒ‘ã‚¹ã®å‡¦ç†
            upset_classifier_path = None
            if upset_classifier_path_str and os.path.exists(upset_classifier_path_str):
                upset_classifier_path = upset_classifier_path_str
                logger.info(f"[UPSET] ç©´é¦¬åˆ†é¡å™¨ã‚’ä½¿ç”¨: {Path(upset_classifier_path).name}")
            
            # universal_testã®predict_with_modelé–¢æ•°ã‚’å‘¼ã³å‡ºã—
            result_df, summary_df, race_count = universal_test.predict_with_model(
                model_filename=model_path,
                track_code=model_config.get('track_code'),
                kyoso_shubetsu_code=model_config.get('kyoso_shubetsu_code'),
                surface_type=model_config.get('surface_type'),
                min_distance=model_config.get('min_distance'),
                max_distance=model_config.get('max_distance'),
                test_year_start=test_year,
                test_year_end=test_year,
                upset_classifier_path=upset_classifier_path
            )
            
            if result_df is None or len(result_df) == 0:
                logger.warning(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãªã—: {model_name} (ãƒ†ã‚¹ãƒˆå¹´: {test_year})")
                return model_name, True, None  # ãƒ‡ãƒ¼ã‚¿ãŒãªã„ã®ã¯ã‚¨ãƒ©ãƒ¼ã§ã¯ãªã„
            
            # çµæœã‚’ä¿å­˜
            universal_test.save_results_with_append(
                df=result_df,
                filename=result_filename,
                append_mode=False,  # WFVã§ã¯å¹´ã”ã¨ã«ç‹¬ç«‹ãƒ•ã‚¡ã‚¤ãƒ«ãªã®ã§ä¸Šæ›¸ã
                output_dir=str(output_dir)
            )
            
            logger.info(f"ãƒ†ã‚¹ãƒˆå®Ÿè¡Œå®Œäº†: {result_filename} (ãƒ¬ãƒ¼ã‚¹æ•°: {race_count})")
            return model_name, True, result_filename
            
        except Exception as e:
            logger.error(f"ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {model_name}")
            logger.error(f"ã‚¨ãƒ©ãƒ¼è©³ç´°: {str(e)}")
            logger.error(traceback.format_exc())
            return model_name, False, None

    def create_model_for_year(
        self, 
        model_name: str, 
        model_config: Dict, 
        train_start: int, 
        train_end: int, 
        output_dir: Path
    ) -> Tuple[bool, Optional[str]]:
        """
        1å¹´åˆ†ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆï¼ˆPhase 2.5: Ranker + ç©´é¦¬åˆ†é¡å™¨ï¼‰
        
        Args:
            model_name: ãƒ¢ãƒ‡ãƒ«å
            model_config: ãƒ¢ãƒ‡ãƒ«è¨­å®š
            train_start: å­¦ç¿’é–‹å§‹å¹´
            train_end: å­¦ç¿’çµ‚äº†å¹´
            output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            
        Returns:
            (æˆåŠŸãƒ•ãƒ©ã‚°, ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹)
        """
        try:
            # ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
            model_filename = self._get_model_filename(model_name, train_start, train_end)
            model_path = output_dir / model_filename
            
            self.logger.info(f"ãƒ¢ãƒ‡ãƒ«ä½œæˆé–‹å§‹: {model_name} (å­¦ç¿’æœŸé–“: {train_start}-{train_end})")
            
            # Phase 1: Rankerãƒ¢ãƒ‡ãƒ«ä½œæˆ
            create_universal_model(
                track_code=model_config.get('track_code'),
                kyoso_shubetsu_code=model_config.get('kyoso_shubetsu_code'),
                surface_type=model_config.get('surface_type'),
                min_distance=model_config.get('min_distance'),
                max_distance=model_config.get('max_distance'),
                model_filename=model_filename,
                output_dir=str(output_dir),
                year_start=train_start,
                year_end=train_end
            )
            
            if not model_path.exists():
                self.logger.error(f"Rankerãƒ¢ãƒ‡ãƒ«ä½œæˆå¤±æ•—: {model_filename}")
                return False, None
            
            self.logger.info(f"Rankerãƒ¢ãƒ‡ãƒ«ä½œæˆå®Œäº†: {model_filename}")
            
            # Phase 2.5: ç©´é¦¬åˆ†é¡å™¨ä½œæˆã¯ run_single_period_mode() ã§ç‹¬ç«‹ã—ã¦å®Ÿè¡Œã•ã‚Œã‚‹ãŸã‚ã€
            # ã“ã“ã§ã¯å®Ÿè¡Œã—ãªã„ï¼ˆé‡è¤‡é˜²æ­¢ï¼‰
            
            return True, str(model_path)
            
        except Exception as e:
            self.logger.error(f"ãƒ¢ãƒ‡ãƒ«ä½œæˆã‚¨ãƒ©ãƒ¼: {model_name}")
            self.logger.error(f"ã‚¨ãƒ©ãƒ¼è©³ç´°: {str(e)}")
            self.logger.debug(traceback.format_exc())
            return False, None
    
    def _create_upset_classifier(
        self,
        model_name: str,
        train_start: int,
        train_end: int,
        output_dir: Path
    ) -> bool:
        """
        Phase 2.5: ç©´é¦¬åˆ†é¡å™¨ã‚’ä½œæˆï¼ˆWalk-Forwardå¯¾å¿œç‰ˆï¼‰
        
        Universal Rankerã§å…¨ç«¶é¦¬å ´äºˆæ¸¬ â†’ ãã®çµæœã§ç©´é¦¬å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ â†’ ç©´é¦¬åˆ†é¡å™¨å­¦ç¿’
        
        Args:
            model_name: ãƒ¢ãƒ‡ãƒ«åï¼ˆå®Ÿéš›ã«ã¯ä½¿ç”¨ã›ãšã€universalãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ï¼‰
            train_start: å­¦ç¿’é–‹å§‹å¹´
            train_end: å­¦ç¿’çµ‚äº†å¹´
            output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            
        Returns:
            æˆåŠŸãƒ•ãƒ©ã‚°
        """
        try:
            # ç©´é¦¬åˆ†é¡å™¨ã®ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆå­¦ç¿’æœŸé–“ã”ã¨ã«1ã¤ï¼‰
            upset_filename = f"upset_classifier_{train_start}-{train_end}.sav"
            upset_path = output_dir / upset_filename
            
            # æ—¢ã«å­˜åœ¨ã™ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆå­¦ç¿’æœŸé–“ãŒåŒã˜ãªã‚‰å…¨ãƒ¢ãƒ‡ãƒ«å…±é€šï¼‰
            if upset_path.exists():
                self.logger.info(f"[UPSET] ç©´é¦¬åˆ†é¡å™¨ã¯æ—¢ã«å­˜åœ¨ï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰: {upset_filename}")
                return True
            
            self.logger.info(f"[UPSET] ç©´é¦¬åˆ†é¡å™¨ä½œæˆé–‹å§‹ (æœŸé–“: {train_start}-{train_end})")
            
            # Universal Rankerãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹ã‚’å–å¾—
            universal_model_name = "all_tracks_all_surfaces_all_ages"
            universal_filename = self._get_model_filename(universal_model_name, train_start, train_end)
            universal_path = output_dir / universal_filename
            
            # upset_classifier_modelsã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‹ã‚‰è¨­å®šã‚’å–å¾—ï¼ˆå…ˆã«å–å¾—ï¼‰
            universal_config = self._get_universal_model_config()
            if universal_config is None:
                self.logger.error(f"[UPSET] Universal Rankerè¨­å®šãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return False
            
            # Universal RankerãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
            universal_already_exists = universal_path.exists()
            
            if not universal_already_exists:
                self.logger.info(f"[UPSET] Universal Rankerä½œæˆ: {universal_filename}")
                
                # Universal Rankerä½œæˆ
                create_universal_model(
                    track_code=None,  # å…¨ç«¶é¦¬å ´
                    kyoso_shubetsu_code=universal_config.get('kyoso_shubetsu_code'),
                    surface_type=None,  # èŠãƒ»ãƒ€ãƒ¼ãƒˆä¸¡æ–¹
                    min_distance=universal_config.get('min_distance'),
                    max_distance=universal_config.get('max_distance'),
                    model_filename=universal_filename,
                    output_dir=str(output_dir),
                    year_start=train_start,
                    year_end=train_end
                )
                
                if not universal_path.exists():
                    self.logger.error(f"[UPSET] Universal Rankerä½œæˆå¤±æ•—: {universal_filename}")
                    return False
                
                self.logger.info(f"[UPSET] Universal Rankerä½œæˆå®Œäº†")
            else:
                self.logger.info(f"[UPSET] Universal Rankerã¯æ—¢ã«å­˜åœ¨: {universal_filename}")
            
            # Universal RankerãŒæ—¢ã«å­˜åœ¨ã—ã¦ã„ãŸå ´åˆã€ç©´é¦¬åˆ†é¡å™¨ã‚‚æ—¢ã«ä½œæˆã•ã‚Œã¦ã„ã‚‹ã¯ãš
            # ï¼ˆæœ€åˆã®ãƒ¢ãƒ‡ãƒ«ã§ä½œæˆæ¸ˆã¿ï¼‰ãªã®ã§ã€ã“ã“ã§æ—©æœŸãƒªã‚¿ãƒ¼ãƒ³
            if universal_already_exists:
                # å¿µã®ãŸã‚ç©´é¦¬åˆ†é¡å™¨ã®å­˜åœ¨ã‚‚ç¢ºèª
                if upset_path.exists():
                    self.logger.info(f"[UPSET] ç©´é¦¬åˆ†é¡å™¨ã‚‚æ—¢ã«å­˜åœ¨ï¼ˆæ—©æœŸãƒªã‚¿ãƒ¼ãƒ³ï¼‰: {upset_filename}")
                    return True
                else:
                    self.logger.warning(f"[UPSET] Universal Rankerã¯å­˜åœ¨ã™ã‚‹ãŒç©´é¦¬åˆ†é¡å™¨ãŒãªã„ï¼ˆç¶šè¡Œã—ã¦ä½œæˆï¼‰")
                    # â˜… return False ã‚’å‰Šé™¤ï¼ç©´é¦¬åˆ†é¡å™¨ä½œæˆã‚’ç¶šè¡Œ
            
            # ã“ã“ã‹ã‚‰å…ˆã¯ã€Universal Rankerã‚’æ–°è¦ä½œæˆã—ãŸå ´åˆã®ã¿å®Ÿè¡Œ
            # å¯¾è±¡æœŸé–“ã®å¹´ãƒªã‚¹ãƒˆï¼ˆ2020é™¤ãï¼‰
            years = [y for y in range(train_start, train_end + 1) if y != 2020]
            
            self.logger.info(f"[UPSET] Universal Rankerã§å…¨ç«¶é¦¬å ´äºˆæ¸¬å®Ÿè¡Œ: {years}")
            
            # Universal Rankerãƒ¢ãƒ‡ãƒ«ã§å¯¾è±¡æœŸé–“ã®ãƒ‡ãƒ¼ã‚¿ã«äºˆæ¸¬ã‚’å®Ÿè¡Œ
            from analyze_upset_patterns import get_data_with_predictions
            
            # å…¨ç«¶é¦¬å ´ãƒ»å…¨è·é›¢ãƒ»èŠãƒ€çµ±åˆãƒ»å…¨å¹´é½¢ã§äºˆæ¸¬
            df_predicted = get_data_with_predictions(
                model_path=str(universal_path),
                years=years,
                track_codes=None,  # å…¨ç«¶é¦¬å ´å¯¾è±¡
                surface_type=None,  # èŠãƒ»ãƒ€ãƒ¼ãƒˆä¸¡æ–¹
                distance_min=1000,  # æœ€å°è·é›¢
                distance_max=9999,  # æœ€å¤§è·é›¢
                kyoso_shubetsu_code=universal_config.get('kyoso_shubetsu_code')  # å…¨å¹´é½¢
            )
            
            if df_predicted is None or len(df_predicted) == 0:
                self.logger.error(f"[UPSET] äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿ãŒ0ä»¶")
                return False
            
            self.logger.info(f"[UPSET] äºˆæ¸¬å®Œäº†: {len(df_predicted)}é ­")
            self.logger.info(f"[UPSET DEBUG] df_predicted.shape={df_predicted.shape}, indexç¯„å›²=[{df_predicted.index.min()}, {df_predicted.index.max()}]")
            
            # ç©´é¦¬å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆï¼ˆ7-12ç•ªäººæ°—ï¼‰
            from analyze_upset_patterns import create_training_dataset
            
            self.logger.info(f"[UPSET DEBUG] create_training_dataseté–‹å§‹")
            df_training, feature_cols = create_training_dataset(
                df_predicted,
                popularity_min=7,
                popularity_max=12
            )
            
            if len(df_training) == 0:
                self.logger.error(f"[UPSET] å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŒ0ä»¶")
                return False
            
            self.logger.info(f"[UPSET] å­¦ç¿’ãƒ‡ãƒ¼ã‚¿: {len(df_training)}é ­ ({train_start}-{train_end}å¹´)")
            self.logger.info(f"[UPSET] ç©´é¦¬: {df_training['is_upset'].sum()}é ­ ({df_training['is_upset'].mean()*100:.2f}%)")
            self.logger.info(f"[UPSET DEBUG] df_training.shape={df_training.shape}, indexç¯„å›²=[{df_training.index.min()}, {df_training.index.max()}]")
            
            # ç‰¹å¾´é‡æº–å‚™
            self.logger.info(f"[UPSET DEBUG] prepare_featuresé–‹å§‹")
            X, y, feature_cols = prepare_features(df_training)
            self.logger.info(f"[UPSET DEBUG] prepare_featureså®Œäº†: X.shape={X.shape}, y.shape={y.shape}, X.indexç¯„å›²=[{X.index.min()}, {X.index.max()}]")
            
            # å­¦ç¿’ï¼ˆã‚¯ãƒ©ã‚¹ã‚¦ã‚§ã‚¤ãƒˆæ–¹å¼ï¼‰
            # Phase A: ç¢ºç‡æ ¡æ­£ã¯ä¸€æ™‚çš„ã«ç„¡åŠ¹åŒ–ï¼ˆIsotonic RegressionãŒéå­¦ç¿’ã™ã‚‹å•é¡Œï¼‰
            # TODO: Platt Scalingï¼ˆã‚·ã‚°ãƒ¢ã‚¤ãƒ‰æ ¡æ­£ï¼‰ã¾ãŸã¯æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿åˆ†é›¢ã§å†å®Ÿè£…
            self.logger.info(f"[UPSET DEBUG] train_with_class_weightsé–‹å§‹: X.shape={X.shape}, y.shape={y.shape}")
            models, cv_results = train_with_class_weights(
                X, y, feature_cols,
                n_splits=5,
                random_state=42,
                use_calibration=False  # Phase A: ä¸€æ™‚çš„ã«ç„¡åŠ¹åŒ–
            )
            self.logger.info(f"[UPSET DEBUG] train_with_class_weightså®Œäº†: {len(models)}ãƒ¢ãƒ‡ãƒ«ä½œæˆ")
            
            # ãƒ¢ãƒ‡ãƒ«ä¿å­˜ï¼ˆæœŸé–“ã”ã¨ã®ãƒ•ã‚¡ã‚¤ãƒ«åï¼‰
            upset_filename = f"upset_classifier_{train_start}-{train_end}.sav"
            upset_path = output_dir / upset_filename
            
            import pickle
            
            # æ–°ã—ã„æ§‹é€ ï¼ˆdictãƒªã‚¹ãƒˆï¼‰ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã¨æ ¡æ­£å™¨ã‚’åˆ†é›¢
            lgb_models = [m['model'] for m in models]
            calibrators = [m['calibrator'] for m in models]
            calibration_method = models[0].get('calibration_method', None)
            has_calibration = calibrators[0] is not None
            
            model_data = {
                'models': lgb_models,
                'calibrators': calibrators,
                'feature_cols': feature_cols,
                'n_models': len(models),
                'train_period': f"{train_start}-{train_end}",
                'has_calibration': has_calibration,
                'calibration_method': calibration_method
            }
            
            with open(upset_path, 'wb') as f:
                pickle.dump(model_data, f)
            
            # NOTE: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã®modelsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¸ã®ã‚³ãƒ”ãƒ¼ã¯æ‰‹å‹•ã§è¡Œã†
            # modelsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚‚ã‚³ãƒ”ãƒ¼ï¼ˆuniversal_test.pyãŒæ¤œå‡ºã§ãã‚‹ã‚ˆã†ã«ï¼‰
            # models_dir = Path('models')
            # models_dir.mkdir(exist_ok=True)
            # models_upset_path = models_dir / upset_filename
            # 
            # with open(models_upset_path, 'wb') as f:
            #     pickle.dump(model_data, f)
            # 
            # self.logger.info(f"[UPSET] ã‚³ãƒ”ãƒ¼å…ˆ: {models_upset_path}")
            
            self.logger.info(f"[UPSET] ç©´é¦¬åˆ†é¡å™¨ä¿å­˜: {upset_path}")
            if has_calibration:
                self.logger.info(f"[UPSET] ğŸ¯ Phase A: ç¢ºç‡æ ¡æ­£ï¼ˆIsotonic Regressionï¼‰æœ‰åŠ¹")
            self.logger.info(f"[UPSET] å¿…è¦ã«å¿œã˜ã¦ models/ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«æ‰‹å‹•ã‚³ãƒ”ãƒ¼ã—ã¦ãã ã•ã„")
            return True
            
        except Exception as e:
            self.logger.error(f"[UPSET] ç©´é¦¬åˆ†é¡å™¨ä½œæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
            self.logger.error(f"[UPSET] ã‚¹ã‚¿ãƒƒã‚¯ãƒˆãƒ¬ãƒ¼ã‚¹:\n{traceback.format_exc()}")
            import sys
            self.logger.error(f"[UPSET] ã‚¨ãƒ©ãƒ¼è©³ç´°: type={type(e).__name__}, args={e.args}")
            return False
    
    def test_model_for_year(
        self,
        model_name: str,
        model_config: Dict,
        model_path: str,
        test_year: int,
        output_dir: Path
    ) -> bool:
        """
        1å¹´åˆ†ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ†ã‚¹ãƒˆ
        
        Args:
            model_name: ãƒ¢ãƒ‡ãƒ«å
            model_config: ãƒ¢ãƒ‡ãƒ«è¨­å®š
            model_path: ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            test_year: ãƒ†ã‚¹ãƒˆå¹´
            output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            
        Returns:
            æˆåŠŸãƒ•ãƒ©ã‚°
        """
        try:
            self.logger.info(f"ãƒ†ã‚¹ãƒˆå®Ÿè¡Œé–‹å§‹: {model_name} (ãƒ†ã‚¹ãƒˆå¹´: {test_year})")
            
            # ãƒ†ã‚¹ãƒˆçµæœãƒ•ã‚¡ã‚¤ãƒ«å
            train_period = Path(model_path).stem.split('_')[-1]  # ä¾‹: "2018-2022"
            result_filename = f"predicted_results_{model_name}_{train_period}_test{test_year}.tsv"
            
            # ç©´é¦¬åˆ†é¡å™¨ã®ãƒ‘ã‚¹ã‚’è¨ˆç®—
            # ãƒ¢ãƒ‡ãƒ«ã¨åŒã˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ç©´é¦¬åˆ†é¡å™¨ãŒã‚ã‚‹ã¯ãš
            model_dir = Path(model_path).parent
            upset_classifier_path = model_dir / f"upset_classifier_{train_period}.sav"
            
            # å­˜åœ¨ã—ãªã„å ´åˆã¯Noneï¼ˆè‡ªå‹•æ¤œç´¢ã«ä»»ã›ã‚‹ï¼‰
            if not upset_classifier_path.exists():
                self.logger.warning(f"[UPSET] ç©´é¦¬åˆ†é¡å™¨ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {upset_classifier_path}")
                upset_classifier_path = None
            else:
                self.logger.info(f"[UPSET] ç©´é¦¬åˆ†é¡å™¨ã‚’ä½¿ç”¨: {upset_classifier_path.name}")
            
            # universal_testã®predict_with_modelé–¢æ•°ã‚’å‘¼ã³å‡ºã—
            result_df, summary_df, race_count = universal_test.predict_with_model(
                model_filename=model_path,
                track_code=model_config.get('track_code'),
                kyoso_shubetsu_code=model_config.get('kyoso_shubetsu_code'),
                surface_type=model_config.get('surface_type'),
                min_distance=model_config.get('min_distance'),
                max_distance=model_config.get('max_distance'),
                test_year_start=test_year,
                test_year_end=test_year,
                upset_classifier_path=str(upset_classifier_path) if upset_classifier_path else None
            )
            
            if result_df is None or len(result_df) == 0:
                self.logger.warning(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãªã—: {model_name} (ãƒ†ã‚¹ãƒˆå¹´: {test_year})")
                return True  # ãƒ‡ãƒ¼ã‚¿ãŒãªã„ã®ã¯ã‚¨ãƒ©ãƒ¼ã§ã¯ãªã„
            
            # çµæœã‚’ä¿å­˜
            universal_test.save_results_with_append(
                df=result_df,
                filename=result_filename,
                append_mode=False,  # WFVã§ã¯å¹´ã”ã¨ã«ç‹¬ç«‹ãƒ•ã‚¡ã‚¤ãƒ«ãªã®ã§ä¸Šæ›¸ã
                output_dir=str(output_dir)
            )
            
            self.logger.info(f"ãƒ†ã‚¹ãƒˆå®Ÿè¡Œå®Œäº†: {result_filename} (ãƒ¬ãƒ¼ã‚¹æ•°: {race_count})")
            return True
            
        except Exception as e:
            self.logger.error(f"ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {model_name}")
            self.logger.error(f"ã‚¨ãƒ©ãƒ¼è©³ç´°: {str(e)}")
            self.logger.debug(traceback.format_exc())
            return False
    
    def run_single_period_mode(self, resume: bool = False, dry_run: bool = False) -> bool:
        """
        å˜ä¸€æœŸé–“ãƒ¢ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œ
        
        Args:
            resume: å‰å›ã‹ã‚‰å†é–‹ã™ã‚‹ã‹
            dry_run: ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ï¼ˆå®Ÿè¡Œè¨ˆç”»ã®ã¿è¡¨ç¤ºï¼‰
            
        Returns:
            æˆåŠŸãƒ•ãƒ©ã‚°
        """
        settings = self.wfv_config['single_period_settings']
        training_period = settings['training_period']
        test_years = self.wfv_config['test_years']
        models_setting = settings['models']
        
        # ãƒ¢ãƒ‡ãƒ«ãƒªã‚¹ãƒˆã‚’å–å¾—
        target_models = self._filter_models(models_setting)
        
        self.logger.info("=" * 80)
        self.logger.info("Walk-Forward Validation - å˜ä¸€æœŸé–“ãƒ¢ãƒ¼ãƒ‰")
        self.logger.info(f"å­¦ç¿’æœŸé–“: {training_period}å¹´")
        self.logger.info(f"ãƒ†ã‚¹ãƒˆå¹´: {test_years}")
        self.logger.info(f"å¯¾è±¡ãƒ¢ãƒ‡ãƒ«æ•°: {len(target_models)}")
        self.logger.info("=" * 80)
        
        if dry_run:
            self.logger.info("[DRY RUN] å®Ÿè¡Œè¨ˆç”»:")
            for year in test_years:
                train_start = year - training_period
                train_end = year - 1
                self.logger.info(f"  ãƒ†ã‚¹ãƒˆå¹´ {year}: å­¦ç¿’æœŸé–“ {train_start}-{train_end}")
                for model_name in target_models:
                    self.logger.info(f"    - {model_name}")
            return True
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        period_key = f"period_{training_period}"
        period_dir = self.output_dir / period_key
        models_dir = period_dir / "models"
        test_results_dir = period_dir / "test_results"
        
        # é€²æ—ãƒ•ã‚¡ã‚¤ãƒ«ã®è¨­å®š
        self.progress_file = self.output_dir / "progress.json"
        
        if resume and self.progress_file.exists():
            self.progress_data = self._load_progress(self.progress_file)
            self.logger.info("å‰å›ã®é€²æ—ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
        else:
            self._initialize_progress('single_period', [training_period], test_years, target_models)
        
        # å„ãƒ†ã‚¹ãƒˆå¹´ã§ãƒ«ãƒ¼ãƒ—
        for test_year in test_years:
            train_start = test_year - training_period
            train_end = test_year - 1
            
            self.logger.info("-" * 80)
            self.logger.info(f"ãƒ†ã‚¹ãƒˆå¹´: {test_year} (å­¦ç¿’æœŸé–“: {train_start}-{train_end})")
            self.logger.info("-" * 80)
            
            # å¹´ã”ã¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
            year_models_dir = models_dir / str(test_year)
            year_test_dir = test_results_dir / str(test_year)
            year_models_dir.mkdir(parents=True, exist_ok=True)
            year_test_dir.mkdir(parents=True, exist_ok=True)
            
            # â˜… UPSETåˆ†é¡å™¨ãƒã‚§ãƒƒã‚¯ï¼ˆå…¨ãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šå‰ã«ç‹¬ç«‹ã—ã¦å®Ÿè¡Œï¼‰
            upset_filename = f"upset_classifier_{train_start}-{train_end}.sav"
            upset_model_name = f"upset_classifier_{train_start}-{train_end}"
            
            # progress.jsonã§ã‚¹ã‚­ãƒƒãƒ—ãƒã‚§ãƒƒã‚¯
            if self._is_model_created(period_key, test_year, upset_model_name):
                self.logger.info(f"[UPSET] ç©´é¦¬åˆ†é¡å™¨: ã‚¹ã‚­ãƒƒãƒ—ï¼ˆä½œæˆæ¸ˆã¿ï¼‰")
            else:
                # UPSETåˆ†é¡å™¨ä½œæˆï¼ˆæœ€åˆã®ãƒ¢ãƒ‡ãƒ«åã‚’ä½¿ç”¨ï¼‰
                first_model_name = target_models[0] if target_models else "default"
                upset_success = self._create_upset_classifier(
                    first_model_name, train_start, train_end, year_models_dir
                )
                
                upset_path = year_models_dir / upset_filename
                if upset_success and upset_path.exists():
                    self._mark_model_created(period_key, test_year, upset_model_name, str(upset_path), True)
                    self.logger.info(f"[UPSET] ç©´é¦¬åˆ†é¡å™¨ä½œæˆå®Œäº†: {upset_filename}")
                    
                    # Universal Rankerã‚‚è¨˜éŒ²
                    universal_filename = self._get_model_filename("all_tracks_all_surfaces_all_ages", train_start, train_end)
                    universal_path = year_models_dir / universal_filename
                    if universal_path.exists():
                        universal_model_name = f"universal_ranker_{train_start}-{train_end}"
                        self._mark_model_created(period_key, test_year, universal_model_name, str(universal_path), True)
                        self.logger.info(f"[UPSET] Universal Rankerè¨˜éŒ²å®Œäº†: {universal_filename}")
                else:
                    self.logger.warning(f"[UPSET] ç©´é¦¬åˆ†é¡å™¨ä½œæˆå¤±æ•—ã¾ãŸã¯æ—¢ã«å­˜åœ¨")
            
            # ãƒ¢ãƒ‡ãƒ«ä½œæˆãƒ•ã‚§ãƒ¼ã‚º (Phase 2: ä¸¦åˆ—å®Ÿè¡Œ)
            self.logger.info(f"[ãƒ¢ãƒ‡ãƒ«ä½œæˆãƒ•ã‚§ãƒ¼ã‚º] {len(target_models)}ãƒ¢ãƒ‡ãƒ« (ä¸¦åˆ—å®Ÿè¡Œ)")
            
            # æœªä½œæˆãƒ¢ãƒ‡ãƒ«ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            models_to_create = []
            for model_name in target_models:
                if self._is_model_created(period_key, test_year, model_name):
                    self.logger.info(f"  {model_name}: ã‚¹ã‚­ãƒƒãƒ—ï¼ˆä½œæˆæ¸ˆã¿ï¼‰")
                else:
                    model_config = self._get_model_config(model_name)
                    if model_config:
                        models_to_create.append((model_name, model_config))
                    else:
                        self._mark_model_created(period_key, test_year, model_name, "", False)
            
            if models_to_create:
                # ProcessPoolExecutorã§ä¸¦åˆ—å®Ÿè¡Œ
                max_workers = min(4, multiprocessing.cpu_count())
                self.logger.info(f"  ä¸¦åˆ—å®Ÿè¡Œãƒ¯ãƒ¼ã‚«ãƒ¼æ•°: {max_workers}")
                
                # å¼•æ•°ãƒªã‚¹ãƒˆã‚’ä½œæˆ
                worker_args = [
                    (name, config, train_start, train_end, str(year_models_dir))
                    for name, config in models_to_create
                ]
                
                with ProcessPoolExecutor(max_workers=max_workers) as executor:
                    # å…¨ã‚¿ã‚¹ã‚¯ã‚’æŠ•å…¥
                    future_to_model = {
                        executor.submit(self._create_model_worker, args): args[0]
                        for args in worker_args
                    }
                    
                    # å®Œäº†ã—ãŸã‚¿ã‚¹ã‚¯ã‹ã‚‰çµæœã‚’å–å¾—
                    completed_count = 0
                    for future in as_completed(future_to_model):
                        model_name = future_to_model[future]
                        try:
                            result_name, success, model_path = future.result()
                            completed_count += 1
                            
                            self.logger.info(
                                f"  [{completed_count}/{len(models_to_create)}] {result_name}: "
                                f"{'å®Œäº†' if success else 'å¤±æ•—'}"
                            )
                            
                            # progress.jsonã«è¨˜éŒ²ï¼ˆãƒ­ãƒƒã‚¯ä»˜ãï¼‰
                            self._mark_model_created(period_key, test_year, result_name, model_path or "", success)
                            
                            if not success:
                                error_action = self.wfv_config['execution'].get('on_model_creation_error', 'skip')
                                if error_action == 'stop':
                                    self.logger.error("ãƒ¢ãƒ‡ãƒ«ä½œæˆã‚¨ãƒ©ãƒ¼ã«ã‚ˆã‚Šå‡¦ç†ã‚’ä¸­æ–­ã—ã¾ã™")
                                    executor.shutdown(wait=False, cancel_futures=True)
                                    return False
                        
                        except Exception as e:
                            self.logger.error(f"  {model_name}: ä¸¦åˆ—å®Ÿè¡Œã‚¨ãƒ©ãƒ¼ - {str(e)}")
                            self._mark_model_created(period_key, test_year, model_name, "", False)
            else:
                self.logger.info("  ä½œæˆå¯¾è±¡ãƒ¢ãƒ‡ãƒ«ãªã—ï¼ˆå…¨ã¦ä½œæˆæ¸ˆã¿ï¼‰")
            
            # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œãƒ•ã‚§ãƒ¼ã‚º (ä¸¦åˆ—å®Ÿè¡Œ)
            self.logger.info(f"[ãƒ†ã‚¹ãƒˆå®Ÿè¡Œãƒ•ã‚§ãƒ¼ã‚º] {len(target_models)}ãƒ¢ãƒ‡ãƒ« (ä¸¦åˆ—å®Ÿè¡Œ)")
            
            # ç©´é¦¬åˆ†é¡å™¨ã®ãƒ‘ã‚¹ã‚’å–å¾—
            upset_classifier_path = year_models_dir / f"upset_classifier_{train_start}-{train_end}.sav"
            upset_classifier_path_str = str(upset_classifier_path) if upset_classifier_path.exists() else None
            
            # æœªãƒ†ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            models_to_test = []
            for model_name in target_models:
                # ã‚¹ã‚­ãƒƒãƒ—åˆ¤å®š
                if self._is_model_tested(period_key, test_year, model_name):
                    self.logger.info(f"  {model_name}: ã‚¹ã‚­ãƒƒãƒ—ï¼ˆãƒ†ã‚¹ãƒˆæ¸ˆã¿ï¼‰")
                    continue
                
                # ãƒ¢ãƒ‡ãƒ«ãŒä½œæˆã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
                year_str = str(test_year)
                model_path = None
                if period_key in self.progress_data.get('progress', {}):
                    if year_str in self.progress_data['progress'][period_key]:
                        if model_name in self.progress_data['progress'][period_key][year_str]:
                            model_info = self.progress_data['progress'][period_key][year_str][model_name]
                            if not model_info.get('model_created', False):
                                self.logger.warning(f"  {model_name}: ã‚¹ã‚­ãƒƒãƒ—ï¼ˆãƒ¢ãƒ‡ãƒ«æœªä½œæˆï¼‰")
                                continue
                            
                            model_path = model_info.get('model_path')
                            if not model_path or not os.path.exists(model_path):
                                self.logger.warning(f"  {model_name}: ã‚¹ã‚­ãƒƒãƒ—ï¼ˆãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ä¸æ˜ï¼‰")
                                continue
                
                model_config = self._get_model_config(model_name)
                if not model_config:
                    self._mark_model_tested(period_key, test_year, model_name, False)
                    continue
                
                models_to_test.append((model_name, model_config, model_path))
            
            if models_to_test:
                # ProcessPoolExecutorã§ä¸¦åˆ—å®Ÿè¡Œ
                max_workers = min(4, multiprocessing.cpu_count())
                self.logger.info(f"  ä¸¦åˆ—å®Ÿè¡Œãƒ¯ãƒ¼ã‚«ãƒ¼æ•°: {max_workers}")
                
                # å¼•æ•°ãƒªã‚¹ãƒˆã‚’ä½œæˆ
                test_worker_args = [
                    (name, config, path, test_year, str(year_test_dir), upset_classifier_path_str)
                    for name, config, path in models_to_test
                ]
                
                with ProcessPoolExecutor(max_workers=max_workers) as executor:
                    # å…¨ã‚¿ã‚¹ã‚¯ã‚’æŠ•å…¥
                    future_to_model = {
                        executor.submit(self._test_model_worker, args): args[0]
                        for args in test_worker_args
                    }
                    
                    # å®Œäº†ã—ãŸã‚¿ã‚¹ã‚¯ã‹ã‚‰çµæœã‚’å–å¾—
                    completed_count = 0
                    for future in as_completed(future_to_model):
                        model_name = future_to_model[future]
                        try:
                            result_name, success, result_filename = future.result()
                            completed_count += 1
                            
                            self.logger.info(
                                f"  [{completed_count}/{len(models_to_test)}] {result_name}: "
                                f"{'å®Œäº†' if success else 'å¤±æ•—'}"
                            )
                            
                            # progress.jsonã«è¨˜éŒ²ï¼ˆãƒ­ãƒƒã‚¯ä»˜ãï¼‰
                            self._mark_model_tested(period_key, test_year, result_name, success)
                            
                            if not success:
                                error_action = self.wfv_config['execution'].get('on_test_error', 'skip')
                                if error_action == 'stop':
                                    self.logger.error("ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼ã«ã‚ˆã‚Šå‡¦ç†ã‚’ä¸­æ–­ã—ã¾ã™")
                                    executor.shutdown(wait=False, cancel_futures=True)
                                    return False
                        
                        except Exception as e:
                            self.logger.error(f"  {model_name}: ä¸¦åˆ—å®Ÿè¡Œã‚¨ãƒ©ãƒ¼ - {str(e)}")
                            self._mark_model_tested(period_key, test_year, model_name, False)
            else:
                self.logger.info("  ãƒ†ã‚¹ãƒˆå¯¾è±¡ãƒ¢ãƒ‡ãƒ«ãªã—ï¼ˆå…¨ã¦ãƒ†ã‚¹ãƒˆæ¸ˆã¿ã¾ãŸã¯ãƒ¢ãƒ‡ãƒ«æœªä½œæˆï¼‰")
        
        self.logger.info("=" * 80)
        self.logger.info("å˜ä¸€æœŸé–“ãƒ¢ãƒ¼ãƒ‰å®Œäº†")
        self.logger.info("=" * 80)
        
        return True
    
    def run_compare_periods_mode(self, resume: bool = False, dry_run: bool = False) -> bool:
        """
        æœŸé–“æ¯”è¼ƒãƒ¢ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œ
        
        Args:
            resume: å‰å›ã‹ã‚‰å†é–‹ã™ã‚‹ã‹
            dry_run: ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ï¼ˆå®Ÿè¡Œè¨ˆç”»ã®ã¿è¡¨ç¤ºï¼‰
            
        Returns:
            æˆåŠŸãƒ•ãƒ©ã‚°
        """
        settings = self.wfv_config['compare_periods_settings']
        training_periods = settings['training_periods']
        test_years = self.wfv_config['test_years']
        models_setting = settings['models']
        
        # ãƒ¢ãƒ‡ãƒ«ãƒªã‚¹ãƒˆã‚’å–å¾—
        target_models = self._filter_models(models_setting)
        
        self.logger.info("=" * 80)
        self.logger.info("Walk-Forward Validation - æœŸé–“æ¯”è¼ƒãƒ¢ãƒ¼ãƒ‰")
        self.logger.info(f"æ¯”è¼ƒæœŸé–“: {training_periods}å¹´")
        self.logger.info(f"ãƒ†ã‚¹ãƒˆå¹´: {test_years}")
        self.logger.info(f"å¯¾è±¡ãƒ¢ãƒ‡ãƒ«æ•°: {len(target_models)}")
        self.logger.info("=" * 80)
        
        if dry_run:
            self.logger.info("[DRY RUN] å®Ÿè¡Œè¨ˆç”»:")
            for period in training_periods:
                self.logger.info(f"  æœŸé–“: {period}å¹´")
                for year in test_years:
                    train_start = year - period
                    train_end = year - 1
                    self.logger.info(f"    ãƒ†ã‚¹ãƒˆå¹´ {year}: å­¦ç¿’æœŸé–“ {train_start}-{train_end}")
                    for model_name in target_models:
                        self.logger.info(f"      - {model_name}")
            return True
        
        # é€²æ—ãƒ•ã‚¡ã‚¤ãƒ«ã®è¨­å®š
        self.progress_file = self.output_dir / "progress.json"
        
        if resume and self.progress_file.exists():
            self.progress_data = self._load_progress(self.progress_file)
            self.logger.info("å‰å›ã®é€²æ—ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
        else:
            self._initialize_progress('compare_periods', training_periods, test_years, target_models)
        
        # å„æœŸé–“ã§ãƒ«ãƒ¼ãƒ—
        for training_period in training_periods:
            period_key = f"period_{training_period}"
            
            self.logger.info("=" * 80)
            self.logger.info(f"å­¦ç¿’æœŸé–“: {training_period}å¹´")
            self.logger.info("=" * 80)
            
            # æœŸé–“ã”ã¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
            period_dir = self.output_dir / period_key
            models_dir = period_dir / "models"
            test_results_dir = period_dir / "test_results"
            
            # å„ãƒ†ã‚¹ãƒˆå¹´ã§ãƒ«ãƒ¼ãƒ—
            for test_year in test_years:
                train_start = test_year - training_period
                train_end = test_year - 1
                
                self.logger.info("-" * 80)
                self.logger.info(f"ãƒ†ã‚¹ãƒˆå¹´: {test_year} (å­¦ç¿’æœŸé–“: {train_start}-{train_end})")
                self.logger.info("-" * 80)
                
                # å¹´ã”ã¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
                year_models_dir = models_dir / str(test_year)
                year_test_dir = test_results_dir / str(test_year)
                year_models_dir.mkdir(parents=True, exist_ok=True)
                year_test_dir.mkdir(parents=True, exist_ok=True)
                
                # â˜… UPSETåˆ†é¡å™¨ãƒã‚§ãƒƒã‚¯ï¼ˆå…¨ãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šå‰ã«ç‹¬ç«‹ã—ã¦å®Ÿè¡Œï¼‰
                upset_filename = f"upset_classifier_{train_start}-{train_end}.sav"
                upset_model_name = f"upset_classifier_{train_start}-{train_end}"
                
                # progress.jsonã§ã‚¹ã‚­ãƒƒãƒ—ãƒã‚§ãƒƒã‚¯
                if self._is_model_created(period_key, test_year, upset_model_name):
                    self.logger.info(f"[UPSET] ç©´é¦¬åˆ†é¡å™¨: ã‚¹ã‚­ãƒƒãƒ—ï¼ˆä½œæˆæ¸ˆã¿ï¼‰")
                else:
                    # UPSETåˆ†é¡å™¨ä½œæˆï¼ˆæœ€åˆã®ãƒ¢ãƒ‡ãƒ«åã‚’ä½¿ç”¨ï¼‰
                    first_model_name = target_models[0] if target_models else "default"
                    upset_success = self._create_upset_classifier(
                        first_model_name, train_start, train_end, year_models_dir
                    )
                    
                    upset_path = year_models_dir / upset_filename
                    if upset_success and upset_path.exists():
                        self._mark_model_created(period_key, test_year, upset_model_name, str(upset_path), True)
                        self.logger.info(f"[UPSET] ç©´é¦¬åˆ†é¡å™¨ä½œæˆå®Œäº†: {upset_filename}")
                        
                        # Universal Rankerã‚‚è¨˜éŒ²
                        universal_filename = self._get_model_filename("all_tracks_all_surfaces_all_ages", train_start, train_end)
                        universal_path = year_models_dir / universal_filename
                        if universal_path.exists():
                            universal_model_name = f"universal_ranker_{train_start}-{train_end}"
                            self._mark_model_created(period_key, test_year, universal_model_name, str(universal_path), True)
                            self.logger.info(f"[UPSET] Universal Rankerè¨˜éŒ²å®Œäº†: {universal_filename}")
                    else:
                        self.logger.warning(f"[UPSET] ç©´é¦¬åˆ†é¡å™¨ä½œæˆå¤±æ•—ã¾ãŸã¯æ—¢ã«å­˜åœ¨")
                
                # ãƒ¢ãƒ‡ãƒ«ä½œæˆãƒ•ã‚§ãƒ¼ã‚º (Phase 2: ä¸¦åˆ—å®Ÿè¡Œ)
                self.logger.info(f"[ãƒ¢ãƒ‡ãƒ«ä½œæˆãƒ•ã‚§ãƒ¼ã‚º] {len(target_models)}ãƒ¢ãƒ‡ãƒ« (ä¸¦åˆ—å®Ÿè¡Œ)")
                
                # æœªä½œæˆãƒ¢ãƒ‡ãƒ«ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                models_to_create = []
                for model_name in target_models:
                    if self._is_model_created(period_key, test_year, model_name):
                        self.logger.info(f"  {model_name}: ã‚¹ã‚­ãƒƒãƒ—ï¼ˆä½œæˆæ¸ˆã¿ï¼‰")
                    else:
                        model_config = self._get_model_config(model_name)
                        if model_config:
                            models_to_create.append((model_name, model_config))
                        else:
                            self._mark_model_created(period_key, test_year, model_name, "", False)
                
                if models_to_create:
                    # ProcessPoolExecutorã§ä¸¦åˆ—å®Ÿè¡Œ
                    max_workers = min(4, multiprocessing.cpu_count())
                    self.logger.info(f"  ä¸¦åˆ—å®Ÿè¡Œãƒ¯ãƒ¼ã‚«ãƒ¼æ•°: {max_workers}")
                    
                    # å¼•æ•°ãƒªã‚¹ãƒˆã‚’ä½œæˆ
                    worker_args = [
                        (name, config, train_start, train_end, str(year_models_dir))
                        for name, config in models_to_create
                    ]
                    
                    with ProcessPoolExecutor(max_workers=max_workers) as executor:
                        # å…¨ã‚¿ã‚¹ã‚¯ã‚’æŠ•å…¥
                        future_to_model = {
                            executor.submit(self._create_model_worker, args): args[0]
                            for args in worker_args
                        }
                        
                        # å®Œäº†ã—ãŸã‚¿ã‚¹ã‚¯ã‹ã‚‰çµæœã‚’å–å¾—
                        completed_count = 0
                        for future in as_completed(future_to_model):
                            model_name = future_to_model[future]
                            try:
                                result_name, success, model_path = future.result()
                                completed_count += 1
                                
                                self.logger.info(
                                    f"  [{completed_count}/{len(models_to_create)}] {result_name}: "
                                    f"{'å®Œäº†' if success else 'å¤±æ•—'}"
                                )
                                
                                # progress.jsonã«è¨˜éŒ²ï¼ˆãƒ­ãƒƒã‚¯ä»˜ãï¼‰
                                self._mark_model_created(period_key, test_year, result_name, model_path or "", success)
                                
                                if not success:
                                    error_action = self.wfv_config['execution'].get('on_model_creation_error', 'skip')
                                    if error_action == 'stop':
                                        self.logger.error("ãƒ¢ãƒ‡ãƒ«ä½œæˆã‚¨ãƒ©ãƒ¼ã«ã‚ˆã‚Šå‡¦ç†ã‚’ä¸­æ–­ã—ã¾ã™")
                                        executor.shutdown(wait=False, cancel_futures=True)
                                        return False
                            
                            except Exception as e:
                                self.logger.error(f"  {model_name}: ä¸¦åˆ—å®Ÿè¡Œã‚¨ãƒ©ãƒ¼ - {str(e)}")
                                self._mark_model_created(period_key, test_year, model_name, "", False)
                else:
                    self.logger.info("  ä½œæˆå¯¾è±¡ãƒ¢ãƒ‡ãƒ«ãªã—ï¼ˆå…¨ã¦ä½œæˆæ¸ˆã¿ï¼‰")
                
                # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œãƒ•ã‚§ãƒ¼ã‚º (ä¸¦åˆ—å®Ÿè¡Œ)
                self.logger.info(f"[ãƒ†ã‚¹ãƒˆå®Ÿè¡Œãƒ•ã‚§ãƒ¼ã‚º] {len(target_models)}ãƒ¢ãƒ‡ãƒ« (ä¸¦åˆ—å®Ÿè¡Œ)")
                
                # ç©´é¦¬åˆ†é¡å™¨ã®ãƒ‘ã‚¹ã‚’å–å¾—
                upset_classifier_path = year_models_dir / f"upset_classifier_{train_start}-{train_end}.sav"
                upset_classifier_path_str = str(upset_classifier_path) if upset_classifier_path.exists() else None
                
                # æœªãƒ†ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                models_to_test = []
                for model_name in target_models:
                    # ã‚¹ã‚­ãƒƒãƒ—åˆ¤å®š
                    if self._is_model_tested(period_key, test_year, model_name):
                        self.logger.info(f"  {model_name}: ã‚¹ã‚­ãƒƒãƒ—ï¼ˆãƒ†ã‚¹ãƒˆæ¸ˆã¿ï¼‰")
                        continue
                    
                    # ãƒ¢ãƒ‡ãƒ«ãŒä½œæˆã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
                    year_str = str(test_year)
                    model_path = None
                    if period_key in self.progress_data.get('progress', {}):
                        if year_str in self.progress_data['progress'][period_key]:
                            if model_name in self.progress_data['progress'][period_key][year_str]:
                                model_info = self.progress_data['progress'][period_key][year_str][model_name]
                                if not model_info.get('model_created', False):
                                    self.logger.warning(f"  {model_name}: ã‚¹ã‚­ãƒƒãƒ—ï¼ˆãƒ¢ãƒ‡ãƒ«æœªä½œæˆï¼‰")
                                    continue
                                
                                model_path = model_info.get('model_path')
                                if not model_path or not os.path.exists(model_path):
                                    self.logger.warning(f"  {model_name}: ã‚¹ã‚­ãƒƒãƒ—ï¼ˆãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ä¸æ˜ï¼‰")
                                    continue
                    
                    model_config = self._get_model_config(model_name)
                    if not model_config:
                        self._mark_model_tested(period_key, test_year, model_name, False)
                        continue
                    
                    models_to_test.append((model_name, model_config, model_path))
                
                if models_to_test:
                    # ProcessPoolExecutorã§ä¸¦åˆ—å®Ÿè¡Œ
                    max_workers = min(4, multiprocessing.cpu_count())
                    self.logger.info(f"  ä¸¦åˆ—å®Ÿè¡Œãƒ¯ãƒ¼ã‚«ãƒ¼æ•°: {max_workers}")
                    
                    # å¼•æ•°ãƒªã‚¹ãƒˆã‚’ä½œæˆ
                    test_worker_args = [
                        (name, config, path, test_year, str(year_test_dir), upset_classifier_path_str)
                        for name, config, path in models_to_test
                    ]
                    
                    with ProcessPoolExecutor(max_workers=max_workers) as executor:
                        # å…¨ã‚¿ã‚¹ã‚¯ã‚’æŠ•å…¥
                        future_to_model = {
                            executor.submit(self._test_model_worker, args): args[0]
                            for args in test_worker_args
                        }
                        
                        # å®Œäº†ã—ãŸã‚¿ã‚¹ã‚¯ã‹ã‚‰çµæœã‚’å–å¾—
                        completed_count = 0
                        for future in as_completed(future_to_model):
                            model_name = future_to_model[future]
                            try:
                                result_name, success, result_filename = future.result()
                                completed_count += 1
                                
                                self.logger.info(
                                    f"  [{completed_count}/{len(models_to_test)}] {result_name}: "
                                    f"{'å®Œäº†' if success else 'å¤±æ•—'}"
                                )
                                
                                # progress.jsonã«è¨˜éŒ²ï¼ˆãƒ­ãƒƒã‚¯ä»˜ãï¼‰
                                self._mark_model_tested(period_key, test_year, result_name, success)
                                
                                if not success:
                                    error_action = self.wfv_config['execution'].get('on_test_error', 'skip')
                                    if error_action == 'stop':
                                        self.logger.error("ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼ã«ã‚ˆã‚Šå‡¦ç†ã‚’ä¸­æ–­ã—ã¾ã™")
                                        executor.shutdown(wait=False, cancel_futures=True)
                                        return False
                            
                            except Exception as e:
                                self.logger.error(f"  {model_name}: ä¸¦åˆ—å®Ÿè¡Œã‚¨ãƒ©ãƒ¼ - {str(e)}")
                                self._mark_model_tested(period_key, test_year, model_name, False)
                else:
                    self.logger.info("  ãƒ†ã‚¹ãƒˆå¯¾è±¡ãƒ¢ãƒ‡ãƒ«ãªã—ï¼ˆå…¨ã¦ãƒ†ã‚¹ãƒˆæ¸ˆã¿ã¾ãŸã¯ãƒ¢ãƒ‡ãƒ«æœªä½œæˆï¼‰")
        
        self.logger.info("=" * 80)
        self.logger.info("æœŸé–“æ¯”è¼ƒãƒ¢ãƒ¼ãƒ‰å®Œäº†")
        self.logger.info("=" * 80)
        
        return True
    
    def run(self, resume: bool = False, dry_run: bool = False) -> bool:
        """
        WFVã‚’å®Ÿè¡Œ
        
        Args:
            resume: å‰å›ã‹ã‚‰å†é–‹ã™ã‚‹ã‹
            dry_run: ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ï¼ˆå®Ÿè¡Œè¨ˆç”»ã®ã¿è¡¨ç¤ºï¼‰
            
        Returns:
            æˆåŠŸãƒ•ãƒ©ã‚°
        """
        execution_mode = self.wfv_config['execution_mode']
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        if not dry_run:
            self.output_dir.mkdir(parents=True, exist_ok=True)
        
        if execution_mode == 'single_period':
            success = self.run_single_period_mode(resume, dry_run)
            if success and not dry_run:
                # ã‚µãƒãƒªãƒ¼ç”Ÿæˆ
                self.generate_single_period_summary()
                # å…¨äºˆæ¸¬çµæœã®çµ±åˆ
                self.generate_consolidated_predictions('single_period')
            return success
        elif execution_mode == 'compare_periods':
            success = self.run_compare_periods_mode(resume, dry_run)
            if success and not dry_run:
                # ã‚µãƒãƒªãƒ¼ç”Ÿæˆ
                self.generate_compare_periods_summary()
                # å…¨äºˆæ¸¬çµæœã®çµ±åˆ
                self.generate_consolidated_predictions('compare_periods')
            return success
        else:
            self.logger.error(f"ä¸æ˜ãªå®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰: {execution_mode}")
            return False
    
    def generate_consolidated_predictions(self, mode: str):
        """å…¨äºˆæ¸¬çµæœã‚’æœŸé–“ã”ã¨ã«çµ±åˆ
        
        Args:
            mode: 'single_period' ã¾ãŸã¯ 'compare_periods'
        """
        try:
            self.logger.info("=" * 80)
            self.logger.info("å…¨äºˆæ¸¬çµæœã®çµ±åˆä¸­...")
            self.logger.info("=" * 80)
            
            if mode == 'single_period':
                settings = self.wfv_config['single_period_settings']
                training_periods = [settings['training_period']]
            else:
                settings = self.wfv_config['compare_periods_settings']
                training_periods = settings['training_periods']
            
            test_years = self.wfv_config['test_years']
            
            for training_period in training_periods:
                period_key = f"period_{training_period}"
                period_dir = self.output_dir / period_key
                test_results_dir = period_dir / "test_results"
                
                if not test_results_dir.exists():
                    self.logger.warning(f"{period_key}: ãƒ†ã‚¹ãƒˆçµæœãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                    continue
                
                self.logger.info(f"{period_key}: äºˆæ¸¬çµæœã‚’çµ±åˆä¸­...")
                
                all_predictions = []
                file_count = 0
                
                # å„å¹´ãƒ»å„ãƒ¢ãƒ‡ãƒ«ã®_all.tsvãƒ•ã‚¡ã‚¤ãƒ«ã‚’åé›†
                for test_year in test_years:
                    year_test_dir = test_results_dir / str(test_year)
                    if not year_test_dir.exists():
                        continue
                    
                    # _all.tsvãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
                    for tsv_file in year_test_dir.glob("predicted_results_*_all.tsv"):
                        try:
                            # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ãƒ¢ãƒ‡ãƒ«åã¨å­¦ç¿’æœŸé–“ã‚’æŠ½å‡º
                            # ä¾‹: predicted_results_tokyo_turf_3ageup_long_2013-2022_test2023_all.tsv
                            filename_parts = tsv_file.stem.split('_')
                            
                            # ãƒ¢ãƒ‡ãƒ«åã‚’æŠ½å‡ºï¼ˆpredicted_results_ã®å¾Œã‹ã‚‰å­¦ç¿’æœŸé–“ã®å‰ã¾ã§ï¼‰
                            model_name_parts = []
                            for part in filename_parts[2:]:
                                if '-' in part and part.replace('-', '').isdigit():
                                    # å­¦ç¿’æœŸé–“ï¼ˆä¾‹: 2013-2022ï¼‰ã«åˆ°é”
                                    training_period_str = part
                                    break
                                model_name_parts.append(part)
                            
                            model_name = '_'.join(model_name_parts)
                            
                            # ãƒ†ã‚¹ãƒˆå¹´ã‚’æŠ½å‡º
                            for part in filename_parts:
                                if part.startswith('test') and part[4:].isdigit():
                                    test_year_str = part[4:]
                                    break
                            
                            # TSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
                            df = pd.read_csv(tsv_file, sep='\t', encoding='utf-8')
                            
                            all_predictions.append(df)
                            file_count += 1
                            
                        except Exception as e:
                            self.logger.warning(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {tsv_file.name} - {e}")
                            continue
                
                if all_predictions:
                    # å…¨ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆ
                    consolidated_df = pd.concat(all_predictions, ignore_index=True)
                    
                    # ä¿å­˜
                    output_file = period_dir / f"all_predictions_period_{training_period}.tsv"
                    consolidated_df.to_csv(output_file, sep='\t', index=False, encoding='utf-8', float_format='%.8f')
                    
                    self.logger.info(f"{period_key}: çµ±åˆå®Œäº†")
                    self.logger.info(f"  ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {file_count}")
                    self.logger.info(f"  ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(consolidated_df)}")
                    self.logger.info(f"  ä¿å­˜å…ˆ: {output_file}")
                else:
                    self.logger.warning(f"{period_key}: çµ±åˆå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            
            self.logger.info("å…¨äºˆæ¸¬çµæœã®çµ±åˆå®Œäº†")
            
        except Exception as e:
            self.logger.error(f"å…¨äºˆæ¸¬çµæœã®çµ±åˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
    
    def generate_single_period_summary(self):
        """å˜ä¸€æœŸé–“ãƒ¢ãƒ¼ãƒ‰ã®ã‚µãƒãƒªãƒ¼ã‚’ç”Ÿæˆ"""
        try:
            self.logger.info("=" * 80)
            self.logger.info("ã‚µãƒãƒªãƒ¼ç”Ÿæˆä¸­...")
            self.logger.info("=" * 80)
            
            settings = self.wfv_config['single_period_settings']
            training_period = settings['training_period']
            test_years = self.wfv_config['test_years']
            
            period_key = f"period_{training_period}"
            period_dir = self.output_dir / period_key
            test_results_dir = period_dir / "test_results"
            
            # å„å¹´ã®ãƒ†ã‚¹ãƒˆçµæœã‚’åé›†
            all_results = []
            
            for test_year in test_years:
                year_test_dir = test_results_dir / str(test_year)
                if not year_test_dir.exists():
                    continue
                
                # ã‚¹ã‚­ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆåˆ†æåˆ—å«ã‚€ï¼‰ã‚’æ¢ã™
                for tsv_file in year_test_dir.glob("predicted_results_*_skipped.tsv"):
                    self.logger.info(f"çµæœé›†è¨ˆä¸­: {tsv_file.name}")
                    
                    # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ãƒ¢ãƒ‡ãƒ«åã¨å­¦ç¿’æœŸé–“ã‚’æŠ½å‡º
                    # ä¾‹: predicted_results_tokyo_turf_3ageup_long_2013-2022_test2023_skipped.tsv
                    filename_parts = tsv_file.stem.replace("predicted_results_", "").replace("_skipped", "").split("_")
                    # æœ€å¾ŒãŒ "testYYYY" ã®å½¢å¼
                    # ãã®å‰ãŒå­¦ç¿’æœŸé–“ "YYYY-YYYY"
                    train_period_str = filename_parts[-2]  # "2013-2022"
                    model_name = "_".join(filename_parts[:-2])  # "tokyo_turf_3ageup_long"
                    
                    # TSVãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
                    try:
                        df = pd.read_csv(tsv_file, sep='\t', encoding='utf-8-sig')
                        
                        if len(df) == 0:
                            continue
                        
                        # å…¨ãƒ¬ãƒ¼ã‚¹ã®DataFrame
                        df_full = df.copy()
                        
                        # è³¼å…¥æ¨å¥¨é¦¬ã‚’æŠ½å‡º
                        if 'è³¼å…¥æ¨å¥¨' in df.columns:
                            buy_horses = df[df['è³¼å…¥æ¨å¥¨'] == True].copy()
                        else:
                            # è³¼å…¥æ¨å¥¨åˆ—ãŒãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                            self.logger.warning(f"è³¼å…¥æ¨å¥¨åˆ—ãªã—: {tsv_file.name}")
                            continue
                        
                        if len(buy_horses) == 0:
                            self.logger.warning(f"è³¼å…¥æ¨å¥¨é¦¬0é ­: {tsv_file.name}")
                            continue
                        
                        # ãƒ¬ãƒ¼ã‚¹æ•°ï¼ˆå…¨ãƒ¬ãƒ¼ã‚¹ï¼‰
                        race_count = df_full.groupby(['é–‹å‚¬å¹´', 'é–‹å‚¬æ—¥', 'ç«¶é¦¬å ´', 'ãƒ¬ãƒ¼ã‚¹ç•ªå·']).ngroups
                        
                        # è³¼å…¥æ¨å¥¨é¦¬æ•°
                        buy_count = len(buy_horses)
                        
                        # é¦¬åˆ¸ç¨®åˆ¥ã”ã¨ã®é›†è¨ˆ
                        results = self._calculate_betting_results(buy_horses, df_full)
                        
                        all_results.append({
                            'ãƒ¢ãƒ‡ãƒ«å': model_name,
                            'å­¦ç¿’æœŸé–“': train_period_str,
                            'ãƒ†ã‚¹ãƒˆå¹´': test_year,
                            'ãƒ¬ãƒ¼ã‚¹æ•°': race_count,
                            'è³¼å…¥æ¨å¥¨é¦¬æ•°': buy_count,
                            'å˜å‹çš„ä¸­æ•°': results['tansho_hit'],
                            'å˜å‹çš„ä¸­ç‡': f"{results['tansho_rate']*100:.1f}%",
                            'å˜å‹å›åç‡': f"{results['tansho_return']*100:.1f}%",
                            'è¤‡å‹çš„ä¸­æ•°': results['fukusho_hit'],
                            'è¤‡å‹çš„ä¸­ç‡': f"{results['fukusho_rate']*100:.1f}%",
                            'è¤‡å‹å›åç‡': f"{results['fukusho_return']*100:.1f}%",
                            'é¦¬é€£çš„ä¸­æ•°': results.get('umaren_hit', 0),
                            'é¦¬é€£çš„ä¸­ç‡': f"{results.get('umaren_rate', 0)*100:.1f}%",
                            'é¦¬é€£å›åç‡': f"{results.get('umaren_return', 0)*100:.1f}%",
                            'ãƒ¯ã‚¤ãƒ‰çš„ä¸­æ•°': results.get('wide_hit', 0),
                            'ãƒ¯ã‚¤ãƒ‰çš„ä¸­ç‡': f"{results.get('wide_rate', 0)*100:.1f}%",
                            'ãƒ¯ã‚¤ãƒ‰å›åç‡': f"{results.get('wide_return', 0)*100:.1f}%",
                            'ç©´é¦¬å€™è£œ': results.get('upset_candidates', 0),
                            'ç©´é¦¬çš„ä¸­': results.get('upset_hits', 0),
                            'ç©´é¦¬é©åˆç‡': f"{results.get('upset_precision', 0):.1f}%",
                            'ç©´é¦¬å†ç¾ç‡': f"{results.get('upset_recall', 0):.1f}%",
                            'ç©´é¦¬ROI': f"{results.get('upset_roi', 0):.1f}%"
                        })
                        
                    except Exception as e:
                        self.logger.warning(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {tsv_file.name} - {str(e)}")
                        continue
            
            if len(all_results) == 0:
                self.logger.warning("é›†è¨ˆå¯èƒ½ãªçµæœãŒã‚ã‚Šã¾ã›ã‚“")
                return
            
            # DataFrameã«å¤‰æ›
            summary_df = pd.DataFrame(all_results)
            
            # ã‚µãƒãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
            summary_file = period_dir / f"summary_period_{training_period}.tsv"
            summary_df.to_csv(summary_file, sep='\t', index=False, encoding='utf-8-sig', float_format='%.8f')
            
            self.logger.info(f"ã‚µãƒãƒªãƒ¼ä¿å­˜å®Œäº†: {summary_file}")
            self.logger.info(f"é›†è¨ˆçµæœ: {len(all_results)}ä»¶")
            
            # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«ã‚‚è¡¨ç¤º
            self.logger.info("\n" + summary_df.to_string(index=False))
            
        except Exception as e:
            self.logger.error(f"ã‚µãƒãƒªãƒ¼ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
            self.logger.debug(traceback.format_exc())
    
    def generate_compare_periods_summary(self):
        """æœŸé–“æ¯”è¼ƒãƒ¢ãƒ¼ãƒ‰ã®ã‚µãƒãƒªãƒ¼ã‚’ç”Ÿæˆ"""
        try:
            self.logger.info("=" * 80)
            self.logger.info("æœŸé–“æ¯”è¼ƒã‚µãƒãƒªãƒ¼ç”Ÿæˆä¸­...")
            self.logger.info("=" * 80)
            
            settings = self.wfv_config['compare_periods_settings']
            training_periods = settings['training_periods']
            
            # å„æœŸé–“ã®ã‚µãƒãƒªãƒ¼ã‚’ç”Ÿæˆ
            for period in training_periods:
                self.logger.info(f"\næœŸé–“ {period}å¹´ ã®ã‚µãƒãƒªãƒ¼ç”Ÿæˆ...")
                # ä¸€æ™‚çš„ã«è¨­å®šã‚’å¤‰æ›´ã—ã¦å˜ä¸€æœŸé–“ã‚µãƒãƒªãƒ¼ã‚’ç”Ÿæˆ
                original_mode = self.wfv_config['execution_mode']
                original_settings = self.wfv_config.get('single_period_settings', {})
                
                self.wfv_config['execution_mode'] = 'single_period'
                self.wfv_config['single_period_settings'] = {
                    'training_period': period,
                    'rolling_type': 'fixed',
                    'models': settings['models']
                }
                
                self.generate_single_period_summary()
                
                # è¨­å®šã‚’æˆ»ã™
                self.wfv_config['execution_mode'] = original_mode
                self.wfv_config['single_period_settings'] = original_settings
            
            self.logger.info("\næœŸé–“æ¯”è¼ƒãƒ¢ãƒ¼ãƒ‰: å…¨æœŸé–“ã®ã‚µãƒãƒªãƒ¼ç”Ÿæˆå®Œäº†")
            
        except Exception as e:
            self.logger.error(f"æœŸé–“æ¯”è¼ƒã‚µãƒãƒªãƒ¼ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
            self.logger.debug(traceback.format_exc())


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(
        description='Walk-Forward Validation for Horse Racing Prediction Models'
    )
    parser.add_argument(
        '--config',
        default='walk_forward_config.json',
        help='è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: walk_forward_config.json)'
    )
    parser.add_argument(
        '--resume',
        action='store_true',
        help='å‰å›ã®å®Ÿè¡Œã‚’é€”ä¸­ã‹ã‚‰å†é–‹'
    )
    parser.add_argument(
        '--dry-run',
        action='store_true',
        help='å®Ÿè¡Œè¨ˆç”»ã®ã¿è¡¨ç¤ºï¼ˆå®Ÿéš›ã«ã¯å®Ÿè¡Œã—ãªã„ï¼‰'
    )
    parser.add_argument(
        '--clean',
        action='store_true',
        help='å‰å›ã®å®Ÿè¡Œçµæœã‚’å‰Šé™¤ã—ã¦ã‚¯ãƒªãƒ¼ãƒ³ã‚¹ã‚¿ãƒ¼ãƒˆ'
    )
    parser.add_argument(
        '--verbose',
        action='store_true',
        help='è©³ç´°ãƒ­ã‚°ã‚’å‡ºåŠ›ï¼ˆDEBUGãƒ¬ãƒ™ãƒ«ï¼‰'
    )
    
    args = parser.parse_args()
    
    # Validatorã‚’ä½œæˆ
    try:
        validator = WalkForwardValidator(args.config)
    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return 1
    
    # verboseãƒ¢ãƒ¼ãƒ‰
    if args.verbose:
        validator.logger.setLevel(logging.DEBUG)
    
    # cleanãƒ¢ãƒ¼ãƒ‰
    if args.clean:
        progress_file = validator.output_dir / "progress.json"
        if progress_file.exists():
            progress_file.unlink()
            validator.logger.info("é€²æ—ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
    
    # å®Ÿè¡Œ
    try:
        success = validator.run(resume=args.resume, dry_run=args.dry_run)
        return 0 if success else 1
    except KeyboardInterrupt:
        validator.logger.info("ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã£ã¦ä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
        return 130
    except Exception as e:
        validator.logger.error(f"äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {str(e)}")
        validator.logger.debug(traceback.format_exc())
        return 1


if __name__ == '__main__':
    sys.exit(main())
