"""
Phase 2: ç©´é¦¬åˆ†é¡ãƒ¢ãƒ‡ãƒ«ä½œæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ

7-12ç•ªäººæ°—ã§3ç€ä»¥å†…ã«å…¥ã‚‹ç©´é¦¬ã‚’æ¤œå‡ºã™ã‚‹äºŒå€¤åˆ†é¡ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’

å®Ÿè£…å†…å®¹:
- LightGBM Classifierã§äºŒå€¤åˆ†é¡
- ã‚¯ãƒ©ã‚¹ã‚¦ã‚§ã‚¤ãƒˆèª¿æ•´ã§ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿ã«å¯¾å¿œï¼ˆSMOTEãªã—ï¼‰
- 5-fold Cross Validationã§è©•ä¾¡
- ãƒ¢ãƒ‡ãƒ«ã‚’models/upset_classifier.savã«ä¿å­˜
"""

import pandas as pd
import numpy as np
import pickle
from pathlib import Path
from sklearn.model_selection import StratifiedKFold, TimeSeriesSplit
from sklearn.metrics import (
    precision_score, recall_score, f1_score, roc_auc_score,
    confusion_matrix, classification_report
)
from sklearn.isotonic import IsotonicRegression
from sklearn.linear_model import LogisticRegression
import lightgbm as lgb


def calibrate_probabilities(y_train_proba, y_train, method='platt'):
    """
    ç¢ºç‡æ ¡æ­£å™¨ã‚’ä½œæˆ
    
    Args:
        y_train_proba: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®äºˆæ¸¬ç¢ºç‡
        y_train: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ãƒ©ãƒ™ãƒ«
        method: 'platt' (ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰) or 'isotonic' (å˜èª¿å›å¸°)
    
    Returns:
        æ ¡æ­£å™¨ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
    """
    if method == 'isotonic':
        calibrator = IsotonicRegression(y_min=0.0, y_max=1.0, out_of_bounds='clip')
        calibrator.fit(y_train_proba, y_train)
    else:  # platt (default)
        # Platt Scaling: ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¢æ•°ã§ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°
        calibrator = LogisticRegression(solver='lbfgs', max_iter=1000)
        calibrator.fit(y_train_proba.reshape(-1, 1), y_train)
    return calibrator


def apply_calibration(calibrator, proba, method='platt'):
    """
    æ ¡æ­£å™¨ã‚’é©ç”¨
    """
    if method == 'isotonic':
        return calibrator.predict(proba)
    else:  # platt
        return calibrator.predict_proba(proba.reshape(-1, 1))[:, 1]


def load_training_data(file_path: str = 'results/upset_training_data_universal.tsv'):
    """
    è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ï¼ˆPhase 2.5: å…¨10ç«¶é¦¬å ´çµ±åˆãƒ‡ãƒ¼ã‚¿ï¼‰
    """
    print(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: {file_path}")
    df = pd.read_csv(file_path, sep='\t')
    
    # 7-12ç•ªäººæ°—ã®ãƒ‡ãƒ¼ã‚¿æ•°ã‚’è¨ˆç®—ï¼ˆç©´é¦¬ç‡ã®æ­£ç¢ºãªè¡¨ç¤ºã®ãŸã‚ï¼‰
    if 'popularity_rank' in df.columns:
        df_target = df[(df['popularity_rank'] >= 7) & (df['popularity_rank'] <= 12)]
        target_count = len(df_target)
        upset_count = df_target['is_upset'].sum()
        upset_rate = upset_count / target_count * 100 if target_count > 0 else 0
        print(f"  å…¨ãƒ‡ãƒ¼ã‚¿æ•°: {len(df)}é ­")
        print(f"  7-12ç•ªäººæ°—: {target_count}é ­")
        print(f"  ç©´é¦¬ï¼ˆ7-12ç•ªäººæ°—ã§3ç€ä»¥å†…ï¼‰: {upset_count}é ­ ({upset_rate:.2f}%)")
    else:
        print(f"  ãƒ‡ãƒ¼ã‚¿æ•°: {len(df)}é ­")
        print(f"  ç©´é¦¬: {df['is_upset'].sum()}é ­ ({df['is_upset'].mean() * 100:.2f}%)")
    
    return df


def prepare_features(df: pd.DataFrame):
    """
    ç‰¹å¾´é‡ã¨ãƒ©ãƒ™ãƒ«ã‚’æº–å‚™
    """
    print(f"\n[DEBUG] prepare_featuresé–‹å§‹: df.shape={df.shape}, df.indexç¯„å›²=[{df.index.min()}, {df.index.max()}]")
    
    # ç‰¹å¾´é‡ã‚«ãƒ©ãƒ ï¼ˆis_upset, ãƒ¡ã‚¿æƒ…å ±ä»¥å¤–ï¼‰
    exclude_cols = [
        'is_upset',
        'kaisai_nen', 'kaisai_tsukihi', 'keibajo_code', 'race_bango',
        'bamei', 'umaban', 'kakutei_chakujun_numeric'
    ]
    
    feature_cols = [col for col in df.columns if col not in exclude_cols]
    
    X = df[feature_cols].copy()
    y = df['is_upset'].copy()
    
    print(f"[DEBUG] X, yä½œæˆå¾Œ: X.shape={X.shape}, y.shape={y.shape}, X.indexç¯„å›²=[{X.index.min()}, {X.index.max()}]")
    
    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ãƒªã‚»ãƒƒãƒˆï¼ˆTimeSeriesSplitã§æ­£ã—ãå‹•ä½œã™ã‚‹ã‚ˆã†ã«ï¼‰
    X = X.reset_index(drop=True)
    y = y.reset_index(drop=True)
    
    print(f"[DEBUG] reset_indexå¾Œ: X.shape={X.shape}, y.shape={y.shape}, X.indexç¯„å›²=[{X.index.min()}, {X.index.max()}]")
    
    # æ¬ æå€¤ã‚’0ã§åŸ‹ã‚ã‚‹
    X = X.fillna(0)
    
    # ç„¡é™å¤§ã‚’æœ€å¤§å€¤/æœ€å°å€¤ã§ç½®æ›
    X = X.replace([np.inf, -np.inf], 0)
    
    print(f"\nç‰¹å¾´é‡æ•°: {len(feature_cols)}å€‹")
    print(f"ç‰¹å¾´é‡: {', '.join(feature_cols)}")
    
    return X, y, feature_cols


def train_with_class_weights(
    X: pd.DataFrame,
    y: pd.Series,
    feature_cols: list,
    n_splits: int = 5,
    random_state: int = 42,
    use_timeseries: bool = True,
    use_calibration: bool = True
):
    """
    ã‚¯ãƒ©ã‚¹ã‚¦ã‚§ã‚¤ãƒˆã§ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿ã‚’èª¿æ•´ã—ã¦å­¦ç¿’ï¼ˆSMOTEãªã—ï¼‰
    
    Args:
        X: ç‰¹å¾´é‡
        y: ãƒ©ãƒ™ãƒ«
        feature_cols: ç‰¹å¾´é‡åãƒªã‚¹ãƒˆ
        n_splits: CVã®foldæ•°
        random_state: ä¹±æ•°ã‚·ãƒ¼ãƒ‰
        use_timeseries: TimeSeriesSplitã‚’ä½¿ç”¨ã™ã‚‹ã‹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: Trueï¼‰
        use_calibration: ç¢ºç‡æ ¡æ­£ã‚’é©ç”¨ã™ã‚‹ã‹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: Trueï¼‰
    
    Returns:
        å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒªã‚¹ãƒˆã€è©•ä¾¡çµæœ
    """
    print(f"\n{'='*80}")
    print(f"ã‚¯ãƒ©ã‚¹ã‚¦ã‚§ã‚¤ãƒˆã‚’ä½¿ã£ãŸå­¦ç¿’é–‹å§‹ï¼ˆSMOTEãªã—ï¼‰")
    if use_calibration:
        print(f"ğŸ¯ Phase A: ç¢ºç‡æ ¡æ­£ï¼ˆIsotonic Regressionï¼‰æœ‰åŠ¹")
    print(f"{'='*80}")
    if use_timeseries:
        print(f"Cross Validation: TimeSeriesSplit {n_splits}-fold (æ™‚ç³»åˆ—å¯¾å¿œ)")
    else:
        print(f"Cross Validation: StratifiedKFold {n_splits}-fold")
    
    # ä¸å‡è¡¡æ¯”ç‡ã‚’è¨ˆç®—
    pos_count = y.sum()
    neg_count = len(y) - pos_count
    base_scale_pos_weight = neg_count / pos_count
    
    # scale_pos_weightèª¿æ•´ä¿‚æ•°ï¼ˆãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ï¼‰
    # 1.0 = ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼ˆneg/posæ¯”ãã®ã¾ã¾ï¼‰â† ãƒ‡ãƒ¼ã‚¿åˆ†æã®çµæœã€ã“ã‚ŒãŒæ­£ã—ã„ï¼ˆ2026-01-21ï¼‰
    # 0.5-0.8 = æ­£ä¾‹ã®é‡ã¿ã‚’ä¸‹ã’ã‚‹ï¼ˆPrecisionã‚’é‡è¦–ï¼‰
    # 1.2-2.0 = æ­£ä¾‹ã®é‡ã¿ã‚’ä¸Šã’ã‚‹ï¼ˆRecallã‚’é‡è¦–ï¼‰
    # 
    # â€» éå»ã®å®Ÿé¨“å±¥æ­´:
    #   - 2.0: Precision 6.20%â†’4.54%ã§å¤±æ•—
    #   - 0.7: æ ¡æ­£ã‚«ãƒ¼ãƒ–é€†è»¢ï¼ˆä½ç¢ºç‡16%çš„ä¸­ã€é«˜ç¢ºç‡0%çš„ä¸­ï¼‰
    #   - 0.5: Recallå´©å£Šï¼ˆ27%ï¼‰
    # 
    # SQLåˆ†æçµæœï¼ˆ2026-01-21ï¼‰:
    #   - è¨“ç·´ãƒ‡ãƒ¼ã‚¿ç©´é¦¬ç‡ï¼ˆ7-12ç•ªäººæ°—ãƒ™ãƒ¼ã‚¹ï¼‰: 9.72%
    #   - ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç©´é¦¬ç‡ï¼ˆå‡½é¤¨7-12ç•ªäººæ°—ï¼‰: 10.77%
    #   - å·®ã¯1.1å€ç¨‹åº¦ â†’ scale_adjustment=1.0ãŒé©åˆ‡
    scale_adjustment = 1.0  # ãƒ‡ãƒ¼ã‚¿åˆ†æã§è¨“ç·´9.72% vs ãƒ†ã‚¹ãƒˆ10.77%ã¨åˆ¤æ˜ï¼ˆå·®1.1å€ï¼‰
    scale_pos_weight = base_scale_pos_weight * scale_adjustment
    
    print(f"ä¸å‡è¡¡æ¯”ç‡: 1:{base_scale_pos_weight:.1f}")
    print(f"scale_pos_weightèª¿æ•´: {scale_adjustment}x â†’ {scale_pos_weight:.2f}")
    print(f"  â€» SQLåˆ†æçµæœ: è¨“ç·´ãƒ‡ãƒ¼ã‚¿9.72% vs ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿10.77%ï¼ˆå·®1.1å€ï¼‰")
    print(f"  â€» scale_adjustment=1.0ã§è¨“ç·´ã¨ãƒ†ã‚¹ãƒˆã®ç©´é¦¬ç‡ãŒã»ã¼ä¸€è‡´")
    print()
    
    # LightGBMãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆã‚¯ãƒ©ã‚¹ã‚¦ã‚§ã‚¤ãƒˆèª¿æ•´ï¼‰
    params = {
        'objective': 'binary',
        'metric': 'auc',
        'boosting_type': 'gbdt',
        'num_leaves': 31,
        'learning_rate': 0.05,
        'feature_fraction': 0.9,
        'bagging_fraction': 0.8,
        'bagging_freq': 5,
        'min_data_in_leaf': 10,
        'verbose': -1,
        'random_state': random_state,
        'scale_pos_weight': scale_pos_weight  # ä¸å‡è¡¡ãƒ‡ãƒ¼ã‚¿å¯¾ç­–ï¼ˆè‡ªå‹•è¨ˆç®—ï¼‰
        # is_unbalanceã¨scale_pos_weightã¯åŒæ™‚ã«è¨­å®šã§ããªã„ãŸã‚ã€scale_pos_weightã®ã¿ä½¿ç”¨
    }
    
    # Cross Validationè¨­å®šï¼ˆæ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯é˜²æ­¢ï¼‰
    if use_timeseries:
        cv_splitter = TimeSeriesSplit(n_splits=n_splits)
    else:
        cv_splitter = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)
    
    models = []
    cv_results = []
    
    print(f"[DEBUG] CVé–‹å§‹: X.shape={X.shape}, y.shape={y.shape}, X.indexç¯„å›²=[{X.index.min()}, {X.index.max()}]")
    
    for fold, (train_idx, val_idx) in enumerate(cv_splitter.split(X, y), 1):
        print(f"Fold {fold}/{n_splits}")
        print(f"[DEBUG] train_idx: min={train_idx.min()}, max={train_idx.max()}, len={len(train_idx)}")
        print(f"[DEBUG] val_idx: min={val_idx.min()}, max={val_idx.max()}, len={len(val_idx)}")
        print(f"[DEBUG] X.shape={X.shape}, len(X)={len(X)}")
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç¯„å›²ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
        max_idx = len(X) - 1
        if train_idx.max() > max_idx or val_idx.max() > max_idx:
            print(f"  âš  ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚¨ãƒ©ãƒ¼æ¤œå‡º: train_idx.max()={train_idx.max()}, val_idx.max()={val_idx.max()}, max_idx={max_idx}")
            raise IndexError(f"train_idx.max()={train_idx.max()} or val_idx.max()={val_idx.max()} exceeds max_idx={max_idx}")
        
        X_train, X_val = X.iloc[train_idx].copy(), X.iloc[val_idx].copy()
        y_train, y_val = y.iloc[train_idx].copy(), y.iloc[val_idx].copy()
        
        print(f"  è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(X_train)}é ­ (ç©´é¦¬: {y_train.sum()}é ­ = {y_train.mean()*100:.2f}%)")
        print(f"  æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {len(X_val)}é ­ (ç©´é¦¬: {y_val.sum()}é ­ = {y_val.mean()*100:.2f}%)")
        
        # LightGBM Datasetä½œæˆï¼ˆSMOTEãªã—ï¼‰
        train_data = lgb.Dataset(X_train, y_train, feature_name=feature_cols)
        val_data = lgb.Dataset(X_val, y_val, reference=train_data, feature_name=feature_cols)
        
        # å­¦ç¿’ï¼ˆearly_stoppingæœ€é©åŒ–: 50â†’20ã«å‰Šæ¸›ã§é«˜é€ŸåŒ–ï¼‰
        model = lgb.train(
            params,
            train_data,
            num_boost_round=300,  # 500â†’300ã«å‰Šæ¸›ï¼ˆé€šå¸¸100-200ã§åæŸï¼‰
            valid_sets=[train_data, val_data],
            valid_names=['train', 'valid'],
            callbacks=[
                lgb.early_stopping(stopping_rounds=20, verbose=False),  # 50â†’20ã§é«˜é€ŸåŒ–
                lgb.log_evaluation(period=50)  # 100â†’50ã§ãƒ­ã‚°é »åº¦ã‚¢ãƒƒãƒ—
            ]
        )
        
        # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã§äºˆæ¸¬
        y_pred_proba = model.predict(X_val, num_iteration=model.best_iteration)
        
        # ç¢ºç‡æ ¡æ­£ï¼ˆæ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã£ã¦å­¦ç¿’ - éå­¦ç¿’é˜²æ­¢ï¼‰
        calibrator = None
        calibration_method = 'platt'  # 'platt' or 'isotonic'
        if use_calibration:
            # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã®äºˆæ¸¬ç¢ºç‡ã‚’ä½¿ã£ã¦æ ¡æ­£å™¨ã‚’å­¦ç¿’ï¼ˆè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§ã¯ãªã„ï¼ï¼‰
            # ã“ã‚Œã«ã‚ˆã‚Šã€æ ¡æ­£å™¨ã¯ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ãŸã“ã¨ã®ãªã„ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’ã•ã‚Œã‚‹
            calibrator = calibrate_probabilities(
                y_pred_proba, y_val.values, 
                method=calibration_method
            )
            
            # æ ¡æ­£å¾Œã®ç¢ºç‡ã‚’å–å¾—ï¼ˆè©•ä¾¡ç”¨ï¼‰
            y_pred_proba_raw = y_pred_proba.copy()
            y_pred_proba_calibrated = apply_calibration(calibrator, y_pred_proba, method=calibration_method)
            
            print(f"  [æ ¡æ­£å‰] min={y_pred_proba_raw.min():.4f}, max={y_pred_proba_raw.max():.4f}, mean={y_pred_proba_raw.mean():.4f}")
            print(f"  [æ ¡æ­£å¾Œ] min={y_pred_proba_calibrated.min():.4f}, max={y_pred_proba_calibrated.max():.4f}, mean={y_pred_proba_calibrated.mean():.4f}")
            
            # è©•ä¾¡ã¯æ ¡æ­£å‰ã®ç¢ºç‡ã§è¡Œã†ï¼ˆCVã®å…¬å¹³æ€§ã®ãŸã‚ï¼‰
            # ãƒ†ã‚¹ãƒˆæ™‚ã«ã¯æ ¡æ­£å¾Œã‚’ä½¿ã†
        else:
            # ç¢ºç‡åˆ†å¸ƒã‚’ç¢ºèª
            print(f"  ç¢ºç‡åˆ†å¸ƒ: min={y_pred_proba.min():.4f}, max={y_pred_proba.max():.4f}, mean={y_pred_proba.mean():.4f}, median={np.median(y_pred_proba):.4f}")
        
        # å‹•çš„é–¾å€¤ã§è©•ä¾¡ï¼ˆå®Ÿãƒ‡ãƒ¼ã‚¿ä¸å‡è¡¡æ¯”ç‡ã«åŸºã¥ãï¼‰
        optimal_threshold = y_train.mean()  # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ç©´é¦¬æ¯”ç‡
        y_pred = (y_pred_proba > optimal_threshold).astype(int)
        
        # è©•ä¾¡
        precision = precision_score(y_val, y_pred, zero_division=0)
        recall = recall_score(y_val, y_pred, zero_division=0)
        f1 = f1_score(y_val, y_pred, zero_division=0)
        auc = roc_auc_score(y_val, y_pred_proba) if y_val.sum() > 0 else 0
        
        print(f"  é–¾å€¤: {optimal_threshold:.4f} (è¨“ç·´ãƒ‡ãƒ¼ã‚¿ç©´é¦¬æ¯”ç‡)")
        print(f"  Precision: {precision:.2%}")
        print(f"  Recall: {recall:.2%}")
        print(f"  F1 Score: {f1:.2%}")
        print(f"  AUC: {auc:.4f}")
        print()
        
        # ãƒ¢ãƒ‡ãƒ«ã¨æ ¡æ­£å™¨ã‚’ãƒšã‚¢ã§ä¿å­˜
        models.append({
            'model': model,
            'calibrator': calibrator,  # use_calibration=Falseã®å ´åˆã¯None
            'calibration_method': calibration_method if use_calibration else None
        })
        cv_results.append({
            'fold': fold,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'auc': auc
        })
    
    # CVçµæœã‚µãƒãƒªãƒ¼
    df_cv = pd.DataFrame(cv_results)
    print(f"{'='*80}")
    print(f"Cross Validationçµæœã‚µãƒãƒªãƒ¼")
    print(f"{'='*80}")
    print(df_cv.to_string(index=False))
    print()
    print(f"å¹³å‡ Precision: {df_cv['precision'].mean():.2%} (Â±{df_cv['precision'].std():.2%})")
    print(f"å¹³å‡ Recall: {df_cv['recall'].mean():.2%} (Â±{df_cv['recall'].std():.2%})")
    print(f"å¹³å‡ F1 Score: {df_cv['f1'].mean():.2%} (Â±{df_cv['f1'].std():.2%})")
    print(f"å¹³å‡ AUC: {df_cv['auc'].mean():.4f} (Â±{df_cv['auc'].std():.4f})")
    
    return models, df_cv


def save_models(models: list, feature_cols: list, output_dir: str = 'models'):
    """
    å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ï¼ˆPhase 2.5: å…¨10ç«¶é¦¬å ´çµ±åˆãƒ¢ãƒ‡ãƒ«ï¼‰
    Phase A: ç¢ºç‡æ ¡æ­£å™¨ã‚‚å«ã‚ã¦ä¿å­˜
    """
    Path(output_dir).mkdir(exist_ok=True)
    
    # ãƒ¢ãƒ‡ãƒ«ã¨æ ¡æ­£å™¨ã‚’åˆ†é›¢
    lgb_models = [m['model'] for m in models]
    calibrators = [m['calibrator'] for m in models]
    calibration_method = models[0].get('calibration_method', None)
    has_calibration = calibrators[0] is not None
    
    # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ï¼ˆå…¨foldã®ãƒ¢ãƒ‡ãƒ«ï¼‰ã‚’ä¿å­˜
    model_data = {
        'models': lgb_models,
        'calibrators': calibrators,
        'feature_cols': feature_cols,
        'n_models': len(models),
        'has_calibration': has_calibration,
        'calibration_method': calibration_method
    }
    
    output_file = Path(output_dir) / 'upset_classifier_universal.sav'
    with open(output_file, 'wb') as f:
        pickle.dump(model_data, f)
    
    print(f"\nãƒ¢ãƒ‡ãƒ«ã‚’ {output_file} ã«ä¿å­˜ã—ã¾ã—ãŸ")
    print(f"  ãƒ¢ãƒ‡ãƒ«æ•°: {len(models)}å€‹ (ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«)")
    print(f"  ç‰¹å¾´é‡æ•°: {len(feature_cols)}å€‹")
    if has_calibration:
        print(f"  ğŸ¯ ç¢ºç‡æ ¡æ­£: æœ‰åŠ¹ï¼ˆIsotonic Regressionï¼‰")


def analyze_feature_importance(models: list, feature_cols: list, top_n: int = 20):
    """
    ç‰¹å¾´é‡é‡è¦åº¦ã‚’åˆ†æ
    """
    print(f"\n{'='*80}")
    print(f"ç‰¹å¾´é‡é‡è¦åº¦ (Top {top_n})")
    print(f"{'='*80}")
    
    # æ–°ã—ã„æ§‹é€ ï¼ˆdictï¼‰ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’å–ã‚Šå‡ºã™
    lgb_models = [m['model'] if isinstance(m, dict) else m for m in models]
    
    # å…¨ãƒ¢ãƒ‡ãƒ«ã®ç‰¹å¾´é‡é‡è¦åº¦ã‚’å¹³å‡
    importance_dict = {feat: [] for feat in feature_cols}
    
    for model in lgb_models:
        importances = model.feature_importance(importance_type='gain')
        for feat, imp in zip(feature_cols, importances):
            importance_dict[feat].append(imp)
    
    # å¹³å‡ã¨æ¨™æº–åå·®ã‚’è¨ˆç®—
    importance_summary = []
    for feat, imps in importance_dict.items():
        importance_summary.append({
            'feature': feat,
            'importance_mean': np.mean(imps),
            'importance_std': np.std(imps)
        })
    
    df_importance = pd.DataFrame(importance_summary)
    df_importance = df_importance.sort_values('importance_mean', ascending=False)
    
    print(df_importance.head(top_n).to_string(index=False))
    
    # CSVã«ä¿å­˜
    output_file = Path('results') / 'upset_classifier_feature_importance.tsv'
    df_importance.to_csv(output_file, sep='\t', index=False, float_format='%.8f')
    print(f"\nç‰¹å¾´é‡é‡è¦åº¦ã‚’ {output_file} ã«ä¿å­˜ã—ã¾ã—ãŸ")


def main():
    """
    ãƒ¡ã‚¤ãƒ³å‡¦ç†
    """
    print("="*80)
    print("Phase 2: ç©´é¦¬åˆ†é¡ãƒ¢ãƒ‡ãƒ«ä½œæˆï¼ˆã‚¯ãƒ©ã‚¹ã‚¦ã‚§ã‚¤ãƒˆæ–¹å¼ï¼‰")
    print("="*80)
    print()
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    df = load_training_data()
    
    # ç‰¹å¾´é‡æº–å‚™
    X, y, feature_cols = prepare_features(df)
    
    # å­¦ç¿’ï¼ˆã‚¯ãƒ©ã‚¹ã‚¦ã‚§ã‚¤ãƒˆã®ã¿ã€SMOTEãªã—ï¼‰
    models, cv_results = train_with_class_weights(
        X, y, feature_cols,
        n_splits=5,
        random_state=42
    )
    
    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
    save_models(models, feature_cols)
    
    # ç‰¹å¾´é‡é‡è¦åº¦åˆ†æ
    analyze_feature_importance(models, feature_cols, top_n=20)
    
    print(f"\n{'='*80}")
    print("å­¦ç¿’å®Œäº†!")
    print(f"{'='*80}")
    print("\næ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
    print("  1. upset_predictor.py ã§äºˆæ¸¬ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰")
    print("  2. 2019-2023å¹´ãƒ‡ãƒ¼ã‚¿ã§è©•ä¾¡")
    print("  3. Phase 1ã¨æ¯”è¼ƒ")


if __name__ == '__main__':
    main()
