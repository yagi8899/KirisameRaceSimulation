#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ±ç”¨ç«¶é¦¬äºˆæ¸¬ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ã€è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã«å¯¾å¿œã—ãŸç«¶é¦¬äºˆæ¸¬ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
model_creator.pyã§ä½œæˆã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦äºˆæ¸¬ã‚’è¡Œã„ã€çµæœã‚’ä¿å­˜ã—ã¾ã™ã€‚
"""

import psycopg2
import pandas as pd
import pickle
import lightgbm as lgb
import numpy as np
import os
from pathlib import Path
from datetime import datetime
from keiba_constants import get_track_name, format_model_description
from model_config_loader import get_all_models, get_legacy_model
from db_query_builder import build_race_data_query

# Phase 1: æœŸå¾…å€¤ãƒ»ã‚±ãƒªãƒ¼åŸºæº–ãƒ»ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã®çµ±åˆ
from expected_value_calculator import ExpectedValueCalculator
from kelly_criterion import KellyCriterion
from race_confidence_scorer import RaceConfidenceScorer

# Phase 2.5: ç©´é¦¬äºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ï¼ˆå…¨10ç«¶é¦¬å ´çµ±åˆï¼‰
import pickle
import json


def load_upset_threshold(track_code: str = None, surface: str = None, distance_category: str = None) -> float:
    """
    è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç©´é¦¬å€™è£œåˆ¤å®šã®é–¾å€¤ã‚’èª­ã¿è¾¼ã‚€
    
    å„ªå…ˆé †ä½:
    1. by_track_surface_distanceï¼ˆæœ€ã‚‚å…·ä½“çš„ï¼‰
    2. by_track_surface
    3. by_track
    4. by_surface
    5. by_distance
    6. default_thresholdï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
    
    Args:
        track_code: ç«¶é¦¬å ´ã‚³ãƒ¼ãƒ‰ï¼ˆ01-10ï¼‰
        surface: èŠãƒ€åŒºåˆ†ï¼ˆ'turf' or 'dirt'ï¼‰
        distance_category: è·é›¢åŒºåˆ†ï¼ˆ'short' or 'long'ï¼‰
    
    Returns:
        float: é–¾å€¤
    """
    config_path = Path(__file__).parent / 'upset_threshold_config.json'
    default_threshold = 0.20
    
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
    except (FileNotFoundError, json.JSONDecodeError) as e:
        print(f"[UPSET-THRESHOLD] è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        print(f"[UPSET-THRESHOLD] ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé–¾å€¤ {default_threshold} ã‚’ä½¿ç”¨")
        return default_threshold
    
    default_threshold = config.get('default_threshold', default_threshold)
    thresholds = config.get('thresholds_by_condition', {})
    
    # 1. æœ€ã‚‚å…·ä½“çš„: ç«¶é¦¬å ´_èŠãƒ€_è·é›¢åŒºåˆ†
    if track_code and surface and distance_category:
        key = f"{track_code}_{surface}_{distance_category}"
        if key in thresholds.get('by_track_surface_distance', {}):
            threshold = thresholds['by_track_surface_distance'][key]
            print(f"[UPSET-THRESHOLD] {key} ã®é–¾å€¤ã‚’ä½¿ç”¨: {threshold}")
            return threshold
    
    # 2. ç«¶é¦¬å ´_èŠãƒ€
    if track_code and surface:
        key = f"{track_code}_{surface}"
        if key in thresholds.get('by_track_surface', {}):
            threshold = thresholds['by_track_surface'][key]
            print(f"[UPSET-THRESHOLD] {key} ã®é–¾å€¤ã‚’ä½¿ç”¨: {threshold}")
            return threshold
    
    # 3. ç«¶é¦¬å ´
    if track_code and track_code in thresholds.get('by_track', {}):
        threshold = thresholds['by_track'][track_code]
        print(f"[UPSET-THRESHOLD] track={track_code} ã®é–¾å€¤ã‚’ä½¿ç”¨: {threshold}")
        return threshold
    
    # 4. èŠãƒ€åŒºåˆ†
    if surface and surface in thresholds.get('by_surface', {}):
        threshold = thresholds['by_surface'][surface]
        print(f"[UPSET-THRESHOLD] surface={surface} ã®é–¾å€¤ã‚’ä½¿ç”¨: {threshold}")
        return threshold
    
    # 5. è·é›¢åŒºåˆ†
    if distance_category and distance_category in thresholds.get('by_distance', {}):
        threshold = thresholds['by_distance'][distance_category]
        print(f"[UPSET-THRESHOLD] distance={distance_category} ã®é–¾å€¤ã‚’ä½¿ç”¨: {threshold}")
        return threshold
    
    # 6. ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
    print(f"[UPSET-THRESHOLD] ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé–¾å€¤ã‚’ä½¿ç”¨: {default_threshold}")
    return default_threshold


def _predict_upset_with_surface_separation(
    df: pd.DataFrame,
    turf_model_path: Path,
    dirt_model_path: Path,
    raw_scores: np.ndarray,
    track_code: str,
    surface_type: str,
    max_distance: int
) -> pd.DataFrame:
    """
    èŠ/ãƒ€ãƒ¼ãƒˆåˆ†é›¢ãƒ¢ãƒ‡ãƒ«ã§ç©´é¦¬äºˆæ¸¬ã‚’å®Ÿè¡Œ
    
    Args:
        df: äºˆæ¸¬å¯¾è±¡ãƒ‡ãƒ¼ã‚¿ï¼ˆsurface_type_for_upsetåˆ—ãŒå¿…è¦ï¼‰
        turf_model_path: èŠãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹
        dirt_model_path: ãƒ€ãƒ¼ãƒˆãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹
        raw_scores: Rankerã®ç”Ÿã‚¹ã‚³ã‚¢
        track_code: ç«¶é¦¬å ´ã‚³ãƒ¼ãƒ‰
        surface_type: è·¯é¢ã‚¿ã‚¤ãƒ—
        max_distance: æœ€å¤§è·é›¢
        
    Returns:
        DataFrame: ç©´é¦¬äºˆæ¸¬çµæœãŒè¿½åŠ ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿
    """
    import pickle
    
    # èŠãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
    with open(turf_model_path, 'rb') as f:
        turf_model_data = pickle.load(f)
    
    # ãƒ€ãƒ¼ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
    with open(dirt_model_path, 'rb') as f:
        dirt_model_data = pickle.load(f)
    
    # ç©´é¦¬äºˆæ¸¬ç”¨ã®ç‰¹å¾´é‡ã‚’æº–å‚™
    df['predicted_rank'] = df['score_rank']
    df['predicted_score'] = raw_scores
    df['popularity_rank'] = df['tansho_ninkijun_numeric']
    df['value_gap'] = df['predicted_rank'] - df['popularity_rank']
    df['keibajo_code_numeric'] = df['keibajo_code'].astype(int)
    
    # èŠãƒ‡ãƒ¼ã‚¿ã¨ ãƒ€ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²
    df_turf = df[df['surface_type_for_upset'] == 'turf'].copy()
    df_dirt = df[df['surface_type_for_upset'] == 'dirt'].copy()
    
    print(f"[UPSET-SURFACE] èŠ: {len(df_turf)}é ­, ãƒ€ãƒ¼ãƒˆ: {len(df_dirt)}é ­")
    
    # èŠã®äºˆæ¸¬
    if len(df_turf) > 0:
        df_turf = _apply_upset_model(df_turf, turf_model_data, 'turf', track_code, surface_type, max_distance)
    
    # ãƒ€ãƒ¼ãƒˆã®äºˆæ¸¬
    if len(df_dirt) > 0:
        df_dirt = _apply_upset_model(df_dirt, dirt_model_data, 'dirt', track_code, surface_type, max_distance)
    
    # çµåˆã—ã¦è¿”ã™
    if len(df_turf) > 0 and len(df_dirt) > 0:
        df_result = pd.concat([df_turf, df_dirt])
    elif len(df_turf) > 0:
        df_result = df_turf
    elif len(df_dirt) > 0:
        df_result = df_dirt
    else:
        df_result = df.copy()
        df_result['upset_probability'] = 0.0
        df_result['is_upset_candidate'] = 0
        df_result['is_actual_upset'] = 0
    
    # å…ƒã®é †åºã«æˆ»ã™
    df_result = df_result.sort_index()
    
    # çµ±è¨ˆæƒ…å ±ã‚’å‡ºåŠ›
    _print_upset_statistics(df_result)
    
    return df_result


def _predict_upset_unified(
    df: pd.DataFrame,
    model_path: Path,
    raw_scores: np.ndarray,
    track_code: str,
    surface_type: str,
    max_distance: int
) -> pd.DataFrame:
    """
    çµ±åˆãƒ¢ãƒ‡ãƒ«ã§ç©´é¦¬äºˆæ¸¬ã‚’å®Ÿè¡Œï¼ˆå¾“æ¥äº’æ›ï¼‰
    """
    import pickle
    
    with open(model_path, 'rb') as f:
        model_data = pickle.load(f)
    
    # ç©´é¦¬äºˆæ¸¬ç”¨ã®ç‰¹å¾´é‡ã‚’æº–å‚™
    df['predicted_rank'] = df['score_rank']
    df['predicted_score'] = raw_scores
    df['popularity_rank'] = df['tansho_ninkijun_numeric']
    df['value_gap'] = df['predicted_rank'] - df['popularity_rank']
    df['keibajo_code_numeric'] = df['keibajo_code'].astype(int)
    
    df = _apply_upset_model(df, model_data, 'unified', track_code, surface_type, max_distance)
    
    _print_upset_statistics(df)
    
    return df


def _apply_upset_model(
    df: pd.DataFrame,
    model_data: dict,
    surface_label: str,
    track_code: str,
    surface_type: str,
    max_distance: int
) -> pd.DataFrame:
    """
    ç©´é¦¬åˆ†é¡ãƒ¢ãƒ‡ãƒ«ã‚’é©ç”¨
    """
    upset_models = model_data['models']
    upset_feature_cols = model_data['feature_cols']
    upset_calibrators = model_data.get('calibrators', [None] * len(upset_models))
    has_calibration = model_data.get('has_calibration', False)
    calibration_method = model_data.get('calibration_method', 'platt')
    
    print(f"[UPSET-{surface_label.upper()}] ãƒ¢ãƒ‡ãƒ«æ•°: {len(upset_models)}å€‹, ç‰¹å¾´é‡: {len(upset_feature_cols)}å€‹")
    
    # ç‰¹å¾´é‡ã‚’æŠ½å‡º
    X_upset = df[upset_feature_cols].copy()
    X_upset = X_upset.fillna(0)
    X_upset = X_upset.replace([np.inf, -np.inf], 0)
    
    # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬
    upset_proba_list = []
    for i, upset_model in enumerate(upset_models):
        if hasattr(upset_model, 'predict_proba'):
            proba = upset_model.predict_proba(X_upset, num_iteration=upset_model.best_iteration)[:, 1]
        else:
            proba = upset_model.predict(X_upset, num_iteration=upset_model.best_iteration)
        
        # ç¢ºç‡æ ¡æ­£ã‚’é©ç”¨
        if has_calibration and upset_calibrators[i] is not None:
            if calibration_method == 'platt':
                proba = upset_calibrators[i].predict_proba(proba.reshape(-1, 1))[:, 1]
            else:
                proba = upset_calibrators[i].predict(proba)
        
        upset_proba_list.append(proba)
    
    df['upset_probability'] = np.mean(upset_proba_list, axis=0)
    
    # ç©´é¦¬å€™è£œåˆ¤å®š
    distance_category = 'short' if max_distance <= 1800 else 'long'
    upset_threshold = load_upset_threshold(
        track_code=track_code,
        surface=surface_type.lower() if surface_type else None,
        distance_category=distance_category
    )
    df['is_upset_candidate'] = (df['upset_probability'] > upset_threshold).astype(int)
    
    # å®Ÿéš›ã®ç©´é¦¬åˆ¤å®š
    df['is_actual_upset'] = (
        (df['tansho_ninkijun_numeric'] >= 7) & 
        (df['tansho_ninkijun_numeric'] <= 12) & 
        (df['actual_chakujun'].isin([1, 2, 3]))
    ).astype(int)
    
    return df


def _print_upset_statistics(df: pd.DataFrame):
    """
    ç©´é¦¬äºˆæ¸¬ã®çµ±è¨ˆæƒ…å ±ã‚’å‡ºåŠ›
    """
    print(f"[UPSET] ç©´é¦¬å€™è£œæ•°: {df['is_upset_candidate'].sum()}é ­")
    print(f"[UPSET] å®Ÿéš›ã®ç©´é¦¬æ•°: {df['is_actual_upset'].sum()}é ­")
    
    upset_candidates = df[df['is_upset_candidate'] == 1]
    if len(upset_candidates) > 0:
        upset_hits = upset_candidates[upset_candidates['is_actual_upset'] == 1]
        upset_precision = len(upset_hits) / len(upset_candidates) * 100
        
        print(f"[UPSET-DEBUG] å€™è£œè©³ç´°:")
        print(f"  å€™è£œæ•°: {len(upset_candidates)}é ­")
        print(f"  å€™è£œã®äººæ°—ç¯„å›²: {upset_candidates['tansho_ninkijun_numeric'].min():.0f}ã€œ{upset_candidates['tansho_ninkijun_numeric'].max():.0f}ç•ªäººæ°—")
        print(f"  å€™è£œã®ç¢ºç‡ç¯„å›²: {upset_candidates['upset_probability'].min():.4f}ã€œ{upset_candidates['upset_probability'].max():.4f}")
        print(f"  çš„ä¸­æ•°: {len(upset_hits)}é ­")
        
        # ROIè¨ˆç®—
        total_bet = len(upset_candidates) * 100
        total_return = (upset_hits['tansho_odds'] * 100).sum()
        upset_roi = (total_return / total_bet) * 100 if total_bet > 0 else 0
        
        print(f"[UPSET] é©åˆç‡: {upset_precision:.2f}%")
        print(f"[UPSET] ROI: {upset_roi:.1f}%")
    else:
        print("[UPSET] ç©´é¦¬å€™è£œãªã—")


def add_purchase_logic(
    output_df: pd.DataFrame,
    prediction_rank_max: int = 3,
    popularity_rank_max: int = 3,
    min_odds: float = 1.5,
    max_odds: float = 20.0,
    min_score_diff: float = 0.05,
    initial_bankroll: float = 1000000,
    bet_unit: int = 1000
) -> pd.DataFrame:
    """
    äºˆæ¸¬çµæœã«è³¼å…¥åˆ¤æ–­ãƒ»è³¼å…¥é¡ã‚’è¿½åŠ  (æ–°æˆ¦ç•¥: æœ¬å‘½Ã—äºˆæ¸¬ä¸Šä½ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼)
    
    Phase 1æ–°æˆ¦ç•¥:
    - äºˆæ¸¬é †ä½1-3ä½ AND äººæ°—é †1-3ä½ ã®ã¿å¯¾è±¡
    - ã‚ªãƒƒã‚ºç¯„å›²ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° (1.5å€ï½20å€)
    - äºˆæ¸¬ã‚¹ã‚³ã‚¢å·®ãŒä¸€å®šä»¥ä¸Šã®ãƒ¬ãƒ¼ã‚¹ã®ã¿ (æœ¬å‘½ãŒæ˜ç¢º)
    - ä¸€å¾‹ãƒ™ãƒƒãƒˆ (ã‚·ãƒ³ãƒ—ãƒ«&ç¢ºå®Ÿ)
    
    Args:
        output_df (DataFrame): äºˆæ¸¬çµæœãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        prediction_rank_max (int): äºˆæ¸¬é †ä½ã®ä¸Šé™ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 3)
        popularity_rank_max (int): äººæ°—é †ã®ä¸Šé™ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 3)
        min_odds (float): æœ€ä½ã‚ªãƒƒã‚º (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1.5å€)
        max_odds (float): æœ€é«˜ã‚ªãƒƒã‚º (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 20å€)
        min_score_diff (float): äºˆæ¸¬1ä½ã¨2ä½ã®ã‚¹ã‚³ã‚¢å·®ã®æœ€å°å€¤ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.05)
        initial_bankroll (float): åˆæœŸè³‡é‡‘ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 100ä¸‡å††)
        bet_unit (int): 1é ­ã‚ãŸã‚Šã®è³¼å…¥é¡ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1000å††)
        
    Returns:
        DataFrame: è³¼å…¥ãƒ­ã‚¸ãƒƒã‚¯ãŒè¿½åŠ ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
    """
    df = output_df.copy()
    
    # ã‚«ãƒ©ãƒ åã‚’ãƒãƒƒãƒ”ãƒ³ã‚° (æ—¥æœ¬èª â†’ è‹±èª)
    df_work = df.rename(columns={
        'é–‹å‚¬å¹´': 'kaisai_year',
        'é–‹å‚¬æ—¥': 'kaisai_date',
        'ç«¶é¦¬å ´': 'keibajo_code',
        'ãƒ¬ãƒ¼ã‚¹ç•ªå·': 'race_number',
        'é¦¬ç•ª': 'umaban_numeric',
        'äºˆæ¸¬é †ä½': 'predicted_rank',
        'äºˆæ¸¬ã‚¹ã‚³ã‚¢': 'predicted_score',
        'äººæ°—é †': 'popularity_rank',
        'å˜å‹ã‚ªãƒƒã‚º': 'tansho_odds',
        'ç¢ºå®šç€é †': 'chakujun_numeric'
    })
    
    # ãƒ¬ãƒ¼ã‚¹ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ã¦å‡¦ç†
    race_groups = df_work.groupby(['kaisai_year', 'kaisai_date', 'keibajo_code', 'race_number'])
    
    all_races = []
    current_bankroll = initial_bankroll
    total_purchased = 0
    total_wins = 0
    
    for race_id, race_df in race_groups:
        race_df = race_df.copy()
        
        # äºˆæ¸¬ã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆ(é™é †)
        race_df_sorted = race_df.sort_values('predicted_score', ascending=False).reset_index(drop=True)
        
        # äºˆæ¸¬1ä½ã¨2ä½ã®ã‚¹ã‚³ã‚¢å·®ã‚’è¨ˆç®—
        if len(race_df_sorted) >= 2:
            score_diff = race_df_sorted.iloc[0]['predicted_score'] - race_df_sorted.iloc[1]['predicted_score']
        else:
            score_diff = 0
        
        # å…¨é¦¬ã«ãƒ¬ãƒ¼ã‚¹æƒ…å ±ã‚’è¿½åŠ 
        race_df['score_diff'] = score_diff
        race_df['skip_reason'] = None
        
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼1: äºˆæ¸¬ã‚¹ã‚³ã‚¢å·®ãŒå°ã•ã„ãƒ¬ãƒ¼ã‚¹ã¯ã‚¹ã‚­ãƒƒãƒ—
        if score_diff < min_score_diff:
            race_df['è³¼å…¥æ¨å¥¨'] = False
            race_df['è³¼å…¥é¡'] = 0
            race_df['skip_reason'] = 'low_score_diff'
            all_races.append(race_df)
            continue
        
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼2: äºˆæ¸¬é †ä½ AND äººæ°—é † AND ã‚ªãƒƒã‚ºç¯„å›²
        race_df['è³¼å…¥æ¨å¥¨'] = (
            (race_df['predicted_rank'] <= prediction_rank_max) &
            (race_df['popularity_rank'] <= popularity_rank_max) &
            (race_df['tansho_odds'] >= min_odds) &
            (race_df['tansho_odds'] <= max_odds)
        )
        
        # ã‚¹ã‚­ãƒƒãƒ—ç†ç”±ã‚’è¨˜éŒ²ï¼ˆå„ªå…ˆé †ä½é †ã«åˆ¤å®šï¼‰
        race_df.loc[~race_df['è³¼å…¥æ¨å¥¨'] & (race_df['predicted_rank'] > prediction_rank_max), 'skip_reason'] = 'low_predicted_rank'
        race_df.loc[~race_df['è³¼å…¥æ¨å¥¨'] & (race_df['popularity_rank'] > popularity_rank_max), 'skip_reason'] = 'low_popularity'
        race_df.loc[~race_df['è³¼å…¥æ¨å¥¨'] & (race_df['tansho_odds'] < min_odds), 'skip_reason'] = 'odds_too_low'
        race_df.loc[~race_df['è³¼å…¥æ¨å¥¨'] & (race_df['tansho_odds'] > max_odds), 'skip_reason'] = 'odds_too_high'
        
        # è³¼å…¥æ¨å¥¨ãŒFalseã§skip_reasonãŒã¾ã Noneã®å ´åˆã¯ã€Œè¤‡åˆæ¡ä»¶ã€ã¨ã—ã¦è¨˜éŒ²
        race_df.loc[~race_df['è³¼å…¥æ¨å¥¨'] & race_df['skip_reason'].isna(), 'skip_reason'] = 'multiple_conditions'
        
        # è³¼å…¥æ¨å¥¨é¦¬ã‚’æŠ½å‡º
        buy_horses = race_df[race_df['è³¼å…¥æ¨å¥¨']].copy()
        
        # è³¼å…¥é¡åˆ—ã‚’åˆæœŸåŒ–
        race_df['è³¼å…¥é¡'] = 0
        
        if len(buy_horses) > 0:
            # ä¸€å¾‹ãƒ™ãƒƒãƒˆ
            total_purchased += len(buy_horses)
            
            # è³‡é‡‘ã‚’æ›´æ–°
            total_bet = bet_unit * len(buy_horses)
            total_return = 0
            
            for idx in buy_horses.index:
                race_df.loc[idx, 'è³¼å…¥é¡'] = bet_unit
                if race_df.loc[idx, 'chakujun_numeric'] == 1:
                    total_return += bet_unit * race_df.loc[idx, 'tansho_odds']
                    total_wins += 1
            
            current_bankroll = current_bankroll - total_bet + total_return
        
        # ç¾åœ¨ã®è³‡é‡‘æ®‹é«˜ã‚’è¨˜éŒ²
        race_df['ç¾åœ¨è³‡é‡‘'] = current_bankroll
        
        all_races.append(race_df)
    
    # å…¨ãƒ¬ãƒ¼ã‚¹ã‚’çµ±åˆ
    df_integrated = pd.concat(all_races, ignore_index=True)
    
    # ã‚«ãƒ©ãƒ åã‚’æ—¥æœ¬èªã«æˆ»ã™(è‹±èªã‹ã‚‰æ—¥æœ¬èªã¸)
    df_integrated = df_integrated.rename(columns={
        'kaisai_year': 'é–‹å‚¬å¹´',
        'kaisai_date': 'é–‹å‚¬æ—¥',
        'keibajo_code': 'ç«¶é¦¬å ´',
        'race_number': 'ãƒ¬ãƒ¼ã‚¹ç•ªå·',
        'umaban_numeric': 'é¦¬ç•ª',
        'predicted_rank': 'äºˆæ¸¬é †ä½',
        'predicted_score': 'äºˆæ¸¬ã‚¹ã‚³ã‚¢',
        'popularity_rank': 'äººæ°—é †',
        'tansho_odds': 'å˜å‹ã‚ªãƒƒã‚º',
        'chakujun_numeric': 'ç¢ºå®šç€é †',
        'score_diff': 'ã‚¹ã‚³ã‚¢å·®',
        'skip_reason': 'ã‚¹ã‚­ãƒƒãƒ—ç†ç”±'
    })
    
    return df_integrated


def save_results_with_append(df, filename, append_mode=True, output_dir='results'):
    """
    çµæœã‚’TSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ï¼ˆè¿½è¨˜ãƒ¢ãƒ¼ãƒ‰å¯¾å¿œï¼‰
    é€šå¸¸ãƒ¬ãƒ¼ã‚¹ã¨ã‚¹ã‚­ãƒƒãƒ—ãƒ¬ãƒ¼ã‚¹ã‚’åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã«åˆ†ã‘ã¦ä¿å­˜
    
    Args:
        df (DataFrame): ä¿å­˜ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        filename (str): ä¿å­˜å…ˆãƒ•ã‚¡ã‚¤ãƒ«å
        append_mode (bool): True=è¿½è¨˜ãƒ¢ãƒ¼ãƒ‰ã€False=ä¸Šæ›¸ããƒ¢ãƒ¼ãƒ‰
        output_dir (str): å‡ºåŠ›å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 'results'ï¼‰
    """
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆï¼ˆå­˜åœ¨ã—ãªã„å ´åˆï¼‰
    output_path = Path(output_dir)
    output_path.mkdir(exist_ok=True)
    
    # skip_reasonåˆ—ãŒå­˜åœ¨ã™ã‚‹å ´åˆã€ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²
    if 'skip_reason' in df.columns or 'ã‚¹ã‚­ãƒƒãƒ—ç†ç”±' in df.columns:
        skip_col = 'skip_reason' if 'skip_reason' in df.columns else 'ã‚¹ã‚­ãƒƒãƒ—ç†ç”±'
        
        # ãƒ¬ãƒ¼ã‚¹å˜ä½ã§åˆ†æç”¨åˆ—ã®æœ‰ç„¡ã‚’åˆ¤å®šï¼ˆãƒ¬ãƒ¼ã‚¹å†…ã®æœ€åˆã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã§ãƒã‚§ãƒƒã‚¯ï¼‰
        # ãƒ¬ãƒ¼ã‚¹IDã‚’ç‰¹å®šã™ã‚‹åˆ—ï¼ˆç«¶é¦¬å ´ã€é–‹å‚¬å¹´ã€é–‹å‚¬æ—¥ã€ãƒ¬ãƒ¼ã‚¹ç•ªå·ï¼‰
        race_id_cols = []
        for col in ['ç«¶é¦¬å ´', 'keibajo_code', 'é–‹å‚¬å¹´', 'kaisai_year', 'é–‹å‚¬æ—¥', 'kaisai_date', 'ãƒ¬ãƒ¼ã‚¹ç•ªå·', 'race_number']:
            if col in df.columns:
                race_id_cols.append(col)
        
        if len(race_id_cols) >= 4:  # æœ€ä½4åˆ—ï¼ˆç«¶é¦¬å ´ã€å¹´ã€æ—¥ã€ãƒ¬ãƒ¼ã‚¹ç•ªå·ï¼‰å¿…è¦
            # å„ãƒ¬ãƒ¼ã‚¹ã®æœ€åˆã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã§skip_reasonã®æœ‰ç„¡ã‚’ãƒã‚§ãƒƒã‚¯
            race_groups = df.groupby(race_id_cols[:4])
            skipped_races = []
            normal_races = []
            
            for race_key, race_df in race_groups:
                # ãƒ¬ãƒ¼ã‚¹å†…ã®ã„ãšã‚Œã‹ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã«skip_reasonãŒã‚ã‚Œã°ã‚¹ã‚­ãƒƒãƒ—ãƒ¬ãƒ¼ã‚¹
                if race_df[skip_col].notna().any():
                    skipped_races.append(race_df)
                else:
                    normal_races.append(race_df)
            
            # ã‚¹ã‚­ãƒƒãƒ—ãƒ¬ãƒ¼ã‚¹ï¼ˆåˆ†æç”¨åˆ—ã‚’å«ã‚€ï¼‰
            if len(skipped_races) > 0:
                df_skipped = pd.concat(skipped_races, ignore_index=True)
            else:
                df_skipped = pd.DataFrame()
            
            # é€šå¸¸ãƒ¬ãƒ¼ã‚¹ï¼ˆåˆ†æç”¨åˆ—ã‚’å‰Šé™¤ï¼‰
            if len(normal_races) > 0:
                df_normal = pd.concat(normal_races, ignore_index=True)
                cols_to_drop = []
                for col in ['score_diff', 'ã‚¹ã‚³ã‚¢å·®', 'skip_reason', 'ã‚¹ã‚­ãƒƒãƒ—ç†ç”±', 'è³¼å…¥æ¨å¥¨', 'è³¼å…¥é¡', 'ç¾åœ¨è³‡é‡‘']:
                    if col in df_normal.columns:
                        cols_to_drop.append(col)
                df_normal_clean = df_normal.drop(columns=cols_to_drop)
            else:
                df_normal_clean = pd.DataFrame()
        else:
            # ãƒ¬ãƒ¼ã‚¹IDãŒç‰¹å®šã§ããªã„å ´åˆã¯å¾“æ¥ã®æ–¹æ³•ï¼ˆãƒ¬ã‚³ãƒ¼ãƒ‰å˜ä½ï¼‰
            df_skipped = df[df[skip_col].notna()].copy()
            df_normal = df[df[skip_col].isna()].copy()
            cols_to_drop = []
            for col in ['score_diff', 'ã‚¹ã‚³ã‚¢å·®', 'skip_reason', 'ã‚¹ã‚­ãƒƒãƒ—ç†ç”±', 'è³¼å…¥æ¨å¥¨', 'è³¼å…¥é¡', 'ç¾åœ¨è³‡é‡‘']:
                if col in df_normal.columns:
                    cols_to_drop.append(col)
            df_normal_clean = df_normal.drop(columns=cols_to_drop)
        
        # é€šå¸¸ãƒ¬ãƒ¼ã‚¹ç”¨ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆåˆ†æç”¨åˆ—ãªã—ï¼‰
        if len(df_normal_clean) > 0:
            filepath_normal = output_path / filename
            if append_mode and filepath_normal.exists():
                print(f"[NOTE] æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆé€šå¸¸ãƒ¬ãƒ¼ã‚¹ï¼‰ã«è¿½è¨˜: {filepath_normal}")
                df_normal_clean.to_csv(filepath_normal, mode='a', header=False, index=False, sep='\t', encoding='utf-8-sig', float_format='%.8f')
            else:
                print(f"[LIST] æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆï¼ˆé€šå¸¸ãƒ¬ãƒ¼ã‚¹ï¼‰: {filepath_normal}")
                df_normal_clean.to_csv(filepath_normal, index=False, sep='\t', encoding='utf-8-sig', float_format='%.8f')
        
        # ã‚¹ã‚­ãƒƒãƒ—ãƒ¬ãƒ¼ã‚¹ç”¨ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ_skippedã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹ï¼‰
        if len(df_skipped) > 0:
            skipped_filename = filename.replace('.tsv', '_skipped.tsv')
            filepath_skipped = output_path / skipped_filename
            if append_mode and filepath_skipped.exists():
                print(f"[NOTE] æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆã‚¹ã‚­ãƒƒãƒ—ãƒ¬ãƒ¼ã‚¹ï¼‰ã«è¿½è¨˜: {filepath_skipped}")
                df_skipped.to_csv(filepath_skipped, mode='a', header=False, index=False, sep='\t', encoding='utf-8-sig', float_format='%.8f')
            else:
                print(f"[LIST] æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆï¼ˆã‚¹ã‚­ãƒƒãƒ—ãƒ¬ãƒ¼ã‚¹ï¼‰: {filepath_skipped}")
                df_skipped.to_csv(filepath_skipped, index=False, sep='\t', encoding='utf-8-sig', float_format='%.8f')
        
        # å…¨ãƒ¬ãƒ¼ã‚¹çµ±åˆãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆé€šå¸¸+ã‚¹ã‚­ãƒƒãƒ—ã€åˆ†æç”¨åˆ—ãªã—ï¼‰
        if len(df_normal_clean) > 0 or len(df_skipped) > 0:
            # ã‚¹ã‚­ãƒƒãƒ—ãƒ¬ãƒ¼ã‚¹ã‹ã‚‰ã‚‚åˆ†æç”¨åˆ—ã‚’å‰Šé™¤
            df_skipped_clean = df_skipped.copy()
            cols_to_drop = []
            for col in ['score_diff', 'ã‚¹ã‚³ã‚¢å·®', 'skip_reason', 'ã‚¹ã‚­ãƒƒãƒ—ç†ç”±', 'è³¼å…¥æ¨å¥¨', 'è³¼å…¥é¡', 'ç¾åœ¨è³‡é‡‘']:
                if col in df_skipped_clean.columns:
                    cols_to_drop.append(col)
            if len(cols_to_drop) > 0:
                df_skipped_clean = df_skipped_clean.drop(columns=cols_to_drop)
            
            # é€šå¸¸ãƒ¬ãƒ¼ã‚¹ã¨ã‚¹ã‚­ãƒƒãƒ—ãƒ¬ãƒ¼ã‚¹ã‚’çµåˆ
            all_races_list = []
            if len(df_normal_clean) > 0:
                all_races_list.append(df_normal_clean)
            if len(df_skipped_clean) > 0:
                all_races_list.append(df_skipped_clean)
            
            df_all = pd.concat(all_races_list, ignore_index=True)
            
            # å…¨ãƒ¬ãƒ¼ã‚¹çµ±åˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ï¼ˆ_allã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹ï¼‰
            all_filename = filename.replace('.tsv', '_all.tsv')
            filepath_all = output_path / all_filename
            if append_mode and filepath_all.exists():
                print(f"[NOTE] æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆå…¨ãƒ¬ãƒ¼ã‚¹çµ±åˆï¼‰ã«è¿½è¨˜: {filepath_all}")
                df_all.to_csv(filepath_all, mode='a', header=False, index=False, sep='\t', encoding='utf-8-sig', float_format='%.8f')
            else:
                print(f"[LIST] æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆï¼ˆå…¨ãƒ¬ãƒ¼ã‚¹çµ±åˆï¼‰: {filepath_all}")
                df_all.to_csv(filepath_all, index=False, sep='\t', encoding='utf-8-sig', float_format='%.8f')
    else:
        # skip_reasonåˆ—ãŒãªã„å ´åˆã¯å¾“æ¥é€šã‚Š
        filepath = output_path / filename
        if append_mode and filepath.exists():
            print(f"[NOTE] æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã«è¿½è¨˜: {filepath}")
            df.to_csv(filepath, mode='a', header=False, index=False, sep='\t', encoding='utf-8-sig', float_format='%.8f')
        else:
            print(f"[LIST] æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ: {filepath}")
            df.to_csv(filepath, index=False, sep='\t', encoding='utf-8-sig', float_format='%.8f')


def predict_with_model(model_filename, track_code, kyoso_shubetsu_code, surface_type, 
                      min_distance, max_distance, test_year_start=2023, test_year_end=2023,
                      upset_classifier_path=None):
    """
    æŒ‡å®šã—ãŸãƒ¢ãƒ‡ãƒ«ã§äºˆæ¸¬ã‚’å®Ÿè¡Œã™ã‚‹æ±ç”¨é–¢æ•°
    
    Args:
        model_filename (str): ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«å
        track_code (str): ç«¶é¦¬å ´ã‚³ãƒ¼ãƒ‰
        kyoso_shubetsu_code (str): ç«¶äº‰ç¨®åˆ¥ã‚³ãƒ¼ãƒ‰
        surface_type (str): 'turf' or 'dirt'
        min_distance (int): æœ€å°è·é›¢
        max_distance (int): æœ€å¤§è·é›¢
        test_year_start (int): ãƒ†ã‚¹ãƒˆå¯¾è±¡é–‹å§‹å¹´ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 2023)
        test_year_end (int): ãƒ†ã‚¹ãƒˆå¯¾è±¡çµ‚äº†å¹´ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 2023)
        upset_classifier_path (str): ç©´é¦¬åˆ†é¡å™¨ã®ãƒ‘ã‚¹ï¼ˆNoneã®å ´åˆã¯è‡ªå‹•æ¤œç´¢ï¼‰
        
    Returns:
        tuple: (äºˆæ¸¬çµæœDataFrame, ã‚µãƒãƒªãƒ¼DataFrame, ãƒ¬ãƒ¼ã‚¹æ•°)
    """
    
    # PostgreSQL ã‚³ãƒã‚¯ã‚·ãƒ§ãƒ³ã®ä½œæˆ
    conn = psycopg2.connect(
        host='localhost',
        port='5432',
        user='postgres',
        password='ahtaht88',
        dbname='keiba'
    )
    
    # SQLã‚¯ã‚¨ãƒªã‚’å…±é€šåŒ–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã§ç”Ÿæˆ
    # æ³¨æ„: universal_test.pyã§ã¯æ‰•ã„æˆ»ã—æƒ…å ±ãŒå¿…è¦ãªã®ã§include_payout=True
    # ã¾ãŸã€year_start/year_endã®ç¯„å›²ã‚’åºƒã’ã¦éå»3å¹´åˆ†ã‚‚å–å¾—ï¼ˆpast_avg_sotai_chakujunè¨ˆç®—ã®ãŸã‚ï¼‰
    # filter_year_start/filter_year_endã§ãƒ†ã‚¹ãƒˆå¹´ã®ã¿ã«çµã‚Šè¾¼ã¿
    sql = build_race_data_query(
        track_code=track_code,
        year_start=test_year_start - 3,  # éå»3å¹´åˆ†ã‚‚å–å¾—
        year_end=test_year_end,
        surface_type=surface_type.lower(),
        distance_min=min_distance,
        distance_max=max_distance,
        kyoso_shubetsu_code=kyoso_shubetsu_code,
        include_payout=True,  # universal_test.pyã§ã¯æ‰•ã„æˆ»ã—æƒ…å ±ãŒå¿…è¦
        filter_year_start=test_year_start,  # ãƒ†ã‚¹ãƒˆå¹´é–‹å§‹
        filter_year_end=test_year_end  # ãƒ†ã‚¹ãƒˆå¹´çµ‚äº†
    )
    
    # ãƒ†ã‚¹ãƒˆç”¨ã®SQLã‚’ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›ï¼ˆå¸¸ã«ä¸Šæ›¸ãï¼‰
    log_filepath = Path('sql_log_test.txt')
    with open(log_filepath, 'w', encoding='utf-8') as f:
        f.write(f"=== ãƒ†ã‚¹ãƒˆç”¨SQL ===\n")
        f.write(f"ãƒ¢ãƒ‡ãƒ«: {model_filename}\n")
        f.write(f"ãƒ†ã‚¹ãƒˆæœŸé–“: {test_year_start}å¹´ã€œ{test_year_end}å¹´\n")
        f.write(f"å®Ÿè¡Œæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"\n{sql}\n")
    print(f"[NOTE] ãƒ†ã‚¹ãƒˆç”¨SQLã‚’ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›: {log_filepath}")

    # ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    df = pd.read_sql_query(sql=sql, con=conn)
    conn.close()
    
    if len(df) == 0:
        print(f"[ERROR] {model_filename} ã«å¯¾å¿œã™ã‚‹ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return None, None, 0

    print(f"[+] ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(df)}ä»¶")

    # ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ï¼ˆå…±é€šåŒ–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ä½¿ç”¨ï¼‰
    from data_preprocessing import preprocess_race_data
    df = preprocess_race_data(df, verbose=True)

    # ç‰¹å¾´é‡ä½œæˆï¼ˆå…±é€šåŒ–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ä½¿ç”¨ï¼‰
    from feature_engineering import create_features, add_advanced_features
    
    # åŸºæœ¬ç‰¹å¾´é‡ã‚’ä½œæˆ
    X = create_features(df)
    
    # é«˜åº¦ãªç‰¹å¾´é‡ã‚’è¿½åŠ ï¼ˆfeature_engineering.pyã§å…±é€šåŒ–ï¼‰
    print("[START] é«˜åº¦ãªç‰¹å¾´é‡ç”Ÿæˆ...")
    X = add_advanced_features(
        df=df,
        X=X,
        surface_type=surface_type,
        min_distance=min_distance,
        max_distance=max_distance,
        logger=None,
        inverse_rank=False  # universal_test.pyã§ã¯ç€é †ã‚’åè»¢ã—ãªã„
    )
    print(f"[OK] ç‰¹å¾´é‡ç”Ÿæˆå®Œäº†: {len(X.columns)}å€‹")

    # è·é›¢åˆ¥ç‰¹å¾´é‡é¸æŠã¯add_advanced_features()å†…ã§å®Ÿæ–½æ¸ˆã¿
    print(f"\n[INFO] ç‰¹å¾´é‡ãƒªã‚¹ãƒˆ: {list(X.columns)}")

    # Phase 2.5: å±•é–‹è¦å› ç‰¹å¾´é‡ã‚’è¿½åŠ ï¼ˆç©´é¦¬äºˆæ¸¬ç”¨ï¼‰
    from feature_engineering import add_upset_features
    df = add_upset_features(df)
    print("[UPSET] å±•é–‹è¦å› ç‰¹å¾´é‡ã‚’è¿½åŠ ã—ã¾ã—ãŸ")

    # ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
    try:
        with open(model_filename, 'rb') as model_file:
            model = pickle.load(model_file)
    except FileNotFoundError:
        print(f"[ERROR] ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ« {model_filename} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return None, None, 0

    # ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¢æ•°ã‚’å®šç¾©
    def sigmoid(x):
        """å€¤ã‚’0-1ã®ç¯„å›²ã«åã‚ã‚‹ã‚ˆï½"""
        return 1 / (1 + np.exp(-x))

    # äºˆæ¸¬ã‚’å®Ÿè¡Œã—ã¦ã€ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¢æ•°ã§å¤‰æ›
    raw_scores = model.predict(X)
    df['predicted_chakujun_score'] = sigmoid(raw_scores)

    # ãƒ‡ãƒ¼ã‚¿ã‚’ã‚½ãƒ¼ãƒˆ
    df = df.sort_values(by=['kaisai_nen', 'kaisai_tsukihi', 'race_bango', 'umaban'], ascending=True)

    # ã‚°ãƒ«ãƒ¼ãƒ—å†…ã§ã®ã‚¹ã‚³ã‚¢é †ä½ã‚’è¨ˆç®—
    df['score_rank'] = df.groupby(['kaisai_nen', 'kaisai_tsukihi', 'race_bango'])['predicted_chakujun_score'].rank(method='min', ascending=False)

    # kakutei_chakujun_numericã‚’å…ƒã®ç€é †ï¼ˆ1=1ç€ï¼‰ã«æˆ»ã™
    # db_query_builder.pyã§ã€Œ18 - ç€é † + 1ã€ã§åè»¢ã•ã‚Œã¦ã‚‹ã®ã§ã€å…ƒã«æˆ»ã™
    df['actual_chakujun'] = 19 - df['kakutei_chakujun_numeric']
    
    # kakutei_chakujun_numeric ã¨ score_rank ã‚’æ•´æ•°ã«å¤‰æ›
    df['kakutei_chakujun_numeric'] = df['kakutei_chakujun_numeric'].fillna(0).astype(int)
    df['actual_chakujun'] = df['actual_chakujun'].fillna(0).astype(int)
    df['tansho_ninkijun_numeric'] = df['tansho_ninkijun_numeric'].fillna(0).astype(int)
    df['score_rank'] = df['score_rank'].fillna(0).astype(int)

    # Phase 2.5: ç©´é¦¬äºˆæ¸¬ã‚’å®Ÿè¡Œï¼ˆèŠ/ãƒ€ãƒ¼ãƒˆåˆ†é›¢å¯¾å¿œï¼‰
    print("\n[UPSET] ç©´é¦¬åˆ†é¡ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
    
    # èŠ/ãƒ€ãƒ¼ãƒˆåˆ†é›¢å¯¾å¿œ: track_codeã‹ã‚‰èŠ/ãƒ€ãƒ¼ãƒˆã‚’åˆ¤å®š
    from keiba_constants import get_surface_type_from_track_cd
    df['surface_type_for_upset'] = df['track_code'].apply(get_surface_type_from_track_cd)
    
    # ç©´é¦¬åˆ†é¡å™¨ã®ãƒ‘ã‚¹ã‚’è§£æï¼ˆèŠ/ãƒ€ãƒ¼ãƒˆåˆ†é›¢ç‰ˆã‹çµ±åˆç‰ˆã‹ï¼‰
    upset_model_path_turf = None
    upset_model_path_dirt = None
    upset_model_path_unified = None
    is_surface_separated = False
    
    if upset_classifier_path:
        # ãƒ‘ã‚¤ãƒ—åŒºåˆ‡ã‚Šã§èŠ/ãƒ€ãƒ¼ãƒˆåˆ†é›¢ç‰ˆã‚’æ¤œå‡º
        if '|' in upset_classifier_path:
            paths = upset_classifier_path.split('|')
            for p in paths:
                p_path = Path(p)
                if p_path.exists():
                    if 'turf' in p_path.name:
                        upset_model_path_turf = p_path
                    elif 'dirt' in p_path.name:
                        upset_model_path_dirt = p_path
            if upset_model_path_turf and upset_model_path_dirt:
                is_surface_separated = True
                print(f"[UPSET] ğŸ¯ èŠ/ãƒ€ãƒ¼ãƒˆåˆ†é›¢ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨")
                print(f"  èŠ: {upset_model_path_turf.name}")
                print(f"  ãƒ€ãƒ¼ãƒˆ: {upset_model_path_dirt.name}")
        else:
            # çµ±åˆç‰ˆ
            upset_model_path_unified = Path(upset_classifier_path)
            if upset_model_path_unified.exists():
                print(f"[UPSET] çµ±åˆãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨: {upset_model_path_unified.name}")
            else:
                print(f"[UPSET] æŒ‡å®šãƒ‘ã‚¹ã«ç©´é¦¬åˆ†é¡å™¨ãªã—: {upset_model_path_unified}")
                upset_model_path_unified = None
    
    # ãƒ‘ã‚¹æŒ‡å®šãŒãªã„å ´åˆã¯è‡ªå‹•æ¤œç´¢
    if not is_surface_separated and upset_model_path_unified is None:
        # Rankerãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰å­¦ç¿’æœŸé–“ã‚’æŠ½å‡º
        model_filename_stem = Path(model_filename).stem
        train_period = None
        
        import re
        period_match = re.search(r'(\d{4})-(\d{4})', model_filename_stem)
        if period_match:
            train_period = f"{period_match.group(1)}-{period_match.group(2)}"
            print(f"[UPSET] Rankerã®å­¦ç¿’æœŸé–“æ¤œå‡º: {train_period}")
        
        # èŠ/ãƒ€ãƒ¼ãƒˆåˆ†é›¢ç‰ˆã‚’å„ªå…ˆã—ã¦æ¢ã™
        if train_period:
            turf_path = Path('models') / f'upset_classifier_turf_{train_period}.sav'
            dirt_path = Path('models') / f'upset_classifier_dirt_{train_period}.sav'
            
            if turf_path.exists() and dirt_path.exists():
                upset_model_path_turf = turf_path
                upset_model_path_dirt = dirt_path
                is_surface_separated = True
                print(f"[UPSET] ğŸ¯ èŠ/ãƒ€ãƒ¼ãƒˆåˆ†é›¢ãƒ¢ãƒ‡ãƒ«ã‚’æ¤œå‡º")
            else:
                # çµ±åˆç‰ˆã‚’æ¢ã™
                unified_path = Path('models') / f'upset_classifier_{train_period}.sav'
                if unified_path.exists():
                    upset_model_path_unified = unified_path
                    print(f"[UPSET] æœŸé–“å¯¾å¿œç©´é¦¬åˆ†é¡å™¨ã‚’ä½¿ç”¨: {unified_path.name}")
        
        # æ±ç”¨ç‰ˆã‚’æ¢ã™
        if not is_surface_separated and upset_model_path_unified is None:
            universal_path = Path('models') / 'upset_classifier_universal.sav'
            if universal_path.exists():
                upset_model_path_unified = universal_path
                print(f"[UPSET] æ±ç”¨ç©´é¦¬åˆ†é¡å™¨ã‚’ä½¿ç”¨: {universal_path.name}")
            else:
                print(f"[UPSET] ç©´é¦¬åˆ†é¡å™¨ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆPhase 1ã®ã¿ã§ç¶™ç¶šï¼‰")
    
    # ç©´é¦¬äºˆæ¸¬ã®å®Ÿè¡Œ
    df['upset_probability'] = 0.0
    df['is_upset_candidate'] = 0
    df['is_actual_upset'] = 0
    
    if is_surface_separated:
        # èŠ/ãƒ€ãƒ¼ãƒˆåˆ†é›¢ç‰ˆ: ãã‚Œãã‚Œã®ãƒ¢ãƒ‡ãƒ«ã§äºˆæ¸¬
        df = _predict_upset_with_surface_separation(
            df, 
            upset_model_path_turf, 
            upset_model_path_dirt,
            raw_scores,
            track_code,
            surface_type,
            max_distance
        )
    elif upset_model_path_unified and upset_model_path_unified.exists():
        # çµ±åˆç‰ˆ: å¾“æ¥é€šã‚Š
        df = _predict_upset_unified(
            df,
            upset_model_path_unified,
            raw_scores,
            track_code,
            surface_type,
            max_distance
        )
    else:
        print(f"[WARNING] ç©´é¦¬åˆ†é¡ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    
    # surface_typeåˆ—ã‚’è¿½åŠ ï¼ˆèŠãƒ»ãƒ€ãƒ¼ãƒˆåŒºåˆ†ï¼‰
    from keiba_constants import get_surface_name
    df['surface_type_name'] = get_surface_name(surface_type)

    # å¿…è¦ãªåˆ—ã‚’é¸æŠ
    output_columns = ['keibajo_name',
                      'kaisai_nen', 
                      'kaisai_tsukihi', 
                      'race_bango',
                      'surface_type_name',
                      'kyori',
                      'umaban', 
                      'bamei', 
                      'tansho_odds', 
                      'tansho_ninkijun_numeric', 
                      'actual_chakujun',  # å…ƒã®ç€é †ï¼ˆ1=1ç€ï¼‰
                      'score_rank', 
                      'predicted_chakujun_score',
                      'upset_probability',  # Phase 2.5: ç©´é¦¬ç¢ºç‡
                      'is_upset_candidate',  # Phase 2.5: ç©´é¦¬å€™è£œãƒ•ãƒ©ã‚°
                      'is_actual_upset',  # Phase 2.5: å®Ÿéš›ã®ç©´é¦¬ãƒ•ãƒ©ã‚°
                      'è¤‡å‹1ç€é¦¬ç•ª',
                      'è¤‡å‹1ç€ã‚ªãƒƒã‚º',
                      'è¤‡å‹1ç€äººæ°—',
                      'è¤‡å‹2ç€é¦¬ç•ª',
                      'è¤‡å‹2ç€ã‚ªãƒƒã‚º',
                      'è¤‡å‹2ç€äººæ°—',
                      'è¤‡å‹3ç€é¦¬ç•ª',
                      'è¤‡å‹3ç€ã‚ªãƒƒã‚º',
                      'è¤‡å‹3ç€äººæ°—',
                      'é¦¬é€£é¦¬ç•ª1',
                      'é¦¬é€£é¦¬ç•ª2',
                      'é¦¬é€£ã‚ªãƒƒã‚º',
                      'ãƒ¯ã‚¤ãƒ‰1_2é¦¬ç•ª1',
                      'ãƒ¯ã‚¤ãƒ‰1_2é¦¬ç•ª2',
                      'ãƒ¯ã‚¤ãƒ‰2_3ç€é¦¬ç•ª1',
                      'ãƒ¯ã‚¤ãƒ‰2_3ç€é¦¬ç•ª2',
                      'ãƒ¯ã‚¤ãƒ‰1_3ç€é¦¬ç•ª1',
                      'ãƒ¯ã‚¤ãƒ‰1_3ç€é¦¬ç•ª2',
                      'ãƒ¯ã‚¤ãƒ‰1_2ã‚ªãƒƒã‚º',
                      'ãƒ¯ã‚¤ãƒ‰2_3ã‚ªãƒƒã‚º',
                      'ãƒ¯ã‚¤ãƒ‰1_3ã‚ªãƒƒã‚º',
                      'é¦¬å˜é¦¬ç•ª1',
                      'é¦¬å˜é¦¬ç•ª2',
                      'é¦¬å˜ã‚ªãƒƒã‚º',
                      'ï¼“é€£è¤‡ã‚ªãƒƒã‚º',]
    
    # DEBUG: upsetåˆ—ã®å­˜åœ¨ç¢ºèª
    upset_cols = ['upset_probability', 'is_upset_candidate', 'is_actual_upset']
    missing_cols = [c for c in upset_cols if c not in df.columns]
    if missing_cols:
        print(f"[DEBUG] ä»¥ä¸‹ã®upsetåˆ—ãŒdf.columnsã«å­˜åœ¨ã—ã¾ã›ã‚“: {missing_cols}")
    
    # output_columnså†…ã®å­˜åœ¨ã—ãªã„åˆ—ã‚’ãƒ•ã‚£ãƒ«ã‚¿
    available_output_columns = [c for c in output_columns if c in df.columns]
    if len(available_output_columns) < len(output_columns):
        missing_output = [c for c in output_columns if c not in df.columns]
        print(f"[DEBUG] ä»¥ä¸‹ã®åˆ—ãŒdf.columnsã«å­˜åœ¨ã—ãªã„ãŸã‚é™¤å¤–: {missing_output}")
    
    output_df = df[available_output_columns]

    # åˆ—åã‚’å¤‰æ›´
    output_df = output_df.rename(columns={
        'keibajo_name': 'ç«¶é¦¬å ´',
        'kaisai_nen': 'é–‹å‚¬å¹´',
        'kaisai_tsukihi': 'é–‹å‚¬æ—¥',
        'race_bango': 'ãƒ¬ãƒ¼ã‚¹ç•ªå·',
        'surface_type_name': 'èŠãƒ€åŒºåˆ†',
        'kyori': 'è·é›¢',
        'umaban': 'é¦¬ç•ª',
        'bamei': 'é¦¬å',
        'tansho_odds': 'å˜å‹ã‚ªãƒƒã‚º',
        'tansho_ninkijun_numeric': 'äººæ°—é †',
        'actual_chakujun': 'ç¢ºå®šç€é †',  # å…ƒã®ç€é †ï¼ˆ1=1ç€ï¼‰ã«æˆ»ã—ãŸã‚‚ã®
        'score_rank': 'äºˆæ¸¬é †ä½',
        'predicted_chakujun_score': 'äºˆæ¸¬ã‚¹ã‚³ã‚¢',
        'upset_probability': 'ç©´é¦¬ç¢ºç‡',  # Phase 2.5
        'is_upset_candidate': 'ç©´é¦¬å€™è£œ',  # Phase 2.5
        'is_actual_upset': 'å®Ÿéš›ã®ç©´é¦¬'  # Phase 2.5
    })

    # æ­£ã—ã„ãƒ¬ãƒ¼ã‚¹æ•°ã®è¨ˆç®—æ–¹æ³•ã¯ã“ã‚Œï½ï¼
    race_count = len(output_df.groupby(['é–‹å‚¬å¹´', 'é–‹å‚¬æ—¥', 'ãƒ¬ãƒ¼ã‚¹ç•ªå·']))

    # çš„ä¸­ç‡ãƒ»å›åç‡è¨ˆç®—ï¼ˆå…ƒã®test.pyã‹ã‚‰ç§»æ¤ï¼‰
    # å˜å‹ã®çš„ä¸­ç‡ã¨å›åç‡
    tansho_hit = (output_df['ç¢ºå®šç€é †'] == 1) & (output_df['äºˆæ¸¬é †ä½'] == 1)
    tansho_hitrate = 100 * tansho_hit.sum() / race_count
    tansho_recoveryrate = 100 * (tansho_hit * output_df['å˜å‹ã‚ªãƒƒã‚º']).sum() / race_count

    # è¤‡å‹ã®çš„ä¸­ç‡ã¨å›åç‡
    fukusho_hit = (output_df['ç¢ºå®šç€é †'].isin([1, 2, 3])) & (output_df['äºˆæ¸¬é †ä½'].isin([1, 2, 3]))
    fukusho_hitrate = fukusho_hit.sum() / (race_count * 3) * 100

    # çš„ä¸­é¦¬ã ã‘å–ã‚Šå‡ºã™
    hit_rows = output_df[fukusho_hit].copy()

    def extract_odds(row):
        """é¦¬ç•ªã¨è¤‡å‹Xç€é¦¬ç•ªã‚’ç…§åˆã—ã¦è©²å½“ã™ã‚‹ã‚ªãƒƒã‚ºã‚’å–å¾—"""
        uma_ban = row.get('é¦¬ç•ª', None)
        if uma_ban is None:
            return 0
        for i in [1, 2, 3]:
            col_ban = f'è¤‡å‹{i}ç€é¦¬ç•ª'
            col_odds = f'è¤‡å‹{i}ç€ã‚ªãƒƒã‚º'
            if col_ban in row and col_odds in row:
                if pd.notna(row[col_ban]) and row[col_ban] == uma_ban:
                    if pd.notna(row[col_odds]):
                        return row[col_odds]
        return 0

    # çš„ä¸­é¦¬ã«å¯¾å¿œã™ã‚‹æ‰•æˆ»ã‚’è¨ˆç®—ï¼ˆ100å††è³­ã‘ãŸã¨ã—ã¦ï¼‰
    hit_rows['çš„ä¸­ã‚ªãƒƒã‚º'] = hit_rows.apply(extract_odds, axis=1)
    total_payout = (hit_rows['çš„ä¸­ã‚ªãƒƒã‚º'] * 100).sum()

    # ç·è³¼å…¥é¡ï¼ˆæ¯ãƒ¬ãƒ¼ã‚¹ã§3é ­ã«100å††ãšã¤ï¼‰
    total_bet = race_count * 3 * 100
    fukusho_recoveryrate = total_payout / total_bet * 100

    # é¦¬é€£ã®çš„ä¸­ç‡ã¨å›åç‡
    umaren_hit = output_df.groupby(['é–‹å‚¬å¹´', 'é–‹å‚¬æ—¥', 'ãƒ¬ãƒ¼ã‚¹ç•ªå·']).apply(
        lambda x: set([1, 2]).issubset(set(x.sort_values('äºˆæ¸¬ã‚¹ã‚³ã‚¢', ascending=False).head(2)['ç¢ºå®šç€é †'].values))
    )
    umaren_hitrate = 100 * umaren_hit.sum() / race_count
    umaren_recoveryrate = 100 * (umaren_hit * output_df.groupby(['é–‹å‚¬å¹´', 'é–‹å‚¬æ—¥', 'ãƒ¬ãƒ¼ã‚¹ç•ªå·'])['é¦¬é€£ã‚ªãƒƒã‚º'].first()).sum() / race_count

    # ãƒ¯ã‚¤ãƒ‰çš„ä¸­ç‡ãƒ»å›åç‡ã‚‚è¨ˆç®—ï¼ˆçœç•¥ã—ã¦ç°¡ç•¥åŒ–ï¼‰
    wide_hitrate = 0  # è¨ˆç®—ãŒè¤‡é›‘ãªã®ã§çœç•¥
    wide_recoveryrate = 0

    # é¦¬å˜ã®çš„ä¸­ç‡ã¨å›åç‡
    umatan_hit = output_df.groupby(['é–‹å‚¬å¹´', 'é–‹å‚¬æ—¥', 'ãƒ¬ãƒ¼ã‚¹ç•ªå·']).apply(
        lambda x: list(x.sort_values('äºˆæ¸¬ã‚¹ã‚³ã‚¢', ascending=False).head(2)['ç¢ºå®šç€é †'].values) == [1, 2]
    )
    umatan_hitrate = 100 * umatan_hit.sum() / race_count
    
    umatan_odds_sum = 0
    for name, race_group in output_df.groupby(['é–‹å‚¬å¹´', 'é–‹å‚¬æ—¥', 'ãƒ¬ãƒ¼ã‚¹ç•ªå·']):
        top_horses = race_group.sort_values('äºˆæ¸¬ã‚¹ã‚³ã‚¢', ascending=False).head(2)
        if list(top_horses['ç¢ºå®šç€é †'].values) == [1, 2]:
            umatan_odds_sum += race_group['é¦¬å˜ã‚ªãƒƒã‚º'].iloc[0]

    umatan_recoveryrate = 100 * umatan_odds_sum / race_count

    # ä¸‰é€£è¤‡ã®çš„ä¸­ç‡ã¨å›åç‡
    sanrenpuku_hit = output_df.groupby(['é–‹å‚¬å¹´', 'é–‹å‚¬æ—¥', 'ãƒ¬ãƒ¼ã‚¹ç•ªå·']).apply(
        lambda x: set([1, 2, 3]).issubset(set(x.sort_values('äºˆæ¸¬ã‚¹ã‚³ã‚¢', ascending=False).head(3)['ç¢ºå®šç€é †'].values))
    )
    sanrenpuku_hitrate = 100 * sanrenpuku_hit.sum() / len(sanrenpuku_hit)
    sanrenpuku_recoveryrate = 100 * (sanrenpuku_hit * output_df.groupby(['é–‹å‚¬å¹´', 'é–‹å‚¬æ—¥', 'ãƒ¬ãƒ¼ã‚¹ç•ªå·'])['ï¼“é€£è¤‡ã‚ªãƒƒã‚º'].first()).sum() / len(sanrenpuku_hit)

    # çµæœã‚’ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«ã¾ã¨ã‚ã‚‹
    summary_df = pd.DataFrame({
        'çš„ä¸­æ•°': [tansho_hit.sum(), fukusho_hit.sum(), umaren_hit.sum(), 0, umatan_hit.sum(), sanrenpuku_hit.sum()],
        'çš„ä¸­ç‡(%)': [tansho_hitrate, fukusho_hitrate, umaren_hitrate, wide_hitrate, umatan_hitrate, sanrenpuku_hitrate],
        'å›åç‡(%)': [tansho_recoveryrate, fukusho_recoveryrate, umaren_recoveryrate, wide_recoveryrate, umatan_recoveryrate, sanrenpuku_recoveryrate]
    }, index=['å˜å‹', 'è¤‡å‹', 'é¦¬é€£', 'ãƒ¯ã‚¤ãƒ‰', 'é¦¬å˜', 'ï¼“é€£è¤‡'])

    # Phase 1çµ±åˆ: æœŸå¾…å€¤ãƒ»ã‚±ãƒªãƒ¼åŸºæº–ãƒ»ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã‚’è¿½åŠ 
    print("[PHASE1] æ–°è³¼å…¥ãƒ­ã‚¸ãƒƒã‚¯(æœ¬å‘½Ã—äºˆæ¸¬ä¸Šä½ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼)ã‚’å®Ÿè¡Œä¸­...")
    try:
        output_df_with_logic = add_purchase_logic(
            output_df,
            prediction_rank_max=3,  # äºˆæ¸¬é †ä½1-3ä½
            popularity_rank_max=3,  # äººæ°—é †1-3ä½
            min_odds=1.5,  # æœ€ä½ã‚ªãƒƒã‚º1.5å€
            max_odds=20.0,  # æœ€é«˜ã‚ªãƒƒã‚º20å€
            min_score_diff=0.0,  # äºˆæ¸¬ã‚¹ã‚³ã‚¢å·®ãƒ•ã‚£ãƒ«ã‚¿ç„¡åŠ¹åŒ–ï¼ˆå…¨ãƒ¬ãƒ¼ã‚¹å¯¾è±¡ï¼‰
            initial_bankroll=1000000,
            bet_unit=1000  # ä¸€å¾‹1000å††ãƒ™ãƒƒãƒˆ
        )
        print("[PHASE1] è³¼å…¥ãƒ­ã‚¸ãƒƒã‚¯çµ±åˆå®Œäº†!")
        
        # è³¼å…¥æ¨å¥¨é¦¬ã®çµ±è¨ˆ
        buy_count = output_df_with_logic['è³¼å…¥æ¨å¥¨'].sum()
        total_bet = output_df_with_logic['è³¼å…¥é¡'].sum()
        final_bankroll = output_df_with_logic['ç¾åœ¨è³‡é‡‘'].iloc[-1]
        
        # çš„ä¸­æ•°ã‚’è¨ˆç®—
        purchased = output_df_with_logic[output_df_with_logic['è³¼å…¥é¡'] > 0]
        wins = len(purchased[purchased['ç¢ºå®šç€é †'] == 1])
        hit_rate = (wins / len(purchased) * 100) if len(purchased) > 0 else 0
        
        print(f"[STATS] è³¼å…¥æ¨å¥¨é¦¬æ•°: {buy_count}")
        print(f"[STATS] å®Ÿè³¼å…¥é¦¬æ•°: {len(purchased)}")
        print(f"[STATS] çš„ä¸­æ•°: {wins}")
        print(f"[STATS] çš„ä¸­ç‡: {hit_rate:.2f}%")
        print(f"[STATS] ç·æŠ•è³‡é¡: {total_bet:,.0f}å††")
        print(f"[STATS] æœ€çµ‚è³‡é‡‘: {final_bankroll:,.0f}å†† (åˆæœŸ: 1,000,000å††)")
        print(f"[STATS] æç›Š: {final_bankroll - 1000000:+,.0f}å††")
        
        # å›åç‡ã‚’è¨ˆç®—
        if total_bet > 0:
            recovery_rate = (final_bankroll - 1000000 + total_bet) / total_bet * 100
            print(f"[STATS] å›åç‡: {recovery_rate:.2f}%")
        
        output_df = output_df_with_logic
    except Exception as e:
        print(f"[WARNING] Phase 1çµ±åˆã§ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
        print("[WARNING] å¾“æ¥ã®äºˆæ¸¬çµæœã®ã¿è¿”ã—ã¾ã™")
        import traceback
        traceback.print_exc()

    return output_df, summary_df, race_count


def test_multiple_models(test_year_start=2023, test_year_end=2023, model_type='all'):
    """
    è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä¸€æ‹¬ã§ãƒ†ã‚¹ãƒˆã™ã‚‹
    
    Args:
        test_year_start (int): ãƒ†ã‚¹ãƒˆå¯¾è±¡é–‹å§‹å¹´ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 2023)
        test_year_end (int): ãƒ†ã‚¹ãƒˆå¯¾è±¡çµ‚äº†å¹´ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 2023)
        model_type (str): ãƒ†ã‚¹ãƒˆã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ— ('all', 'standard', 'custom')
    """
    
    # JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’èª­ã¿è¾¼ã¿
    try:
        if model_type == 'custom':
            # customãƒ¢ãƒ‡ãƒ«ã®ã¿èª­ã¿è¾¼ã¿
            import json
            with open('model_configs.json', 'r', encoding='utf-8') as f:
                config_data = json.load(f)
            model_configs = config_data.get('custom_models', [])
            print(f"[INFO] customãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿: {len(model_configs)}å€‹")
        elif model_type == 'standard':
            # standardãƒ¢ãƒ‡ãƒ«ã®ã¿èª­ã¿è¾¼ã¿
            import json
            with open('model_configs.json', 'r', encoding='utf-8') as f:
                config_data = json.load(f)
            model_configs = config_data.get('standard_models', [])
            print(f"[INFO] standardãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿: {len(model_configs)}å€‹")
        else:
            # å…¨ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
            model_configs = get_all_models()
            print(f"[INFO] å…¨ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿: {len(model_configs)}å€‹")
    except Exception as e:
        print(f"[ERROR] è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return
    
    if not model_configs:
        print("[!] ãƒ†ã‚¹ãƒˆå¯¾è±¡ã®ãƒ¢ãƒ‡ãƒ«è¨­å®šãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return
    
    print("[RACE] è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ãƒ†ã‚¹ãƒˆã‚’é–‹å§‹ã—ã¾ã™ï¼")
    print("=" * 60)
    
    all_results = {}
    # çµ±åˆãƒ•ã‚¡ã‚¤ãƒ«ã®åˆå›æ›¸ãè¾¼ã¿ãƒ•ãƒ©ã‚°
    first_unified_write = True
    
    for i, config in enumerate(model_configs, 1):
        base_model_filename = config['model_filename']
        description = config.get('description', f"ãƒ¢ãƒ‡ãƒ«{i}")
        
        print(f"\nã€{i}/{len(model_configs)}ã€‘ {description} ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ†ã‚¹ãƒˆä¸­...")
        
        # å¹´ç¯„å›²ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
        # ä¾‹: tokyo_turf_3ageup_long_2020-2022.sav
        import glob
        base_name = base_model_filename.replace('.sav', '')
        model_pattern = f"models/{base_name}_*-*.sav"
        matching_models = glob.glob(model_pattern)
        
        # ãƒãƒƒãƒã™ã‚‹ãƒ¢ãƒ‡ãƒ«ãŒãªã‘ã‚Œã°å…ƒã®ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ä½¿ç”¨
        if not matching_models:
            model_filename = base_model_filename
            train_year_range = "unknown"
        else:
            # æœ€æ–°ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åã§ã‚½ãƒ¼ãƒˆï¼‰
            model_filename = sorted(matching_models)[-1]
            # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰å­¦ç¿’æœŸé–“ã‚’æŠ½å‡º
            import re
            match = re.search(r'_(\d{4})-(\d{4})\.sav$', model_filename)
            if match:
                train_year_range = f"{match.group(1)}-{match.group(2)}"
            else:
                train_year_range = "unknown"
        
        print(f"[FILE] ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«: {model_filename}")
        if train_year_range != "unknown":
            print(f"[RUN] å­¦ç¿’æœŸé–“: {train_year_range}")
        
        # ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
        model_path = model_filename
        if not os.path.exists(model_path):
            # modelsãƒ•ã‚©ãƒ«ãƒ€ã‚‚ç¢ºèª
            models_path = f"models/{base_model_filename}"
            if os.path.exists(models_path):
                model_path = models_path
                train_year_range = "unknown"
                print(f"[DIR] modelsãƒ•ã‚©ãƒ«ãƒ€å†…ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨: {models_path}")
            else:
                print(f"[!] ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                continue
        
        try:
            output_df, summary_df, race_count = predict_with_model(
                model_filename=model_path,  # å­˜åœ¨ç¢ºèªæ¸ˆã¿ã®ãƒ‘ã‚¹ã‚’ä½¿ç”¨
                track_code=config['track_code'],
                kyoso_shubetsu_code=config['kyoso_shubetsu_code'],
                surface_type=config['surface_type'],
                min_distance=config['min_distance'],
                max_distance=config['max_distance'],
                test_year_start=test_year_start,
                test_year_end=test_year_end
            )
            
            if output_df is not None:
                # çµæœãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆï¼ˆå­¦ç¿’æœŸé–“ã¨ãƒ†ã‚¹ãƒˆå¹´ã‚’å«ã‚ã‚‹ï¼‰
                base_filename = base_model_filename.replace('.sav', '')
                test_year_str = f"{test_year_start}-{test_year_end}" if test_year_start != test_year_end else str(test_year_start)
                individual_output_file = f"predicted_results_{base_filename}_train{train_year_range}_test{test_year_str}.tsv"
                summary_file = f"betting_summary_{base_filename}_train{train_year_range}_test{test_year_str}.tsv"
                
                # å€‹åˆ¥ãƒ¢ãƒ‡ãƒ«çµæœã‚’ä¸Šæ›¸ãä¿å­˜ï¼ˆè¿½è¨˜ã§ã¯ãªãä¸Šæ›¸ãï¼‰
                save_results_with_append(output_df, individual_output_file, append_mode=False)
                
                # å…¨ãƒ¢ãƒ‡ãƒ«çµ±åˆãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ï¼ˆåˆå›ã¯ä¸Šæ›¸ãã€ä»¥é™ã¯è¿½è¨˜ï¼‰
                unified_output_file = "predicted_results.tsv"
                save_results_with_append(output_df, unified_output_file, append_mode=not first_unified_write)
                first_unified_write = False  # åˆå›æ›¸ãè¾¼ã¿å®Œäº†
                
                # ã‚µãƒãƒªãƒ¼ã¯å€‹åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
                results_dir = Path('results')
                results_dir.mkdir(exist_ok=True)
                summary_filepath = results_dir / summary_file
                summary_df.to_csv(summary_filepath, index=True, sep='\t', encoding='utf-8-sig', float_format='%.8f')
                
                print(f"[OK] å®Œäº†ï¼ãƒ¬ãƒ¼ã‚¹æ•°: {race_count}")
                print(f"  - å€‹åˆ¥çµæœ: {individual_output_file}")
                print(f"  - çµ±åˆçµæœï¼ˆé€šå¸¸ãƒ¬ãƒ¼ã‚¹ï¼‰: {unified_output_file}")
                print(f"  - çµ±åˆçµæœï¼ˆã‚¹ã‚­ãƒƒãƒ—ãƒ¬ãƒ¼ã‚¹ï¼‰: {unified_output_file.replace('.tsv', '_skipped.tsv')}")
                print(f"  - ã‚µãƒãƒªãƒ¼: {summary_file}")
                
                # çµæœã‚’ä¿å­˜ï¼ˆå¾Œã§æ¯”è¼ƒç”¨ï¼‰
                all_results[description] = {
                    'summary': summary_df,
                    'race_count': race_count,
                    'model_filename': model_filename
                }
                
                # ä¸»è¦ãªçµæœã‚’è¡¨ç¤º
                print(f"  - å˜å‹çš„ä¸­ç‡: {summary_df.loc['å˜å‹', 'çš„ä¸­ç‡(%)']:.2f}%")
                print(f"  - å˜å‹å›åç‡: {summary_df.loc['å˜å‹', 'å›åç‡(%)']:.2f}%")
                print(f"  - è¤‡å‹çš„ä¸­ç‡: {summary_df.loc['è¤‡å‹', 'çš„ä¸­ç‡(%)']:.2f}%")
                print(f"  - è¤‡å‹å›åç‡: {summary_df.loc['è¤‡å‹', 'å›åç‡(%)']:.2f}%")
                
            else:
                print(f"[ERROR] ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                
        except Exception as e:
            print(f"[ERROR] ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            import traceback
            traceback.print_exc()
        
        print("-" * 60)
    
    # è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®æ¯”è¼ƒçµæœã‚’ä½œæˆ
    if len(all_results) > 1:
        print("\n[+] ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒçµæœ")
        print("=" * 60)
        
        comparison_data = []
        for description, result in all_results.items():
            summary = result['summary']
            comparison_data.append({
                'ãƒ¢ãƒ‡ãƒ«': description,
                'ãƒ¬ãƒ¼ã‚¹æ•°': result['race_count'],
                'å˜å‹çš„ä¸­ç‡': f"{summary.loc['å˜å‹', 'çš„ä¸­ç‡(%)']:.2f}%",
                'å˜å‹å›åç‡': f"{summary.loc['å˜å‹', 'å›åç‡(%)']:.2f}%",
                'è¤‡å‹çš„ä¸­ç‡': f"{summary.loc['è¤‡å‹', 'çš„ä¸­ç‡(%)']:.2f}%",
                'è¤‡å‹å›åç‡': f"{summary.loc['è¤‡å‹', 'å›åç‡(%)']:.2f}%",
                'ä¸‰é€£è¤‡çš„ä¸­ç‡': f"{summary.loc['ï¼“é€£è¤‡', 'çš„ä¸­ç‡(%)']:.2f}%",
                'ä¸‰é€£è¤‡å›åç‡': f"{summary.loc['ï¼“é€£è¤‡', 'å›åç‡(%)']:.2f}%"
            })
        
        comparison_df = pd.DataFrame(comparison_data)
        
        # æ¯”è¼ƒçµæœã‚’ä¿å­˜
        comparison_file = 'model_comparison.tsv'
        results_dir = Path('results')
        results_dir.mkdir(exist_ok=True)
        comparison_filepath = results_dir / comparison_file
        
        comparison_df.to_csv(comparison_filepath, index=False, sep='\t', encoding='utf-8-sig', float_format='%.8f')
        
        print(comparison_df.to_string(index=False))
        print(f"\n[LIST] æ¯”è¼ƒçµæœã‚’ {comparison_filepath} ã«ä¿å­˜ã—ã¾ã—ãŸï¼")
    
    print("\n[DONE] ã™ã¹ã¦ã®ãƒ†ã‚¹ãƒˆãŒå®Œäº†ã—ã¾ã—ãŸï¼")


def predict_and_save_results():
    """
    æ—§ãƒãƒ¼ã‚¸ãƒ§ãƒ³äº’æ›æ€§ã®ãŸã‚ã®é–¢æ•°
    é˜ªç¥ç«¶é¦¬å ´ã®ï¼“æ­³ä»¥ä¸ŠèŠä¸­é•·è·é›¢ãƒ¢ãƒ‡ãƒ«ã§ãƒ†ã‚¹ãƒˆ
    """
    output_df, summary_df, race_count = predict_with_model(
        model_filename='hanshin_shiba_3ageup_model.sav',
        track_code='09',  # é˜ªç¥
        kyoso_shubetsu_code='13',  # 3æ­³ä»¥ä¸Š
        surface_type='turf',  # èŠ
        min_distance=1700,  # ä¸­é•·è·é›¢
        max_distance=9999  # ä¸Šé™ãªã—
    )
    
    if output_df is not None:
        # resultsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
        results_dir = Path('results')
        results_dir.mkdir(exist_ok=True)
        
        # çµæœã‚’TSVã«ä¿å­˜ï¼ˆè¿½è¨˜ãƒ¢ãƒ¼ãƒ‰ï¼‰
        output_file = 'predicted_results.tsv'
        save_results_with_append(output_df, output_file, append_mode=True)
        print(f"äºˆæ¸¬çµæœã‚’ results/{output_file} ã«ä¿å­˜ã—ã¾ã—ãŸï¼")

        # çš„ä¸­ç‡ã¨å›åç‡ã‚’åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        summary_file = 'betting_summary.tsv'
        summary_filepath = results_dir / summary_file
        summary_df.to_csv(summary_filepath, index=True, sep='\t', encoding='utf-8-sig', float_format='%.8f')
        print(f"çš„ä¸­ç‡ãƒ»å›åç‡ãƒ»çš„ä¸­æ•°ã‚’ results/{summary_file} ã«ä¿å­˜ã—ã¾ã—ãŸï¼")


if __name__ == '__main__':
    # å®Ÿè¡Œæ–¹æ³•ã‚’é¸æŠã§ãã‚‹ã‚ˆã†ã«
    import sys
    
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ†ã‚¹ãƒˆå¹´ç¯„å›²
    test_year_start = 2023
    test_year_end = 2023
    
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‚’è§£æ
    mode = 'single'  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯å˜ä¸€ãƒ¢ãƒ‡ãƒ«ãƒ†ã‚¹ãƒˆ
    model_type = 'all'  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯å…¨ãƒ¢ãƒ‡ãƒ«
    
    for arg in sys.argv[1:]:
        if arg == 'multi':
            mode = 'multi'
        elif arg in ['--custom', '--model-type=custom']:
            model_type = 'custom'
            mode = 'multi'
        elif arg in ['--standard', '--model-type=standard']:
            model_type = 'standard'
            mode = 'multi'
        elif '-' in arg and arg[0].isdigit():
            # "2020-2023" å½¢å¼ã®å¹´ç¯„å›²æŒ‡å®š
            try:
                years = arg.split('-')
                if len(years) == 2:
                    test_year_start = int(years[0])
                    test_year_end = int(years[1])
                    print(f"[DATE] ãƒ†ã‚¹ãƒˆå¹´ç¯„å›²æŒ‡å®š: {test_year_start}å¹´~{test_year_end}å¹´")
            except ValueError:
                print(f"[!] ç„¡åŠ¹ãªå¹´ç¯„å›²ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ: {arg} (ä¾‹: 2020-2023)")
        elif arg.isdigit() and len(arg) == 4:
            # "2023" å½¢å¼ã®å˜ä¸€å¹´æŒ‡å®š
            test_year_start = test_year_end = int(arg)
            print(f"[DATE] ãƒ†ã‚¹ãƒˆå¹´æŒ‡å®š: {test_year_start}å¹´")
    
    if mode == 'multi':
        # python universal_test.py multi [å¹´ç¯„å›²]
        # python universal_test.py --custom 2023 (customãƒ¢ãƒ‡ãƒ«ã®ã¿)
        test_multiple_models(test_year_start=test_year_start, test_year_end=test_year_end, model_type=model_type)
    else:
        # python universal_test.py [å¹´ç¯„å›²] (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ)
        # å˜ä¸€ãƒ¢ãƒ‡ãƒ«ãƒ†ã‚¹ãƒˆã§å¹´ç¯„å›²ã‚’ä½¿ç”¨
        output_df, summary_df, race_count = predict_with_model(
            model_filename='models/tokyo_turf_3ageup_long_baseline.sav',
            track_code='05',  # æ±äº¬
            kyoso_shubetsu_code='13',  # 3æ­³ä»¥ä¸Š
            surface_type='turf',  # èŠ
            min_distance=1700,  # ä¸­é•·è·é›¢
            max_distance=9999,  # ä¸Šé™ãªã—
            test_year_start=test_year_start,
            test_year_end=test_year_end
        )
        
        if output_df is not None:
            # resultsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
            results_dir = Path('results')
            results_dir.mkdir(exist_ok=True)
            
            # çµæœã‚’TSVã«ä¿å­˜ï¼ˆè¿½è¨˜ãƒ¢ãƒ¼ãƒ‰ï¼‰
            output_file = 'predicted_results.tsv'
            save_results_with_append(output_df, output_file, append_mode=True)
            print(f"äºˆæ¸¬çµæœã‚’ results/{output_file} ã«ä¿å­˜ã—ã¾ã—ãŸ!")

            # çš„ä¸­ç‡ã¨å›åç‡ã‚’åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
            summary_file = 'betting_summary.tsv'
            summary_filepath = results_dir / summary_file
            summary_df.to_csv(summary_filepath, index=True, sep='\t', encoding='utf-8-sig', float_format='%.8f')
            print(f"çš„ä¸­ç‡ãƒ»å›åç‡ãƒ»çš„ä¸­æ•°ã‚’ results/{summary_file} ã«ä¿å­˜ã—ã¾ã—ãŸ!")