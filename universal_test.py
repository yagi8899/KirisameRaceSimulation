#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ±ç”¨ç«¶é¦¬äºˆæ¸¬ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ã€è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã«å¯¾å¿œã—ãŸç«¶é¦¬äºˆæ¸¬ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
model_creator.pyã§ä½œæˆã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦äºˆæ¸¬ã‚’è¡Œã„ã€çµæœã‚’ä¿å­˜ã—ã¾ã™ã€‚
"""

import psycopg2
import pandas as pd
import pickle
import lightgbm as lgb
import numpy as np
import os
from pathlib import Path
from datetime import datetime
from keiba_constants import get_track_name, format_model_description
from model_config_loader import get_all_models, get_legacy_model

# Phase 1: æœŸå¾…å€¤ãƒ»ã‚±ãƒªãƒ¼åŸºæº–ãƒ»ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã®çµ±åˆ
from expected_value_calculator import ExpectedValueCalculator
from kelly_criterion import KellyCriterion
from race_confidence_scorer import RaceConfidenceScorer


def add_purchase_logic(
    output_df: pd.DataFrame,
    prediction_rank_max: int = 3,
    popularity_rank_max: int = 3,
    min_odds: float = 1.5,
    max_odds: float = 20.0,
    min_score_diff: float = 0.05,
    initial_bankroll: float = 1000000,
    bet_unit: int = 1000
) -> pd.DataFrame:
    """
    äºˆæ¸¬çµæœã«è³¼å…¥åˆ¤æ–­ãƒ»è³¼å…¥é¡ã‚’è¿½åŠ  (æ–°æˆ¦ç•¥: æœ¬å‘½Ã—äºˆæ¸¬ä¸Šä½ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼)
    
    Phase 1æ–°æˆ¦ç•¥:
    - äºˆæ¸¬é †ä½1-3ä½ AND äººæ°—é †1-3ä½ ã®ã¿å¯¾è±¡
    - ã‚ªãƒƒã‚ºç¯„å›²ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° (1.5å€ï½20å€)
    - äºˆæ¸¬ã‚¹ã‚³ã‚¢å·®ãŒä¸€å®šä»¥ä¸Šã®ãƒ¬ãƒ¼ã‚¹ã®ã¿ (æœ¬å‘½ãŒæ˜ç¢º)
    - ä¸€å¾‹ãƒ™ãƒƒãƒˆ (ã‚·ãƒ³ãƒ—ãƒ«&ç¢ºå®Ÿ)
    
    Args:
        output_df (DataFrame): äºˆæ¸¬çµæœãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        prediction_rank_max (int): äºˆæ¸¬é †ä½ã®ä¸Šé™ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 3)
        popularity_rank_max (int): äººæ°—é †ã®ä¸Šé™ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 3)
        min_odds (float): æœ€ä½ã‚ªãƒƒã‚º (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1.5å€)
        max_odds (float): æœ€é«˜ã‚ªãƒƒã‚º (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 20å€)
        min_score_diff (float): äºˆæ¸¬1ä½ã¨2ä½ã®ã‚¹ã‚³ã‚¢å·®ã®æœ€å°å€¤ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.05)
        initial_bankroll (float): åˆæœŸè³‡é‡‘ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 100ä¸‡å††)
        bet_unit (int): 1é ­ã‚ãŸã‚Šã®è³¼å…¥é¡ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1000å††)
        
    Returns:
        DataFrame: è³¼å…¥ãƒ­ã‚¸ãƒƒã‚¯ãŒè¿½åŠ ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
    """
    df = output_df.copy()
    
    # ã‚«ãƒ©ãƒ åã‚’ãƒãƒƒãƒ”ãƒ³ã‚° (æ—¥æœ¬èª â†’ è‹±èª)
    df_work = df.rename(columns={
        'é–‹å‚¬å¹´': 'kaisai_year',
        'é–‹å‚¬æ—¥': 'kaisai_date',
        'ç«¶é¦¬å ´': 'keibajo_code',
        'ãƒ¬ãƒ¼ã‚¹ç•ªå·': 'race_number',
        'é¦¬ç•ª': 'umaban_numeric',
        'äºˆæ¸¬é †ä½': 'predicted_rank',
        'äºˆæ¸¬ã‚¹ã‚³ã‚¢': 'predicted_score',
        'äººæ°—é †': 'popularity_rank',
        'å˜å‹ã‚ªãƒƒã‚º': 'tansho_odds',
        'ç¢ºå®šç€é †': 'chakujun_numeric'
    })
    
    # ãƒ¬ãƒ¼ã‚¹ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ã¦å‡¦ç†
    race_groups = df_work.groupby(['kaisai_year', 'kaisai_date', 'keibajo_code', 'race_number'])
    
    all_races = []
    current_bankroll = initial_bankroll
    total_purchased = 0
    total_wins = 0
    
    for race_id, race_df in race_groups:
        race_df = race_df.copy()
        
        # äºˆæ¸¬ã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆ(é™é †)
        race_df_sorted = race_df.sort_values('predicted_score', ascending=False).reset_index(drop=True)
        
        # äºˆæ¸¬1ä½ã¨2ä½ã®ã‚¹ã‚³ã‚¢å·®ã‚’è¨ˆç®—
        if len(race_df_sorted) >= 2:
            score_diff = race_df_sorted.iloc[0]['predicted_score'] - race_df_sorted.iloc[1]['predicted_score']
        else:
            score_diff = 0
        
        # å…¨é¦¬ã«ãƒ¬ãƒ¼ã‚¹æƒ…å ±ã‚’è¿½åŠ 
        race_df['score_diff'] = score_diff
        race_df['skip_reason'] = None
        
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼1: äºˆæ¸¬ã‚¹ã‚³ã‚¢å·®ãŒå°ã•ã„ãƒ¬ãƒ¼ã‚¹ã¯ã‚¹ã‚­ãƒƒãƒ—
        if score_diff < min_score_diff:
            race_df['è³¼å…¥æ¨å¥¨'] = False
            race_df['è³¼å…¥é¡'] = 0
            race_df['skip_reason'] = 'low_score_diff'
            all_races.append(race_df)
            continue
        
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼2: äºˆæ¸¬é †ä½ AND äººæ°—é † AND ã‚ªãƒƒã‚ºç¯„å›²
        race_df['è³¼å…¥æ¨å¥¨'] = (
            (race_df['predicted_rank'] <= prediction_rank_max) &
            (race_df['popularity_rank'] <= popularity_rank_max) &
            (race_df['tansho_odds'] >= min_odds) &
            (race_df['tansho_odds'] <= max_odds)
        )
        
        # ã‚¹ã‚­ãƒƒãƒ—ç†ç”±ã‚’è¨˜éŒ²
        race_df.loc[~race_df['è³¼å…¥æ¨å¥¨'] & (race_df['predicted_rank'] > prediction_rank_max), 'skip_reason'] = 'low_predicted_rank'
        race_df.loc[~race_df['è³¼å…¥æ¨å¥¨'] & (race_df['popularity_rank'] > popularity_rank_max), 'skip_reason'] = 'low_popularity'
        race_df.loc[~race_df['è³¼å…¥æ¨å¥¨'] & (race_df['tansho_odds'] < min_odds), 'skip_reason'] = 'odds_too_low'
        race_df.loc[~race_df['è³¼å…¥æ¨å¥¨'] & (race_df['tansho_odds'] > max_odds), 'skip_reason'] = 'odds_too_high'
        
        # è³¼å…¥æ¨å¥¨é¦¬ã‚’æŠ½å‡º
        buy_horses = race_df[race_df['è³¼å…¥æ¨å¥¨']].copy()
        
        # è³¼å…¥é¡åˆ—ã‚’åˆæœŸåŒ–
        race_df['è³¼å…¥é¡'] = 0
        
        if len(buy_horses) > 0:
            # ä¸€å¾‹ãƒ™ãƒƒãƒˆ
            total_purchased += len(buy_horses)
            
            # è³‡é‡‘ã‚’æ›´æ–°
            total_bet = bet_unit * len(buy_horses)
            total_return = 0
            
            for idx in buy_horses.index:
                race_df.loc[idx, 'è³¼å…¥é¡'] = bet_unit
                if race_df.loc[idx, 'chakujun_numeric'] == 1:
                    total_return += bet_unit * race_df.loc[idx, 'tansho_odds']
                    total_wins += 1
            
            current_bankroll = current_bankroll - total_bet + total_return
        
        # ç¾åœ¨ã®è³‡é‡‘æ®‹é«˜ã‚’è¨˜éŒ²
        race_df['ç¾åœ¨è³‡é‡‘'] = current_bankroll
        
        all_races.append(race_df)
    
    # å…¨ãƒ¬ãƒ¼ã‚¹ã‚’çµ±åˆ
    df_integrated = pd.concat(all_races, ignore_index=True)
    
    # ã‚«ãƒ©ãƒ åã‚’æ—¥æœ¬èªã«æˆ»ã™(è‹±èªã‹ã‚‰æ—¥æœ¬èªã¸)
    df_integrated = df_integrated.rename(columns={
        'kaisai_year': 'é–‹å‚¬å¹´',
        'kaisai_date': 'é–‹å‚¬æ—¥',
        'keibajo_code': 'ç«¶é¦¬å ´',
        'race_number': 'ãƒ¬ãƒ¼ã‚¹ç•ªå·',
        'umaban_numeric': 'é¦¬ç•ª',
        'predicted_rank': 'äºˆæ¸¬é †ä½',
        'predicted_score': 'äºˆæ¸¬ã‚¹ã‚³ã‚¢',
        'popularity_rank': 'äººæ°—é †',
        'tansho_odds': 'å˜å‹ã‚ªãƒƒã‚º',
        'chakujun_numeric': 'ç¢ºå®šç€é †',
        'score_diff': 'ã‚¹ã‚³ã‚¢å·®',
        'skip_reason': 'ã‚¹ã‚­ãƒƒãƒ—ç†ç”±'
    })
    
    return df_integrated


def save_results_with_append(df, filename, append_mode=True, output_dir='results'):
    """
    çµæœã‚’TSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ï¼ˆè¿½è¨˜ãƒ¢ãƒ¼ãƒ‰å¯¾å¿œï¼‰
    
    Args:
        df (DataFrame): ä¿å­˜ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        filename (str): ä¿å­˜å…ˆãƒ•ã‚¡ã‚¤ãƒ«å
        append_mode (bool): True=è¿½è¨˜ãƒ¢ãƒ¼ãƒ‰ã€False=ä¸Šæ›¸ããƒ¢ãƒ¼ãƒ‰
        output_dir (str): å‡ºåŠ›å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 'results'ï¼‰
    """
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆï¼ˆå­˜åœ¨ã—ãªã„å ´åˆï¼‰
    output_path = Path(output_dir)
    output_path.mkdir(exist_ok=True)
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’ä½œæˆ
    filepath = output_path / filename
    
    if append_mode and filepath.exists():
        # ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ—¢ã«å­˜åœ¨ã™ã‚‹å ´åˆã¯è¿½è¨˜ï¼ˆãƒ˜ãƒƒãƒ€ãƒ¼ãªã—ï¼‰
        print(f"[NOTE] æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã«è¿½è¨˜: {filepath}")
        df.to_csv(filepath, mode='a', header=False, index=False, sep='\t', encoding='utf-8-sig')
    else:
        # ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯æ–°è¦ä½œæˆï¼ˆãƒ˜ãƒƒãƒ€ãƒ¼ã‚ã‚Šï¼‰
        print(f"[LIST] æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ: {filepath}")
        df.to_csv(filepath, index=False, sep='\t', encoding='utf-8-sig')


def predict_with_model(model_filename, track_code, kyoso_shubetsu_code, surface_type, 
                      min_distance, max_distance, test_year_start=2023, test_year_end=2023):
    """
    æŒ‡å®šã—ãŸãƒ¢ãƒ‡ãƒ«ã§äºˆæ¸¬ã‚’å®Ÿè¡Œã™ã‚‹æ±ç”¨é–¢æ•°
    
    Args:
        model_filename (str): ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«å
        track_code (str): ç«¶é¦¬å ´ã‚³ãƒ¼ãƒ‰
        kyoso_shubetsu_code (str): ç«¶äº‰ç¨®åˆ¥ã‚³ãƒ¼ãƒ‰
        surface_type (str): 'turf' or 'dirt'
        min_distance (int): æœ€å°è·é›¢
        max_distance (int): æœ€å¤§è·é›¢
        test_year_start (int): ãƒ†ã‚¹ãƒˆå¯¾è±¡é–‹å§‹å¹´ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 2023)
        test_year_end (int): ãƒ†ã‚¹ãƒˆå¯¾è±¡çµ‚äº†å¹´ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 2023)
        
    Returns:
        tuple: (äºˆæ¸¬çµæœDataFrame, ã‚µãƒãƒªãƒ¼DataFrame, ãƒ¬ãƒ¼ã‚¹æ•°)
    """
    
    # PostgreSQL ã‚³ãƒã‚¯ã‚·ãƒ§ãƒ³ã®ä½œæˆ
    conn = psycopg2.connect(
        host='localhost',
        port='5432',
        user='postgres',
        password='ahtaht88',
        dbname='keiba'
    )

    # ãƒˆãƒ©ãƒƒã‚¯æ¡ä»¶ã‚’å‹•çš„ã«è¨­å®š
    if surface_type.lower() == 'turf':
        # èŠã®å ´åˆ
        track_condition = "cast(rase.track_code as integer) between 10 and 22"
        baba_condition = "ra.babajotai_code_shiba"
    else:
        # ãƒ€ãƒ¼ãƒˆã®å ´åˆ
        track_condition = "cast(rase.track_code as integer) between 23 and 29"
        baba_condition = "ra.babajotai_code_dirt"

    # è·é›¢æ¡ä»¶ã‚’è¨­å®š
    if max_distance == 9999:
        distance_condition = f"cast(rase.kyori as integer) >= {min_distance}"
    else:
        distance_condition = f"cast(rase.kyori as integer) between {min_distance} and {max_distance}"

    # ç«¶äº‰ç¨®åˆ¥ã‚’è¨­å®š
    if kyoso_shubetsu_code == '12':
        # 3æ­³æˆ¦
        kyoso_shubetsu_condition = "cast(rase.kyoso_shubetsu_code as integer) = 12"
    elif kyoso_shubetsu_code == '13':
        kyoso_shubetsu_condition = "cast(rase.kyoso_shubetsu_code as integer) >= 13"

    # SQLã‚¯ã‚¨ãƒªã‚’å‹•çš„ã«ç”Ÿæˆ
    sql = f"""
    select * from (
        select
        ra.kaisai_nen,
        ra.kaisai_tsukihi,
        ra.race_bango,
        seum.umaban,
        seum.bamei,
        ra.keibajo_code,
        CASE 
            WHEN ra.keibajo_code = '01' THEN 'æœ­å¹Œ' 
            WHEN ra.keibajo_code = '02' THEN 'å‡½é¤¨' 
            WHEN ra.keibajo_code = '03' THEN 'ç¦å³¶' 
            WHEN ra.keibajo_code = '04' THEN 'æ–°æ½Ÿ' 
            WHEN ra.keibajo_code = '05' THEN 'æ±äº¬' 
            WHEN ra.keibajo_code = '06' THEN 'ä¸­å±±' 
            WHEN ra.keibajo_code = '07' THEN 'ä¸­äº¬' 
            WHEN ra.keibajo_code = '08' THEN 'äº¬éƒ½' 
            WHEN ra.keibajo_code = '09' THEN 'é˜ªç¥' 
            WHEN ra.keibajo_code = '10' THEN 'å°å€‰' 
            ELSE '' 
        END keibajo_name,
        ra.kyori,
        ra.shusso_tosu,
        ra.tenko_code,
        {baba_condition} as babajotai_code,
        ra.grade_code,
        ra.kyoso_joken_code,
        ra.kyoso_shubetsu_code,
        ra.track_code,
        seum.ketto_toroku_bango,
        seum.wakuban,
        cast(seum.umaban as integer) as umaban_numeric,
        seum.barei,
        seum.kishu_code,
        seum.chokyoshi_code,
        seum.kishu_name,
        seum.chokyoshi_name,
        seum.futan_juryo,
        seum.seibetsu_code,
        seum.corner_1,
        seum.corner_2,
        seum.corner_3,
        seum.corner_4,
        seum.kyakushitsu_hantei,
        nullif(cast(seum.tansho_odds as float), 0) / 10 as tansho_odds,
        nullif(cast(seum.tansho_ninkijun as integer), 0) as tansho_ninkijun_numeric,
        nullif(cast(seum.kakutei_chakujun as integer), 0) as kakutei_chakujun_numeric,
        1.0 / nullif(cast(seum.kakutei_chakujun as integer), 0) as chakujun_score,
        AVG(
            (1 - (cast(seum.kakutei_chakujun as float) / cast(ra.shusso_tosu as float)))
            * CASE
                WHEN seum.time_sa LIKE '-%' THEN 1.00  -- 1ç€(ãƒã‚¤ãƒŠã‚¹å€¤) â†’ ä¿‚æ•°1.00(æº€ç‚¹)
                WHEN CAST(REPLACE(seum.time_sa, '+', '') AS INTEGER) <= 5 THEN 0.85   -- 0.5ç§’å·®ä»¥å†… â†’ 0.85å€(15%æ¸›)
                WHEN CAST(REPLACE(seum.time_sa, '+', '') AS INTEGER) <= 10 THEN 0.70  -- 1.0ç§’å·®ä»¥å†… â†’ 0.70å€(30%æ¸›)
                WHEN CAST(REPLACE(seum.time_sa, '+', '') AS INTEGER) <= 20 THEN 0.50  -- 2.0ç§’å·®ä»¥å†… â†’ 0.50å€(50%æ¸›)
                WHEN CAST(REPLACE(seum.time_sa, '+', '') AS INTEGER) <= 30 THEN 0.30  -- 3.0ç§’å·®ä»¥å†… â†’ 0.30å€(70%æ¸›)
                ELSE 0.20  -- 3.0ç§’è¶… â†’ 0.20å€(å¤§æ•—ã¯ã»ã¼ç„¡è¦–)
            END
        ) OVER (
            PARTITION BY seum.ketto_toroku_bango
            ORDER BY cast(ra.kaisai_nen as integer), cast(ra.kaisai_tsukihi as integer)
            ROWS BETWEEN 3 PRECEDING AND 1 PRECEDING
        ) AS past_avg_sotai_chakujun,
        AVG(
            cast(ra.kyori as integer) /
            NULLIF(
                FLOOR(cast(seum.soha_time as integer) / 1000) * 60 +
                FLOOR((cast(seum.soha_time as integer) % 1000) / 10) +
                (cast(seum.soha_time as integer) % 10) * 0.1,
                0
            )
        ) OVER (
            PARTITION BY seum.ketto_toroku_bango
            ORDER BY cast(ra.kaisai_nen as integer), cast(ra.kaisai_tsukihi as integer)
            ROWS BETWEEN 3 PRECEDING AND 1 PRECEDING
        ) AS time_index,
        SUM(
            CASE 
                WHEN seum.kakutei_chakujun = '01' THEN 100
                WHEN seum.kakutei_chakujun = '02' THEN 80
                WHEN seum.kakutei_chakujun = '03' THEN 60
                WHEN seum.kakutei_chakujun = '04' THEN 40
                WHEN seum.kakutei_chakujun = '05' THEN 30
                WHEN seum.kakutei_chakujun = '06' THEN 20
                WHEN seum.kakutei_chakujun = '07' THEN 10
                ELSE 5 
            END
            * CASE 
                WHEN ra.grade_code = 'A' THEN 3.00
                WHEN ra.grade_code = 'B' THEN 2.00
                WHEN ra.grade_code = 'C' THEN 1.50
                WHEN ra.grade_code <> 'A' AND ra.grade_code <> 'B' AND ra.grade_code <> 'C' AND ra.kyoso_joken_code = '999' THEN 1.00
                WHEN ra.grade_code <> 'A' AND ra.grade_code <> 'B' AND ra.grade_code <> 'C' AND ra.kyoso_joken_code = '016' THEN 0.80
                WHEN ra.grade_code <> 'A' AND ra.grade_code <> 'B' AND ra.grade_code <> 'C' AND ra.kyoso_joken_code = '010' THEN 0.60
                WHEN ra.grade_code <> 'A' AND ra.grade_code <> 'B' AND ra.grade_code <> 'C' AND ra.kyoso_joken_code = '005' THEN 0.40
                ELSE 0.20
            END
        ) OVER (
            PARTITION BY seum.ketto_toroku_bango
            ORDER BY cast(ra.kaisai_nen as integer), cast(ra.kaisai_tsukihi as integer)
            ROWS BETWEEN 3 PRECEDING AND 1 PRECEDING  
        ) AS past_score,
        CASE 
            WHEN AVG(
                CASE 
                    WHEN cast(seum.kohan_3f as integer) > 0 AND cast(seum.kohan_3f as integer) < 999 THEN
                    CAST(seum.kohan_3f AS FLOAT) / 10
                    ELSE NULL
                END
            ) OVER (
                PARTITION BY seum.ketto_toroku_bango
                ORDER BY cast(ra.kaisai_nen as integer), cast(ra.kaisai_tsukihi as integer)
                ROWS BETWEEN 3 PRECEDING AND 1 PRECEDING
            ) IS NOT NULL THEN
            AVG(
                CASE 
                    WHEN cast(seum.kohan_3f as integer) > 0 AND cast(seum.kohan_3f as integer) < 999 THEN
                    CAST(seum.kohan_3f AS FLOAT) / 10
                    ELSE NULL
                END
            ) OVER (
                PARTITION BY seum.ketto_toroku_bango
                ORDER BY cast(ra.kaisai_nen as integer), cast(ra.kaisai_tsukihi as integer)
                ROWS BETWEEN 3 PRECEDING AND 1 PRECEDING
            ) - 
            CASE
                WHEN cast(ra.kyori as integer) <= 1600 THEN 33.5
                WHEN cast(ra.kyori as integer) <= 2000 THEN 35.0
                WHEN cast(ra.kyori as integer) <= 2400 THEN 36.0
                ELSE 37.0
            END
            ELSE 0
        END AS kohan_3f_index
        -- é¦¬ä½“é‡é–¢é€£ã®ç‰¹å¾´é‡
        ,nullif(cast(seum.bataiju as integer), 0) as bataiju_current
        ,CASE 
            WHEN seum.zogen_fugo = '-' THEN -1 * nullif(cast(seum.zogen_sa as integer), 0)
            ELSE nullif(cast(seum.zogen_sa as integer), 0)
        END as bataiju_change
        -- å‰èµ°ã®é¦¬ä½“é‡
        ,LAG(nullif(cast(seum.bataiju as integer), 0)) OVER (
            PARTITION BY seum.ketto_toroku_bango
            ORDER BY cast(ra.kaisai_nen as integer), cast(ra.kaisai_tsukihi as integer)
        ) as bataiju_prev
        -- éå»3èµ°ã®å¹³å‡é¦¬ä½“é‡
        ,AVG(nullif(cast(seum.bataiju as integer), 0)) OVER (
            PARTITION BY seum.ketto_toroku_bango
            ORDER BY cast(ra.kaisai_nen as integer), cast(ra.kaisai_tsukihi as integer)
            ROWS BETWEEN 3 PRECEDING AND 1 PRECEDING
        ) as bataiju_avg_3races
        ,nullif(cast(nullif(trim(hr.haraimodoshi_fukusho_1a), '') as integer), 0) as è¤‡å‹1ç€é¦¬ç•ª
        ,nullif(cast(nullif(trim(hr.haraimodoshi_fukusho_1b), '') as float), 0) / 100 as è¤‡å‹1ç€ã‚ªãƒƒã‚º
        ,nullif(cast(nullif(trim(hr.haraimodoshi_fukusho_1c), '') as integer), 0) as è¤‡å‹1ç€äººæ°—
        ,nullif(cast(nullif(trim(hr.haraimodoshi_fukusho_2a), '') as integer), 0) as è¤‡å‹2ç€é¦¬ç•ª
        ,nullif(cast(nullif(trim(hr.haraimodoshi_fukusho_2b), '') as float), 0) / 100 as è¤‡å‹2ç€ã‚ªãƒƒã‚º
        ,nullif(cast(nullif(trim(hr.haraimodoshi_fukusho_2c), '') as integer), 0) as è¤‡å‹2ç€äººæ°—
        ,nullif(cast(nullif(trim(hr.haraimodoshi_fukusho_3a), '') as integer), 0) as è¤‡å‹3ç€é¦¬ç•ª
        ,nullif(cast(nullif(trim(hr.haraimodoshi_fukusho_3b), '') as float), 0) / 100 as è¤‡å‹3ç€ã‚ªãƒƒã‚º
        ,nullif(cast(nullif(trim(hr.haraimodoshi_fukusho_3c), '') as integer), 0) as è¤‡å‹3ç€äººæ°—
        ,cast(substring(trim(hr.haraimodoshi_umaren_1a), 1, 2) as integer) as é¦¬é€£é¦¬ç•ª1
        ,cast(substring(trim(hr.haraimodoshi_umaren_1a), 3, 2) as integer) as é¦¬é€£é¦¬ç•ª2
        ,nullif(cast(nullif(trim(hr.haraimodoshi_umaren_1b), '') as float), 0) / 100 as é¦¬é€£ã‚ªãƒƒã‚º
        ,cast(substring(trim(hr.haraimodoshi_wide_1a), 1, 2) as integer) as ãƒ¯ã‚¤ãƒ‰1_2é¦¬ç•ª1
        ,cast(substring(trim(hr.haraimodoshi_wide_1a), 3, 2) as integer) as ãƒ¯ã‚¤ãƒ‰1_2é¦¬ç•ª2
        ,cast(substring(trim(hr.haraimodoshi_wide_2a), 1, 2) as integer) as ãƒ¯ã‚¤ãƒ‰2_3ç€é¦¬ç•ª1
        ,cast(substring(trim(hr.haraimodoshi_wide_2a), 3, 2) as integer) as ãƒ¯ã‚¤ãƒ‰2_3ç€é¦¬ç•ª2
        ,cast(substring(trim(hr.haraimodoshi_wide_3a), 1, 2) as integer) as ãƒ¯ã‚¤ãƒ‰1_3ç€é¦¬ç•ª1
        ,cast(substring(trim(hr.haraimodoshi_wide_3a), 3, 2) as integer) as ãƒ¯ã‚¤ãƒ‰1_3ç€é¦¬ç•ª2
        ,nullif(cast(nullif(trim(hr.haraimodoshi_wide_1b), '') as float), 0) / 100 as ãƒ¯ã‚¤ãƒ‰1_2ã‚ªãƒƒã‚º
        ,nullif(cast(nullif(trim(hr.haraimodoshi_wide_2b), '') as float), 0) / 100 as ãƒ¯ã‚¤ãƒ‰2_3ã‚ªãƒƒã‚º
        ,nullif(cast(nullif(trim(hr.haraimodoshi_wide_3b), '') as float), 0) / 100 as ãƒ¯ã‚¤ãƒ‰1_3ã‚ªãƒƒã‚º
        ,cast(substring(trim(hr.haraimodoshi_umatan_1a), 1, 2) as integer) as é¦¬å˜é¦¬ç•ª1
        ,cast(substring(trim(hr.haraimodoshi_umatan_1a), 3, 2) as integer) as é¦¬å˜é¦¬ç•ª2
        ,nullif(cast(nullif(trim(hr.haraimodoshi_umatan_1b), '') as float), 0) / 100 as é¦¬å˜ã‚ªãƒƒã‚º
        ,nullif(cast(nullif(trim(hr.haraimodoshi_sanrenpuku_1b), '') as float), 0) / 100 as ï¼“é€£è¤‡ã‚ªãƒƒã‚º
    from
        jvd_ra ra 
        inner join ( 
            select
                se.kaisai_nen
                , se.kaisai_tsukihi
                , se.keibajo_code
                , se.race_bango
                , se.kakutei_chakujun
                , se.ketto_toroku_bango
                , se.bamei
                , se.wakuban
                , se.umaban
                , se.barei
                , se.seibetsu_code
                , se.futan_juryo
                , se.kishu_code
                , se.chokyoshi_code
                , trim(se.kishumei_ryakusho) as kishu_name
                , trim(se.chokyoshimei_ryakusho) as chokyoshi_name
                , se.tansho_odds
                , se.tansho_ninkijun
                , se.bataiju
                , se.zogen_fugo
                , se.zogen_sa
                , se.kohan_3f
                , se.soha_time
                , se.time_sa
                , se.corner_1
                , se.corner_2
                , se.corner_3
                , se.corner_4
                , se.kyakushitsu_hantei
            from
                jvd_se se 
            where
                se.kohan_3f <> '000' 
                and se.kohan_3f <> '999'
        ) seum 
            on ra.kaisai_nen = seum.kaisai_nen 
            and ra.kaisai_tsukihi = seum.kaisai_tsukihi 
            and ra.keibajo_code = seum.keibajo_code 
            and ra.race_bango = seum.race_bango
        inner join jvd_hr hr
            on ra.kaisai_nen = hr.kaisai_nen 
            and ra.kaisai_tsukihi = hr.kaisai_tsukihi 
            and ra.keibajo_code = hr.keibajo_code 
            and ra.race_bango = hr.race_bango
    where
        cast(ra.kaisai_nen as integer) between {test_year_start - 3} and {test_year_end}  --ãƒ†ã‚¹ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿ã®å¯¾è±¡å¹´ç¯„å›²
    ) rase 
    where 
    rase.keibajo_code = '{track_code}'
    and cast(rase.kaisai_nen as integer) between {test_year_start} and {test_year_end}  --ãƒ†ã‚¹ãƒˆå¹´ç¯„å›²
    and {kyoso_shubetsu_condition}
    and {track_condition}
    and {distance_condition}
    """
    
    # ãƒ†ã‚¹ãƒˆç”¨ã®SQLã‚’ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›ï¼ˆå¸¸ã«ä¸Šæ›¸ãï¼‰
    log_filepath = Path('sql_log_test.txt')
    with open(log_filepath, 'w', encoding='utf-8') as f:
        f.write(f"=== ãƒ†ã‚¹ãƒˆç”¨SQL ===\n")
        f.write(f"ãƒ¢ãƒ‡ãƒ«: {model_filename}\n")
        f.write(f"ãƒ†ã‚¹ãƒˆæœŸé–“: {test_year_start}å¹´ã€œ{test_year_end}å¹´\n")
        f.write(f"å®Ÿè¡Œæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"\n{sql}\n")
    print(f"[NOTE] ãƒ†ã‚¹ãƒˆç”¨SQLã‚’ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›: {log_filepath}")

    # ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    df = pd.read_sql_query(sql=sql, con=conn)
    conn.close()
    
    if len(df) == 0:
        print(f"[ERROR] {model_filename} ã«å¯¾å¿œã™ã‚‹ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return None, None, 0

    print(f"[+] ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(df)}ä»¶")

    # ä¿®æ­£: ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã‚’é©åˆ‡ã«å®Ÿæ–½ï¼ˆmodel_creator.pyã¨åŒã˜ãƒ­ã‚¸ãƒƒã‚¯ï¼‰
    # é¨æ‰‹ã‚³ãƒ¼ãƒ‰ãƒ»èª¿æ•™å¸«ã‚³ãƒ¼ãƒ‰ãƒ»é¦¬åãªã©ã®æ–‡å­—åˆ—åˆ—ã‚’ä¿æŒã—ãŸã¾ã¾ã€æ•°å€¤åˆ—ã®ã¿ã‚’å‡¦ç†
    print("[TEST] ãƒ‡ãƒ¼ã‚¿å‹ç¢ºèª...")
    print(f"  kishu_codeå‹ï¼ˆä¿®æ­£å‰ï¼‰: {df['kishu_code'].dtype}")
    print(f"  kishu_codeã‚µãƒ³ãƒ—ãƒ«: {df['kishu_code'].head(5).tolist()}")
    print(f"  kishu_codeãƒ¦ãƒ‹ãƒ¼ã‚¯æ•°: {df['kishu_code'].nunique()}")
    
    # æ•°å€¤åŒ–ã™ã‚‹åˆ—ã‚’æ˜ç¤ºçš„ã«æŒ‡å®šï¼ˆæ–‡å­—åˆ—åˆ—ã¯é™¤å¤–ï¼‰
    numeric_columns = [
        'wakuban', 'umaban_numeric', 'barei', 'futan_juryo', 'tansho_odds',
        'kaisai_nen', 'kaisai_tsukihi', 'race_bango', 'kyori', 'shusso_tosu',
        'tenko_code', 'babajotai_code', 'grade_code', 'kyoso_joken_code',
        'kyoso_shubetsu_code', 'track_code', 'seibetsu_code',
        'kakutei_chakujun_numeric', 'chakujun_score', 'past_avg_sotai_chakujun',
        'time_index', 'past_score', 'kohan_3f_index', 'corner_1', 'corner_2',
        'corner_3', 'corner_4', 'kyakushitsu_hantei'
    ]
    
    # æ•°å€¤åŒ–ã™ã‚‹åˆ—ã®ã¿å‡¦ç†ï¼ˆæ–‡å­—åˆ—åˆ—ã¯ä¿æŒï¼‰
    for col in numeric_columns:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce')
    
    # æ¬ æå€¤ã‚’0ã§åŸ‹ã‚ã‚‹ï¼ˆæ•°å€¤åˆ—ã®ã¿ã€å­˜åœ¨ã™ã‚‹åˆ—ã®ã¿å‡¦ç†ï¼‰
    existing_numeric_columns = [col for col in numeric_columns if col in df.columns]
    df[existing_numeric_columns] = df[existing_numeric_columns].fillna(0)
    
    # æ–‡å­—åˆ—å‹ã®åˆ—ã¯ãã®ã¾ã¾ä¿æŒï¼ˆkishu_code, chokyoshi_code, bamei ãªã©ï¼‰
    print(f"  kishu_codeå‹ï¼ˆä¿®æ­£å¾Œï¼‰: {df['kishu_code'].dtype}")
    print(f"  kishu_codeã‚µãƒ³ãƒ—ãƒ«: {df['kishu_code'].head(5).tolist()}")
    print("[OK] ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†å®Œäº†ï¼ˆæ–‡å­—åˆ—åˆ—ã‚’ä¿æŒï¼‰")

    # past_avg_sotai_chakujunã¯SQLã§è¨ˆç®—æ¸ˆã¿ã®å˜ç´”ç§»å‹•å¹³å‡ã‚’ä½¿ç”¨
    # (EWMå®Ÿé¨“ã®çµæœã€å˜ç´”å¹³å‡ã®æ–¹ãŒè¤‡å‹ãƒ»ä¸‰é€£è¤‡ã§å®‰å®šã—ãŸæ€§èƒ½ã‚’ç¤ºã—ãŸ)

    # ç‰¹å¾´é‡ã‚’é¸æŠï¼ˆmodel_creator.pyã¨åŒã˜ç‰¹å¾´é‡ï¼‰
    X = df.loc[:, [
        # "futan_juryo",
        "past_score",
        "kohan_3f_index",
        "past_avg_sotai_chakujun",
        "time_index",
    ]].astype(float)
    
    # é«˜æ€§èƒ½ãªæ´¾ç”Ÿç‰¹å¾´é‡ã‚’è¿½åŠ ï¼ï¼ˆmodel_creator.pyã¨åŒã˜ï¼‰
    # æ ç•ªã¨é ­æ•°ã®æ¯”ç‡ï¼ˆå†…æ æœ‰åˆ©åº¦ï¼‰
    max_wakuban = df.groupby(['kaisai_nen', 'kaisai_tsukihi', 'race_bango'])['wakuban'].transform('max')
    df['wakuban_ratio'] = df['wakuban'] / max_wakuban
    X['wakuban_ratio'] = df['wakuban_ratio']
    
    # æ–¤é‡ã¨é¦¬é½¢ã®æ¯”ç‡ï¼ˆè‹¥é¦¬ã®è² æ‹…èƒ½åŠ›ï¼‰
    df['futan_per_barei'] = df['futan_juryo'] / df['barei'].replace(0, 1)
    X['futan_per_barei'] = df['futan_per_barei']
    
    # æ”¹å–„ã•ã‚ŒãŸç‰¹å¾´é‡
    # 2. futan_per_bareiã®éç·šå½¢å¤‰æ›
    df['futan_per_barei_log'] = np.log(df['futan_per_barei'].clip(lower=0.1))
    X['futan_per_barei_log'] = df['futan_per_barei_log']
    
    # æœŸå¾…æ–¤é‡ã‹ã‚‰ã®å·®åˆ†ï¼ˆå¹´é½¢åˆ¥æœŸå¾…æ–¤é‡ã¨ã®å·®ï¼‰
    expected_weight_by_age = {2: 48, 3: 52, 4: 55, 5: 57, 6: 57, 7: 56, 8: 55}
    df['futan_deviation'] = df.apply(
        lambda row: row['futan_juryo'] - expected_weight_by_age.get(row['barei'], 55), 
        axis=1
    )
    X['futan_deviation'] = df['futan_deviation']
    
    # ğŸ´ é¦¬ä½“é‡é–¢é€£ã®ç‰¹å¾´é‡ (Phase 2: ç‰¹å¾´é‡å¼·åŒ–)
    # å‰èµ°ã‹ã‚‰ã®é¦¬ä½“é‡å¤‰åŒ–ç‡ (%)
    df['bataiju_change_rate'] = np.where(
        (df['bataiju_prev'].notna()) & (df['bataiju_prev'] > 0),
        (df['bataiju_current'] - df['bataiju_prev']) / df['bataiju_prev'] * 100,
        0
    )
    X['bataiju_change_rate'] = df['bataiju_change_rate']
    
    # å¹³å‡é¦¬ä½“é‡ã¨ã®å·®ã®æ¯”ç‡ (%)
    df['bataiju_deviation_rate'] = np.where(
        (df['bataiju_avg_3races'].notna()) & (df['bataiju_avg_3races'] > 0),
        (df['bataiju_current'] - df['bataiju_avg_3races']) / df['bataiju_avg_3races'] * 100,
        0
    )
    X['bataiju_deviation_rate'] = df['bataiju_deviation_rate']
    
    # é¦¬ä½“é‡å¢—æ¸›ã®çµ¶å¯¾å€¤ (kg) - DBç”±æ¥
    X['bataiju_change'] = df['bataiju_change'].fillna(0)
    
    # é¦¬ä½“é‡ãƒˆãƒ¬ãƒ³ãƒ‰ (å¢—åŠ å‚¾å‘/æ¸›å°‘å‚¾å‘)
    df['bataiju_trend'] = np.where(
        (df['bataiju_change_rate'].notna()),
        np.sign(df['bataiju_change_rate']) * np.log1p(abs(df['bataiju_change_rate'])),
        0
    )
    X['bataiju_trend'] = df['bataiju_trend']

    # é¦¬ç•ªÃ—è·é›¢ã®ç›¸äº’ä½œç”¨ï¼ˆå†…å¤–æ ã®è·é›¢é©æ€§ï¼‰
    df['umaban_kyori_interaction'] = df['umaban_numeric'] * df['kyori'] / 1000  # ã‚¹ã‚±ãƒ¼ãƒ«èª¿æ•´
    X['umaban_kyori_interaction'] = df['umaban_kyori_interaction']
    
    # çŸ­è·é›¢ç‰¹åŒ–ç‰¹å¾´é‡
    # æ ç•ªÃ—è·é›¢ã®ç›¸äº’ä½œç”¨ï¼ˆçŸ­è·é›¢ã»ã©å†…æ æœ‰åˆ©ã‚’æ•°å€¤åŒ–ï¼‰
    # è·é›¢ãŒçŸ­ã„ã»ã©æ ç•ªã®å½±éŸ¿ãŒå¤§ãã„: (2000 - è·é›¢) / 1000 ã§é‡ã¿ä»˜ã‘
    df['wakuban_kyori_interaction'] = df['wakuban'] * (2000 - df['kyori']) / 1000
    X['wakuban_kyori_interaction'] = df['wakuban_kyori_interaction']
    
    # 4. è¤‡æ•°ã®ãƒ”ãƒ¼ã‚¯å¹´é½¢ãƒ‘ã‚¿ãƒ¼ãƒ³
    # df['barei_peak_distance'] = abs(df['barei'] - 4)  # 4æ­³ã‚’ãƒ”ãƒ¼ã‚¯ã¨ä»®å®šï¼ˆæ—¢å­˜ï¼‰
    # X['barei_peak_distance'] = df['barei_peak_distance']
    
    # 3æ­³çŸ­è·é›¢ãƒ”ãƒ¼ã‚¯ï¼ˆæ—©ç†Ÿå‹ï¼‰
    # df['barei_peak_short'] = abs(df['barei'] - 3)
    # X['barei_peak_short'] = df['barei_peak_short']
    
    # # 5æ­³é•·è·é›¢ãƒ”ãƒ¼ã‚¯ï¼ˆæ™©æˆå‹ï¼‰
    # df['barei_peak_long'] = abs(df['barei'] - 5)
    # X['barei_peak_long'] = df['barei_peak_long']

    # 5. æ ç•ªãƒã‚¤ã‚¢ã‚¹ã‚¹ã‚³ã‚¢ï¼ˆæ ç•ªã®æ­´å²çš„å„ªä½æ€§ã‚’æ•°å€¤åŒ–ï¼‰
    # æ ç•ªåˆ¥ã®æ­´å²çš„ç€é †åˆ†å¸ƒã‚’è¨ˆç®—
    wakuban_stats = df.groupby('wakuban').agg({
        'kakutei_chakujun_numeric': ['mean', 'std', 'count']
    }).round(4)
    wakuban_stats.columns = ['waku_avg_rank', 'waku_std_rank', 'waku_count']
    wakuban_stats = wakuban_stats.reset_index()
    
    # å…¨ä½“å¹³å‡ã‹ã‚‰ã®åå·®ã§ãƒã‚¤ã‚¢ã‚¹ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
    overall_avg_rank = df['kakutei_chakujun_numeric'].mean()
    wakuban_stats['wakuban_bias_score'] = (overall_avg_rank - wakuban_stats['waku_avg_rank']) / wakuban_stats['waku_std_rank']
    wakuban_stats['wakuban_bias_score'] = wakuban_stats['wakuban_bias_score'].fillna(0)  # NaNã‚’0ã§åŸ‹ã‚ã‚‹
    
    # DataFrameã«ãƒãƒ¼ã‚¸
    df = df.merge(wakuban_stats[['wakuban', 'wakuban_bias_score']], on='wakuban', how='left')
    # X['wakuban_bias_score'] = df['wakuban_bias_score']

    # ãƒ¬ãƒ¼ã‚¹å†…ã§ã®é¦¬ç•ªç›¸å¯¾ä½ç½®ï¼ˆé ­æ•°ã«ã‚ˆã‚‹æ­£è¦åŒ–ï¼‰
    df['umaban_percentile'] = df.groupby(['kaisai_nen', 'kaisai_tsukihi', 'race_bango'])['umaban_numeric'].transform(
        lambda x: x.rank(pct=True)
    )
    X['umaban_percentile'] = df['umaban_percentile']
    
    # ç ”ç©¶ç”¨ç‰¹å¾´é‡ è¿½åŠ 
    # æ–¤é‡åå·®å€¤ï¼ˆãƒ¬ãƒ¼ã‚¹å†…ã§æ¨™æº–åŒ–ï¼‰
    # ãƒ¬ãƒ¼ã‚¹å†…ã®å¹³å‡ã¨æ¨™æº–åå·®ã‚’è¨ˆç®—ã—ã¦ã€å„é¦¬ã®æ–¤é‡ãŒã©ã‚Œãã‚‰ã„é‡ã„/è»½ã„ã‹ã‚’è¡¨ç¾
    race_group = df.groupby(['kaisai_nen', 'kaisai_tsukihi', 'race_bango'])['futan_juryo']
    df['futan_mean'] = race_group.transform('mean')
    df['futan_std'] = race_group.transform('std')
    
    # æ¨™æº–åå·®ãŒ0ã®å ´åˆï¼ˆå…¨é ­åŒã˜æ–¤é‡ï¼‰ã¯0ã«ã™ã‚‹
    df['futan_zscore'] = np.where(
        df['futan_std'] > 0,
        (df['futan_juryo'] - df['futan_mean']) / df['futan_std'],
        0
    )
    X['futan_zscore'] = df['futan_zscore']
    
    # ãƒ¬ãƒ¼ã‚¹å†…ã§ã®æ–¤é‡é †ä½ï¼ˆãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«ï¼‰
    # 0.0=æœ€è»½é‡ã€1.0=æœ€é‡é‡
    df['futan_percentile'] = race_group.transform(lambda x: x.rank(pct=True))
    X['futan_percentile'] = df['futan_percentile']

    # æ–°æ©Ÿèƒ½: è·é›¢é©æ€§ã‚¹ã‚³ã‚¢ã‚’è¿½åŠ ï¼ˆ3ç¨®é¡ï¼‰
    # model_creator.pyã¨åŒã˜å‡¦ç†ã‚’å®Ÿè¡Œ
    
    # è·é›¢ã‚«ãƒ†ã‚´ãƒªåˆ†é¡é–¢æ•°
    def categorize_distance(kyori):
        """è·é›¢ã‚’4ã‚«ãƒ†ã‚´ãƒªã«åˆ†é¡"""
        if kyori <= 1400:
            return 'short'  # çŸ­è·é›¢
        elif kyori <= 1800:
            return 'mile'   # ãƒã‚¤ãƒ«
        elif kyori <= 2400:
            return 'middle' # ä¸­è·é›¢
        else:
            return 'long'   # é•·è·é›¢
    
    # ä»Šå›ã®ãƒ¬ãƒ¼ã‚¹ã®è·é›¢ã‚«ãƒ†ã‚´ãƒªã‚’è¿½åŠ 
    df['distance_category'] = df['kyori'].apply(categorize_distance)
    
    # é‡è¦: é¦¬å ´æƒ…å ±ã‚‚å…ˆã«è¿½åŠ ï¼ˆdf_sortedã§ä½¿ã†ãŸã‚ï¼‰
    # èŠ/ãƒ€ãƒ¼ãƒˆåˆ†é¡é–¢æ•°
    def categorize_surface(track_code):
        """ãƒˆãƒ©ãƒƒã‚¯ã‚³ãƒ¼ãƒ‰ã‹ã‚‰èŠ/ãƒ€ãƒ¼ãƒˆã‚’åˆ¤å®š"""
        track_code_int = int(track_code)
        if 10 <= track_code_int <= 22:
            return 'turf'
        elif 23 <= track_code_int <= 24:
            return 'dirt'
        else:
            return 'unknown'
    
    # é¦¬å ´çŠ¶æ…‹åˆ†é¡é–¢æ•°
    def categorize_baba_condition(baba_code):
        """é¦¬å ´çŠ¶æ…‹ã‚³ãƒ¼ãƒ‰ã‚’åˆ†é¡"""
        if baba_code == 1:
            return 'good'
        elif baba_code == 2:
            return 'slightly'
        elif baba_code == 3:
            return 'heavy'
        elif baba_code == 4:
            return 'bad'
        else:
            return 'unknown'
    
    # ä»Šå›ã®ãƒ¬ãƒ¼ã‚¹ã®é¦¬å ´æƒ…å ±ã‚’è¿½åŠ 
    df['surface_type'] = df['track_code'].apply(categorize_surface)
    df['baba_condition'] = df['babajotai_code'].apply(categorize_baba_condition)
    
    # æ™‚ç³»åˆ—é †ã«ã‚½ãƒ¼ãƒˆï¼ˆé¦¬ã”ã¨ã«éå»ãƒ‡ãƒ¼ã‚¿ã‚’å‚ç…§ã™ã‚‹ãŸã‚ï¼‰
    df_sorted = df.sort_values(['ketto_toroku_bango', 'kaisai_nen', 'kaisai_tsukihi']).copy()
    
    # 1ï¸âƒ£ è·é›¢ã‚«ãƒ†ã‚´ãƒªåˆ¥é©æ€§ã‚¹ã‚³ã‚¢
    def calc_distance_category_score(group):
        scores = []
        for idx in range(len(group)):
            if idx == 0:
                scores.append(0.5)  # [OK] ä¿®æ­£: ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã¯ä¸­ç«‹å€¤
                continue
            
            current_category = group.iloc[idx]['distance_category']
            past_same_category = group.iloc[:idx][
                group.iloc[:idx]['distance_category'] == current_category
            ].tail(5)
            
            if len(past_same_category) > 0:
                avg_score = (1 - (past_same_category['kakutei_chakujun_numeric'] / 18.0)).mean()
                scores.append(avg_score)
            else:
                scores.append(0.5)  # [OK] ä¿®æ­£: ãƒ‡ãƒ¼ã‚¿ãªã—ã¯ä¸­ç«‹å€¤
        
        return pd.Series(scores, index=group.index)
    
    df_sorted['distance_category_score'] = df_sorted.groupby('ketto_toroku_bango', group_keys=False).apply(
        calc_distance_category_score
    ).values
    
    # 2ï¸âƒ£ è¿‘ä¼¼è·é›¢ã§ã®æˆç¸¾
    def calc_similar_distance_score(group):
        scores = []
        for idx in range(len(group)):
            if idx == 0:
                scores.append(0.5)  # [OK] ä¿®æ­£: ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã¯ä¸­ç«‹å€¤
                continue
            
            current_kyori = group.iloc[idx]['kyori']
            past_similar = group.iloc[:idx][
                abs(group.iloc[:idx]['kyori'] - current_kyori) <= 200
            ].tail(10)
            
            if len(past_similar) > 0:
                avg_score = (1 - (past_similar['kakutei_chakujun_numeric'] / 18.0)).mean()
                scores.append(avg_score)
            else:
                scores.append(0.5)  # [OK] ä¿®æ­£: ãƒ‡ãƒ¼ã‚¿ãªã—ã¯ä¸­ç«‹å€¤
        
        return pd.Series(scores, index=group.index)
    
    df_sorted['similar_distance_score'] = df_sorted.groupby('ketto_toroku_bango', group_keys=False).apply(
        calc_similar_distance_score
    ).values
    
    # 3ï¸âƒ£ è·é›¢å¤‰åŒ–å¯¾å¿œåŠ›
    def calc_distance_change_adaptability(group):
        scores = []
        for idx in range(len(group)):
            if idx < 2:
                scores.append(0.5)  # [OK] ä¿®æ­£: ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã¯ä¸­ç«‹å€¤
                continue
            
            # [OK] ä¿®æ­£: éå»6èµ°åˆ†ã‚’å–å¾—ï¼ˆå‰èµ°ã¨ã®å·®åˆ†ã‚’è¦‹ã‚‹ãŸã‚ï¼‰
            past_races = group.iloc[max(0, idx-6):idx].copy()
            
            if len(past_races) >= 3:  # [OK] ä¿®æ­£: æœ€ä½3èµ°å¿…è¦ï¼ˆå·®åˆ†2å€‹ï¼‰
                past_races['kyori_diff'] = past_races['kyori'].diff().abs()
                
                # [OK] ä¿®æ­£: æœ€æ–°5èµ°ã®ã¿ã‚’è©•ä¾¡ï¼ˆæœ€åˆã®1è¡Œã¯NaNãªã®ã§é™¤å¤–ï¼‰
                past_races_eval = past_races.tail(5)
                changed_races = past_races_eval[past_races_eval['kyori_diff'] >= 100]
                
                if len(changed_races) > 0:
                    avg_score = (1 - (changed_races['kakutei_chakujun_numeric'] / 18.0)).mean()
                    scores.append(avg_score)
                else:
                    scores.append(0.5)  # [OK] ä¿®æ­£: å¤‰åŒ–ãªã—ã¯ä¸­ç«‹
            else:
                scores.append(0.5)  # [OK] ä¿®æ­£: ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã¯ä¸­ç«‹å€¤
        
        return pd.Series(scores, index=group.index)
    
    df_sorted['distance_change_adaptability'] = df_sorted.groupby('ketto_toroku_bango', group_keys=False).apply(
        calc_distance_change_adaptability
    ).values
    
    # çŸ­è·é›¢ç‰¹åŒ–: å‰èµ°è·é›¢å·®ã‚’è¨ˆç®—
    def calc_zenso_kyori_sa(group):
        """å‰èµ°ã‹ã‚‰ã®è·é›¢å·®ã‚’è¨ˆç®—ï¼ˆçŸ­è·é›¢ã®è·é›¢å¤‰åŒ–å½±éŸ¿ã‚’è©•ä¾¡ï¼‰"""
        diffs = []
        for idx in range(len(group)):
            if idx == 0:
                diffs.append(0)  # åˆå›ã¯å‰èµ°ãªã—
            else:
                current_kyori = group.iloc[idx]['kyori']
                previous_kyori = group.iloc[idx-1]['kyori']
                diffs.append(abs(current_kyori - previous_kyori))
        return pd.Series(diffs, index=group.index)
    
    df_sorted['zenso_kyori_sa'] = df_sorted.groupby('ketto_toroku_bango', group_keys=False).apply(
        calc_zenso_kyori_sa
    ).values
    
    # [NEW] é•·è·é›¢çµŒé¨“å›æ•°ï¼ˆ2400mä»¥ä¸Šã®ãƒ¬ãƒ¼ã‚¹çµŒé¨“æ•°ï¼‰
    def calc_long_distance_experience_count(group):
        """é•·è·é›¢(2400mä»¥ä¸Š)ã®ãƒ¬ãƒ¼ã‚¹çµŒé¨“å›æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ"""
        counts = []
        for idx in range(len(group)):
            if idx == 0:
                counts.append(0)  # åˆå›ã¯çµŒé¨“ãªã—
            else:
                # éå»ã®ãƒ¬ãƒ¼ã‚¹ã§2400mä»¥ä¸Šã‚’èµ°ã£ãŸå›æ•°
                past_long_count = (group.iloc[:idx]['kyori'] >= 2400).sum()
                counts.append(past_long_count)
        return pd.Series(counts, index=group.index)
    
    df_sorted['long_distance_experience_count'] = df_sorted.groupby('ketto_toroku_bango', group_keys=False).apply(
        calc_long_distance_experience_count
    ).values
    
    # å…ƒã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹é †ã«æˆ»ã™
    df = df.copy()
    df['distance_category_score'] = df_sorted.sort_index()['distance_category_score']
    df['similar_distance_score'] = df_sorted.sort_index()['similar_distance_score']
    df['distance_change_adaptability'] = df_sorted.sort_index()['distance_change_adaptability']
    df['zenso_kyori_sa'] = df_sorted.sort_index()['zenso_kyori_sa']
    df['long_distance_experience_count'] = df_sorted.sort_index()['long_distance_experience_count']
    
    # ç‰¹å¾´é‡ã«è¿½åŠ 
    X['distance_category_score'] = df['distance_category_score']
    X['similar_distance_score'] = df['similar_distance_score']
    # X['distance_change_adaptability'] = df['distance_change_adaptability']
    X['zenso_kyori_sa'] = df['zenso_kyori_sa']
    X['long_distance_experience_count'] = df['long_distance_experience_count']

    # æ–°æ©Ÿèƒ½: ã‚¹ã‚¿ãƒ¼ãƒˆæŒ‡æ•°ã‚’è¿½åŠ ï¼ˆç¬¬1ã‚³ãƒ¼ãƒŠãƒ¼é€šéé †ä½ã‹ã‚‰ç®—å‡ºï¼‰
    if 'corner_1' in df.columns:
        print("[DONE] ã‚¹ã‚¿ãƒ¼ãƒˆæŒ‡æ•°ã‚’è¨ˆç®—ä¸­...")
        
        def calc_start_index(group):
            """
            éå»10èµ°ã®ç¬¬1ã‚³ãƒ¼ãƒŠãƒ¼é€šéé †ä½ã‹ã‚‰ã‚¹ã‚¿ãƒ¼ãƒˆèƒ½åŠ›ã‚’è©•ä¾¡
            - æ—©æœŸä½ç½®å–ã‚Šèƒ½åŠ›ï¼ˆé€šéé †ä½ãŒè‰¯ã„ = ã‚¹ã‚¿ãƒ¼ãƒˆè‰¯å¥½ï¼‰
            - ä¸€è²«æ€§ï¼ˆæ¨™æº–åå·®ãŒå°ã•ã„ = ã‚¹ã‚¿ãƒ¼ãƒˆå®‰å®šï¼‰
            """
            scores = []
            for idx in range(len(group)):
                if idx == 0:
                    scores.append(0.5)  # åˆå›ã¯ä¸­ç«‹å€¤
                    continue
                
                # éå»10èµ°ã®ç¬¬1ã‚³ãƒ¼ãƒŠãƒ¼é€šéé †ä½ã‚’å–å¾—ï¼ˆcorner_1ã¯æ—¢ã«æ•°å€¤åŒ–æ¸ˆã¿ï¼‰
                past_corners = group.iloc[max(0, idx-10):idx]['corner_1'].dropna()
                
                if len(past_corners) >= 3:  # æœ€ä½3èµ°å¿…è¦
                    avg_position = past_corners.mean()
                    std_position = past_corners.std()
                    
                    # ã‚¹ã‚³ã‚¢è¨ˆç®—: 
                    # 1. é€šéé †ä½ãŒè‰¯ã„ï¼ˆå°ã•ã„ï¼‰ã»ã©é«˜ã‚¹ã‚³ã‚¢ â†’ 1.0 - (avg_position / 18)
                    # 2. å®‰å®šæ€§ãƒœãƒ¼ãƒŠã‚¹: std ãŒå°ã•ã„ã»ã©é«˜è©•ä¾¡ â†’ æœ€å¤§0.2ã®ãƒœãƒ¼ãƒŠã‚¹
                    position_score = max(0, 1.0 - (avg_position / 18.0))
                    stability_bonus = max(0, 0.2 - (std_position / 10.0))
                    
                    total_score = position_score + stability_bonus
                    scores.append(min(1.0, total_score))  # æœ€å¤§1.0ã«ã‚¯ãƒªãƒƒãƒ—
                else:
                    scores.append(0.5)  # ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã¯ä¸­ç«‹å€¤
            
            return pd.Series(scores, index=group.index)
        
        df_sorted['start_index'] = df_sorted.groupby('ketto_toroku_bango', group_keys=False).apply(
            calc_start_index
        ).values
        
        df['start_index'] = df_sorted.sort_index()['start_index']
        X['start_index'] = df['start_index']
        
        print(f"[OK] ã‚¹ã‚¿ãƒ¼ãƒˆæŒ‡æ•°ã‚’è¿½åŠ ã—ã¾ã—ãŸï¼")
        print(f"  - start_index: éå»10èµ°ã®ç¬¬1ã‚³ãƒ¼ãƒŠãƒ¼é€šéé †ä½ã‹ã‚‰ç®—å‡ºï¼ˆæ—©æœŸä½ç½®å–ã‚Šèƒ½åŠ›+å®‰å®šæ€§ï¼‰")
    else:
        print("[!] corner_1ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ãªã„ãŸã‚ã€ã‚¹ã‚¿ãƒ¼ãƒˆæŒ‡æ•°ã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
        # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã§0.5ï¼ˆä¸­ç«‹å€¤ï¼‰ã‚’è¨­å®š
        df['start_index'] = 0.5
        X['start_index'] = 0.5
    
    # çŸ­è·é›¢ç‰¹åŒ–: ã‚³ãƒ¼ãƒŠãƒ¼é€šéä½ç½®ã‚¹ã‚³ã‚¢ï¼ˆå…¨ã‚³ãƒ¼ãƒŠãƒ¼ã®å¹³å‡ï¼‰
    if all(col in df.columns for col in ['corner_1', 'corner_2', 'corner_3', 'corner_4']):
        print("[DONE] ã‚³ãƒ¼ãƒŠãƒ¼é€šéä½ç½®ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ä¸­...")
        
        def calc_corner_position_score(group):
            """
            éå»3èµ°ã®å…¨ã‚³ãƒ¼ãƒŠãƒ¼(1-4)é€šéä½ç½®ã®å¹³å‡ã¨å®‰å®šæ€§ã‚’è¨ˆç®—
            - ä½ç½®å–ã‚ŠãŒè‰¯ã„(æ•°å€¤ãŒå°ã•ã„)ã»ã©é«˜ã‚¹ã‚³ã‚¢
            - å®‰å®šæ€§ã‚‚è©•ä¾¡ â†’ é¦¬é€£ãƒ»ãƒ¯ã‚¤ãƒ‰ã®ç²¾åº¦å‘ä¸Š
            """
            scores = []
            for idx in range(len(group)):
                if idx < 1:  # æœ€ä½1èµ°åˆ†ã®ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦
                    scores.append(0.5)
                    continue
                
                # éå»3èµ°ã‚’å–å¾—
                past_3_races = group.iloc[max(0, idx-2):idx+1]
                
                if len(past_3_races) >= 1:
                    # å„ãƒ¬ãƒ¼ã‚¹ã®å…¨ã‚³ãƒ¼ãƒŠãƒ¼å¹³å‡ä½ç½®ã‚’è¨ˆç®—
                    corner_averages = []
                    for _, race in past_3_races.iterrows():
                        corners = []
                        for corner_col in ['corner_1', 'corner_2', 'corner_3', 'corner_4']:
                            corner_val = race[corner_col]
                            if pd.notna(corner_val) and corner_val > 0:
                                corners.append(corner_val)
                        if len(corners) > 0:
                            corner_averages.append(np.mean(corners))
                    
                    if len(corner_averages) > 0:
                        avg_position = np.mean(corner_averages)
                        std_position = np.std(corner_averages) if len(corner_averages) > 1 else 0
                        
                        # ã‚¹ã‚³ã‚¢è¨ˆç®—:
                        # 1. ä½ç½®å–ã‚Šã‚¹ã‚³ã‚¢: å‰æ–¹ã»ã©é«˜è©•ä¾¡
                        position_score = max(0, 1.0 - (avg_position / 18.0))
                        
                        # 2. å®‰å®šæ€§ãƒœãƒ¼ãƒŠã‚¹: stdãŒå°ã•ã„ã»ã©é«˜è©•ä¾¡ (æœ€å¤§+0.3)
                        stability_bonus = max(0, 0.3 - (std_position / 10.0))
                        
                        # åˆè¨ˆã‚¹ã‚³ã‚¢ (æœ€å¤§1.0ã«ã‚¯ãƒªãƒƒãƒ—)
                        total_score = position_score + stability_bonus
                        scores.append(min(1.0, total_score))
                    else:
                        scores.append(0.5)
                else:
                    scores.append(0.5)
            
            return pd.Series(scores, index=group.index)
        
        df_sorted['corner_position_score'] = df_sorted.groupby('ketto_toroku_bango', group_keys=False).apply(
            calc_corner_position_score
        ).values
        
        df['corner_position_score'] = df_sorted.sort_index()['corner_position_score']
        X['corner_position_score'] = df['corner_position_score']
        
        print(f"[OK] ã‚³ãƒ¼ãƒŠãƒ¼é€šéä½ç½®ã‚¹ã‚³ã‚¢ã‚’è¿½åŠ ã—ã¾ã—ãŸï¼")
        print(f"  - corner_position_score: éå»3èµ°ã®å…¨ã‚³ãƒ¼ãƒŠãƒ¼(1-4)é€šéä½ç½®å¹³å‡+å®‰å®šæ€§ï¼ˆãƒã‚¸ã‚·ãƒ§ãƒ‹ãƒ³ã‚°èƒ½åŠ›+å®‰å®šæ€§ï¼‰")
    else:
        print("[!] corner_2~4ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ãªã„ãŸã‚ã€ã‚³ãƒ¼ãƒŠãƒ¼é€šéä½ç½®ã‚¹ã‚³ã‚¢ã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
        df['corner_position_score'] = 0.5
        X['corner_position_score'] = 0.5

    # æ–°æ©Ÿèƒ½: é¦¬å ´é©æ€§ã‚¹ã‚³ã‚¢ã‚’è¿½åŠ ï¼ˆ3ç¨®é¡ï¼‰
    # é¦¬å ´æƒ…å ±ã¯æ—¢ã«df_sortedã«å«ã¾ã‚Œã¦ã„ã‚‹ã®ã§ã€ãã®ã¾ã¾ä½¿ç”¨
    
    # 1ï¸âƒ£ èŠ/ãƒ€ãƒ¼ãƒˆåˆ¥é©æ€§ã‚¹ã‚³ã‚¢
    def calc_surface_score(group):
        scores = []
        for idx in range(len(group)):
            if idx == 0:
                scores.append(0.5)  # [OK] ä¿®æ­£: ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã¯ä¸­ç«‹å€¤
                continue
            
            current_surface = group.iloc[idx]['surface_type']
            past_same_surface = group.iloc[:idx][
                group.iloc[:idx]['surface_type'] == current_surface
            ].tail(10)
            
            if len(past_same_surface) > 0:
                avg_score = (1 - (past_same_surface['kakutei_chakujun_numeric'] / 18.0)).mean()
                scores.append(avg_score)
            else:
                scores.append(0.5)  # [OK] ä¿®æ­£: ãƒ‡ãƒ¼ã‚¿ãªã—ã¯ä¸­ç«‹å€¤
        
        return pd.Series(scores, index=group.index)
    
    df_sorted['surface_aptitude_score'] = df_sorted.groupby('ketto_toroku_bango', group_keys=False).apply(
        calc_surface_score
    ).values
    
    # 2ï¸âƒ£ é¦¬å ´çŠ¶æ…‹åˆ¥é©æ€§ã‚¹ã‚³ã‚¢
    def calc_baba_condition_score(group):
        scores = []
        for idx in range(len(group)):
            if idx == 0:
                scores.append(0.5)  # [OK] ä¿®æ­£: ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã¯ä¸­ç«‹å€¤
                continue
            
            current_condition = group.iloc[idx]['baba_condition']
            past_same_condition = group.iloc[:idx][
                group.iloc[:idx]['baba_condition'] == current_condition
            ].tail(10)
            
            if len(past_same_condition) > 0:
                avg_score = (1 - (past_same_condition['kakutei_chakujun_numeric'] / 18.0)).mean()
                scores.append(avg_score)
            else:
                scores.append(0.5)  # [OK] ä¿®æ­£: ãƒ‡ãƒ¼ã‚¿ãªã—ã¯ä¸­ç«‹å€¤
        
        return pd.Series(scores, index=group.index)
    
    df_sorted['baba_condition_score'] = df_sorted.groupby('ketto_toroku_bango', group_keys=False).apply(
        calc_baba_condition_score
    ).values
    
    # 3ï¸âƒ£ é¦¬å ´å¤‰åŒ–å¯¾å¿œåŠ›
    def calc_baba_change_adaptability(group):
        scores = []
        for idx in range(len(group)):
            if idx < 2:
                scores.append(0.5)  # [OK] ä¿®æ­£: ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã¯ä¸­ç«‹å€¤
                continue
            
            # [OK] ä¿®æ­£: éå»6èµ°åˆ†ã‚’å–å¾—ï¼ˆå‰èµ°ã¨ã®å¤‰åŒ–ã‚’è¦‹ã‚‹ãŸã‚ï¼‰
            past_races = group.iloc[max(0, idx-6):idx].copy()
            
            if len(past_races) >= 3:  # [OK] ä¿®æ­£: æœ€ä½3èµ°å¿…è¦
                past_races['baba_changed'] = past_races['baba_condition'].shift(1) != past_races['baba_condition']
                
                # [OK] ä¿®æ­£: æœ€æ–°5èµ°ã®ã¿ã‚’è©•ä¾¡
                past_races_eval = past_races.tail(5)
                changed_races = past_races_eval[past_races_eval['baba_changed'] == True]
                
                if len(changed_races) > 0:
                    avg_score = (1 - (changed_races['kakutei_chakujun_numeric'] / 18.0)).mean()
                    scores.append(avg_score)
                else:
                    scores.append(0.5)  # [OK] ä¿®æ­£: å¤‰åŒ–ãªã—ã¯ä¸­ç«‹
            else:
                scores.append(0.5)  # [OK] ä¿®æ­£: ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã¯ä¸­ç«‹å€¤
        
        return pd.Series(scores, index=group.index)
    
    df_sorted['baba_change_adaptability'] = df_sorted.groupby('ketto_toroku_bango', group_keys=False).apply(
        calc_baba_change_adaptability
    ).values
    
    # å…ƒã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹é †ã«æˆ»ã™
    df['surface_aptitude_score'] = df_sorted.sort_index()['surface_aptitude_score']
    df['baba_condition_score'] = df_sorted.sort_index()['baba_condition_score']
    df['baba_change_adaptability'] = df_sorted.sort_index()['baba_change_adaptability']
    
    # ç‰¹å¾´é‡ã«è¿½åŠ 
    X['surface_aptitude_score'] = df['surface_aptitude_score']
    # X['baba_condition_score'] = df['baba_condition_score']
    X['baba_change_adaptability'] = df['baba_change_adaptability']

    # æ–°æ©Ÿèƒ½: é¨æ‰‹ãƒ»èª¿æ•™å¸«ã®å‹•çš„èƒ½åŠ›ã‚¹ã‚³ã‚¢ã‚’è¿½åŠ ï¼ˆ4ç¨®é¡ï¼‰
    # model_creator.pyã¨å®Œå…¨ã«åŒã˜ãƒ­ã‚¸ãƒƒã‚¯
    
    # [OK] ä¿®æ­£: race_bangoã‚’è¿½åŠ ã—ã¦æ™‚ç³»åˆ—ãƒªãƒ¼ã‚¯ã‚’é˜²æ­¢
    df_sorted_kishu = df.sort_values(['kishu_code', 'kaisai_nen', 'kaisai_tsukihi', 'race_bango']).copy()
    
    # 1ï¸âƒ£ é¨æ‰‹ã®å®ŸåŠ›è£œæ­£ã‚¹ã‚³ã‚¢ï¼ˆæœŸå¾…ç€é †ã¨ã®å·®åˆ†ã€ç›´è¿‘3ãƒ¶æœˆï¼‰
    def calc_kishu_skill_adjusted_score(group):
        """é¨æ‰‹ã®ç´”ç²‹ãªæŠ€è¡“ã‚’è©•ä¾¡ï¼ˆé¦¬ã®å®ŸåŠ›ã‚’è£œæ­£ï¼‰"""
        scores = []
        
        for idx in range(len(group)):
            # é¨æ‰‹ã‚³ãƒ¼ãƒ‰ãŒãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            if pd.isna(group.iloc[idx]['kishu_code']) or group.iloc[idx]['kishu_code'] == '':
                scores.append(0.5)
                continue
                
            current_date = pd.to_datetime(
                str(int(group.iloc[idx]['kaisai_nen'])) + str(int(group.iloc[idx]['kaisai_tsukihi'])).zfill(4),
                format='%Y%m%d'
            )
            
            # 3ãƒ¶æœˆå‰ã®æ—¥ä»˜
            three_months_ago = current_date - pd.DateOffset(months=3)
            
            # éå»3ãƒ¶æœˆã®ãƒ¬ãƒ¼ã‚¹ã‚’æŠ½å‡ºï¼ˆæœªæ¥ã®ãƒ‡ãƒ¼ã‚¿ã¯è¦‹ãªã„ï¼ï¼‰
            past_races = group.iloc[:idx]
            
            if len(past_races) > 0:
                past_races = past_races.copy()
                past_races['kaisai_date'] = pd.to_datetime(
                    past_races['kaisai_nen'].astype(str) + past_races['kaisai_tsukihi'].astype(str).str.zfill(4),
                    format='%Y%m%d'
                )
                recent_races = past_races[past_races['kaisai_date'] >= three_months_ago]
                
                if len(recent_races) >= 3:  # æœ€ä½3ãƒ¬ãƒ¼ã‚¹å¿…è¦
                    # [OK] ä¿®æ­£: é¨æ‰‹ã®ç´”ç²‹ãªæˆç¸¾ã‚’è©•ä¾¡ï¼ˆé¦¬ã®å®ŸåŠ›è£œæ­£ã§ã¯ãªãã€é¨æ‰‹ã®å¹³å‡æˆç¸¾ï¼‰
                    # ç€é †ã‚’ã‚¹ã‚³ã‚¢åŒ–ï¼ˆ1ç€=1.0, 18ç€=0.0ï¼‰
                    recent_races['rank_score'] = 1.0 - ((18 - recent_races['kakutei_chakujun_numeric'] + 1) / 18.0)
                    
                    # é¨æ‰‹ã®å¹³å‡ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
                    avg_score = recent_races['rank_score'].mean()
                    
                    # 0-1ã®ç¯„å›²ã«ã‚¯ãƒªãƒƒãƒ—ï¼ˆæ—¢ã«ç¯„å›²å†…ã ãŒå¿µã®ãŸã‚ï¼‰
                    normalized_score = max(0.0, min(1.0, avg_score))
                    
                    scores.append(normalized_score)
                else:
                    scores.append(0.5)  # ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã¯ä¸­ç«‹
            else:
                scores.append(0.5)  # åˆå›ã¯ä¸­ç«‹
        
        return pd.Series(scores, index=group.index)
    
    df_sorted_kishu['kishu_skill_score'] = df_sorted_kishu.groupby('kishu_code', group_keys=False).apply(
        calc_kishu_skill_adjusted_score
    ).values
    
    # 2ï¸âƒ£ é¨æ‰‹ã®äººæ°—å·®ã‚¹ã‚³ã‚¢ï¼ˆã‚ªãƒƒã‚ºè£œæ­£ã€ç›´è¿‘3ãƒ¶æœˆï¼‰
    def calc_kishu_popularity_adjusted_score(group):
        """é¨æ‰‹ã®äººæ°—è£œæ­£ã‚¹ã‚³ã‚¢ï¼ˆäººæ°—ã‚ˆã‚Šä¸Šä½ã«æ¥ã‚Œã‚‹ã‹ï¼‰"""
        scores = []
        
        for idx in range(len(group)):
            if pd.isna(group.iloc[idx]['kishu_code']) or group.iloc[idx]['kishu_code'] == '':
                scores.append(0.5)
                continue
                
            current_date = pd.to_datetime(
                str(int(group.iloc[idx]['kaisai_nen'])) + str(int(group.iloc[idx]['kaisai_tsukihi'])).zfill(4),
                format='%Y%m%d'
            )
            
            three_months_ago = current_date - pd.DateOffset(months=3)
            
            past_races = group.iloc[:idx]
            
            if len(past_races) > 0:
                past_races = past_races.copy()
                past_races['kaisai_date'] = pd.to_datetime(
                    past_races['kaisai_nen'].astype(str) + past_races['kaisai_tsukihi'].astype(str).str.zfill(4),
                    format='%Y%m%d'
                )
                recent_races = past_races[past_races['kaisai_date'] >= three_months_ago]
                
                if len(recent_races) >= 3:
                    # ã‚ªãƒƒã‚ºãŒ0ã‚„ç•°å¸¸å€¤ã®å ´åˆã‚’é™¤å¤–
                    valid_races = recent_races[recent_races['tansho_odds'] > 0]
                    
                    if len(valid_races) >= 3:
                        # [OK] ä¿®æ­£: ã‚ªãƒƒã‚ºãƒ™ãƒ¼ã‚¹ã®æœŸå¾…æˆç¸¾ã¨å®Ÿéš›ã®æˆç¸¾ã‚’æ¯”è¼ƒ
                        # ã‚ªãƒƒã‚ºãŒä½ã„ = æœŸå¾…å€¤ãŒé«˜ã„ï¼ˆ1ã«è¿‘ã„ï¼‰
                        # ã‚ªãƒƒã‚ºãŒé«˜ã„ = æœŸå¾…å€¤ãŒä½ã„ï¼ˆ0ã«è¿‘ã„ï¼‰
                        max_odds = valid_races['tansho_odds'].max()
                        valid_races['odds_expectation'] = 1.0 - (valid_races['tansho_odds'] / (max_odds + 1.0))
                        
                        # å®Ÿéš›ã®æˆç¸¾ã‚¹ã‚³ã‚¢
                        valid_races['actual_score'] = 1.0 - ((18 - valid_races['kakutei_chakujun_numeric'] + 1) / 18.0)
                        
                        # æœŸå¾…ã‚’ä¸Šå›ã£ãŸåº¦åˆã„ï¼ˆãƒ—ãƒ©ã‚¹ãªã‚‰æœŸå¾…ä»¥ä¸Šï¼‰
                        valid_races['performance_diff'] = valid_races['actual_score'] - valid_races['odds_expectation']
                        
                        # å¹³å‡å·®åˆ†ã‚’ã‚¹ã‚³ã‚¢åŒ–ï¼ˆ0.5ãŒä¸­ç«‹ï¼‰
                        avg_diff = valid_races['performance_diff'].mean()
                        normalized_score = 0.5 + (avg_diff * 0.5)  # Â±0.5ã®ç¯„å›²ã«åã‚ã‚‹
                        normalized_score = max(0.0, min(1.0, normalized_score))
                        
                        scores.append(normalized_score)
                    else:
                        scores.append(0.5)
                else:
                    scores.append(0.5)
            else:
                scores.append(0.5)
        
        return pd.Series(scores, index=group.index)
    
    df_sorted_kishu['kishu_popularity_score'] = df_sorted_kishu.groupby('kishu_code', group_keys=False).apply(
        calc_kishu_popularity_adjusted_score
    ).values
    
    # 3ï¸âƒ£ é¨æ‰‹ã®èŠ/ãƒ€ãƒ¼ãƒˆåˆ¥ã‚¹ã‚³ã‚¢ï¼ˆé¦¬å ´é©æ€§è€ƒæ…®ã€ç›´è¿‘6ãƒ¶æœˆï¼‰
    def calc_kishu_surface_score(group):
        """é¨æ‰‹ã®é¦¬å ´ã‚¿ã‚¤ãƒ—åˆ¥ç›´è¿‘6ãƒ¶æœˆæˆç¸¾"""
        scores = []
        
        for idx in range(len(group)):
            if pd.isna(group.iloc[idx]['kishu_code']) or group.iloc[idx]['kishu_code'] == '':
                scores.append(0.5)
                continue
                
            current_date = pd.to_datetime(
                str(int(group.iloc[idx]['kaisai_nen'])) + str(int(group.iloc[idx]['kaisai_tsukihi'])).zfill(4),
                format='%Y%m%d'
            )
            current_surface = group.iloc[idx]['surface_type']
            
            six_months_ago = current_date - pd.DateOffset(months=6)
            
            past_races = group.iloc[:idx]
            
            if len(past_races) > 0:
                past_races = past_races.copy()
                past_races['kaisai_date'] = pd.to_datetime(
                    past_races['kaisai_nen'].astype(str) + past_races['kaisai_tsukihi'].astype(str).str.zfill(4),
                    format='%Y%m%d'
                )
                # åŒã˜é¦¬å ´ã‚¿ã‚¤ãƒ—ã§ã®ç›´è¿‘6ãƒ¶æœˆ
                recent_same_surface = past_races[
                    (past_races['kaisai_date'] >= six_months_ago) &
                    (past_races['surface_type'] == current_surface)
                ]
                
                if len(recent_same_surface) >= 5:  # æœ€ä½5ãƒ¬ãƒ¼ã‚¹å¿…è¦
                    avg_score = (1 - ((18 - recent_same_surface['kakutei_chakujun_numeric'] + 1) / 18.0)).mean()
                    scores.append(avg_score)
                else:
                    scores.append(0.5)
            else:
                scores.append(0.5)
        
        return pd.Series(scores, index=group.index)
    
    df_sorted_kishu['kishu_surface_score'] = df_sorted_kishu.groupby('kishu_code', group_keys=False).apply(
        calc_kishu_surface_score
    ).values
    
    # [OK] ä¿®æ­£: race_bangoã‚’è¿½åŠ ã—ã¦æ™‚ç³»åˆ—ãƒªãƒ¼ã‚¯ã‚’é˜²æ­¢
    df_sorted_chokyoshi = df.sort_values(['chokyoshi_code', 'kaisai_nen', 'kaisai_tsukihi', 'race_bango']).copy()
    
    # 4ï¸âƒ£ èª¿æ•™å¸«ã®ç›´è¿‘3ãƒ¶æœˆæˆç¸¾ã‚¹ã‚³ã‚¢
    def calc_chokyoshi_recent_score(group):
        """èª¿æ•™å¸«ã®ç›´è¿‘3ãƒ¶æœˆæˆç¸¾"""
        scores = []
        
        for idx in range(len(group)):
            if pd.isna(group.iloc[idx]['chokyoshi_code']) or group.iloc[idx]['chokyoshi_code'] == '':
                scores.append(0.5)
                continue
                
            current_date = pd.to_datetime(
                str(int(group.iloc[idx]['kaisai_nen'])) + str(int(group.iloc[idx]['kaisai_tsukihi'])).zfill(4),
                format='%Y%m%d'
            )
            
            three_months_ago = current_date - pd.DateOffset(months=3)
            
            past_races = group.iloc[:idx]
            
            if len(past_races) > 0:
                past_races = past_races.copy()
                past_races['kaisai_date'] = pd.to_datetime(
                    past_races['kaisai_nen'].astype(str) + past_races['kaisai_tsukihi'].astype(str).str.zfill(4),
                    format='%Y%m%d'
                )
                recent_races = past_races[past_races['kaisai_date'] >= three_months_ago]
                
                if len(recent_races) >= 5:  # [OK] ä¿®æ­£: 5ãƒ¬ãƒ¼ã‚¹ã«å¤‰æ›´ï¼ˆ10ãƒ¬ãƒ¼ã‚¹ã§ã¯å¤§éƒ¨åˆ†ãŒä¸­ç«‹å€¤ã«ãªã‚‹ï¼‰
                    avg_score = (1 - ((18 - recent_races['kakutei_chakujun_numeric'] + 1) / 18.0)).mean()
                    scores.append(avg_score)
                else:
                    scores.append(0.5)
            else:
                scores.append(0.5)
        
        return pd.Series(scores, index=group.index)
    
    df_sorted_chokyoshi['chokyoshi_recent_score'] = df_sorted_chokyoshi.groupby('chokyoshi_code', group_keys=False).apply(
        calc_chokyoshi_recent_score
    ).values
    
    # å…ƒã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹é †ã«æˆ»ã™
    df['kishu_skill_score'] = df_sorted_kishu.sort_index()['kishu_skill_score']
    df['kishu_popularity_score'] = df_sorted_kishu.sort_index()['kishu_popularity_score']
    df['kishu_surface_score'] = df_sorted_kishu.sort_index()['kishu_surface_score']
    df['chokyoshi_recent_score'] = df_sorted_chokyoshi.sort_index()['chokyoshi_recent_score']
    
    # ç‰¹å¾´é‡ã«è¿½åŠ 
    X['kishu_skill_score'] = df['kishu_skill_score']
    X['kishu_popularity_score'] = df['kishu_popularity_score']
    X['kishu_surface_score'] = df['kishu_surface_score']
    X['chokyoshi_recent_score'] = df['chokyoshi_recent_score']

    # éå»ãƒ¬ãƒ¼ã‚¹ã§ã€Œäººæ°—è–„ãªã®ã«å¥½èµ°ã—ãŸå›æ•°ã€
    # df['upset_count'] = df.groupby('ketto_toroku_bango').apply(
    #     lambda g: ((g['tansho_ninkijun_numeric'] >= 5) & (g['kakutei_chakujun_numeric'] <= 3)).sum()
    # )
    # X['upset_count'] = df['upset_count']

    # # ç ”ç©¶ç”¨ç‰¹å¾´é‡ è¿½åŠ 

    # ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã‚’ä½œæˆ
    # X['kyori'] = X['kyori'].astype('category')
    # X['tenko_code'] = X['tenko_code'].astype('category')
    # X['babajotai_code'] = X['babajotai_code'].astype('category')
    # X['seibetsu_code'] = X['seibetsu_code'].astype('category')

    # [TARGET] è·¯é¢Ã—è·é›¢åˆ¥ç‰¹å¾´é‡é¸æŠï¼ˆSHAPåˆ†æçµæœã«åŸºã¥ãæœ€é©åŒ–ï¼‰
    print(f"\n[RACE] è·¯é¢Ã—è·é›¢åˆ¥ç‰¹å¾´é‡é¸æŠã‚’å®Ÿæ–½...")
    print(f"  è·¯é¢: {surface_type}, è·é›¢: {min_distance}m ã€œ {max_distance}m")
    
    # è·¯é¢ã¨è·é›¢ã®çµ„ã¿åˆã‚ã›ã§ç‰¹å¾´é‡ã‚’èª¿æ•´
    is_turf = surface_type.lower() == 'turf'
    is_short = max_distance <= 1600
    is_long = min_distance >= 1700
    
    # çŸ­è·é›¢å°‚ç”¨ç‰¹å¾´é‡ã®è¿½åŠ 
    if is_short:
        print(f"  [TARGET] çŸ­è·é›¢ãƒ¢ãƒ‡ãƒ«: çŸ­è·é›¢ç‰¹åŒ–ç‰¹å¾´é‡ã‚’è¿½åŠ ")
        # wakuban_kyori_interaction, zenso_kyori_sa, start_index, corner_position_scoreã¯æ—¢ã«dfã¨Xã«è¿½åŠ æ¸ˆã¿
        # çŸ­è·é›¢ãƒ¢ãƒ‡ãƒ«ã§ã®ã¿ä½¿ç”¨ã™ã‚‹ãŸã‚ã€é•·è·é›¢ã§ã¯å‰Šé™¤ã™ã‚‹
        features_added_short = ['wakuban_kyori_interaction', 'zenso_kyori_sa', 'start_index', 'corner_position_score']
        print(f"    [OK] çŸ­è·é›¢ç‰¹åŒ–ç‰¹å¾´é‡: {features_added_short}")
        # é•·è·é›¢ç‰¹åŒ–ç‰¹å¾´é‡ã¯çŸ­è·é›¢ã§ã¯ä¸è¦
        if 'long_distance_experience_count' in X.columns:
            X = X.drop(columns=['long_distance_experience_count'])
            print(f"    [OK] å‰Šé™¤ï¼ˆçŸ­è·é›¢ç”¨ï¼‰: long_distance_experience_count")
    else:
        # é•·è·é›¢ãƒ»ä¸­è·é›¢ãƒ¢ãƒ‡ãƒ«ã§ã¯çŸ­è·é›¢ç‰¹åŒ–ç‰¹å¾´é‡ã‚’å‰Šé™¤
        print(f"  [PIN] ä¸­é•·è·é›¢ãƒ¢ãƒ‡ãƒ«: çŸ­è·é›¢ç‰¹åŒ–ç‰¹å¾´é‡ã‚’å‰Šé™¤")
        features_to_remove_for_long = ['wakuban_kyori_interaction', 'zenso_kyori_sa', 'start_index', 'corner_position_score']
        for feature in features_to_remove_for_long:
            if feature in X.columns:
                X = X.drop(columns=[feature])
                print(f"    [OK] å‰Šé™¤ï¼ˆé•·è·é›¢ç”¨ï¼‰: {feature}")
        # é•·è·é›¢(2200mä»¥ä¸Š)ã§ã¯long_distance_experience_countã‚’ä½¿ç”¨
        if min_distance >= 2200:
            print(f"  [TARGET] é•·è·é›¢ãƒ¢ãƒ‡ãƒ«: é•·è·é›¢ç‰¹åŒ–ç‰¹å¾´é‡ã‚’ä½¿ç”¨")
            print(f"    [OK] é•·è·é›¢ç‰¹åŒ–ç‰¹å¾´é‡: ['long_distance_experience_count']")
        else:
            # ä¸­è·é›¢ã§ã¯é•·è·é›¢ç‰¹åŒ–ç‰¹å¾´é‡ã¯ä¸è¦
            if 'long_distance_experience_count' in X.columns:
                X = X.drop(columns=['long_distance_experience_count'])
                print(f"    [OK] å‰Šé™¤ï¼ˆä¸­è·é›¢ç”¨ï¼‰: long_distance_experience_count")
    
    features_to_remove = []
    
    if is_turf and is_long:
        # ğŸŒ¿ èŠä¸­é•·è·é›¢ï¼ˆãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ï¼‰: å…¨ç‰¹å¾´é‡ã‚’ä½¿ç”¨
        print("  [PIN] èŠä¸­é•·è·é›¢ï¼ˆãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ï¼‰: å…¨ç‰¹å¾´é‡ã‚’ä½¿ç”¨")
        print(f"  [OK] ã“ã‚ŒãŒæœ€ã‚‚æˆåŠŸã—ã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«ã§ã™!")
    
    elif is_turf and is_short:
        # ğŸŒ¿ èŠçŸ­è·é›¢: SHAPåˆ†æã§åŠ¹æœãŒä½ã„ç‰¹å¾´é‡ã‚’å‰Šé™¤
        print("  [PIN] èŠçŸ­è·é›¢: ä¸è¦ãªç‰¹å¾´é‡ã‚’å‰Šé™¤")
        features_to_remove = [
            'kohan_3f_index',           # SHAP 0.030 â†’ å¾ŒåŠã®è„šã¯çŸ­è·é›¢ã§ã¯é‡è¦åº¦ä½ã„
            'surface_aptitude_score',   # SHAP 0.000 â†’ å®Œå…¨ã«ç„¡æ„å‘³
            'wakuban_ratio',            # SHAP 0.008 â†’ ã»ã¼ç„¡åŠ¹
        ]
    
    elif not is_turf and is_long:
        # ğŸœï¸ ãƒ€ãƒ¼ãƒˆä¸­é•·è·é›¢: èŠç‰¹æœ‰ã®ç‰¹å¾´é‡ã‚’èª¿æ•´
        print("  [PIN] ãƒ€ãƒ¼ãƒˆä¸­é•·è·é›¢: èŠç‰¹æœ‰ã®ç‰¹å¾´é‡ã‚’èª¿æ•´")
        # ãƒ€ãƒ¼ãƒˆã§ã¯èŠã¨ç•°ãªã‚‹ç‰¹æ€§ãŒã‚ã‚‹ãŸã‚ã€å¿…è¦ã«å¿œã˜ã¦ç‰¹å¾´é‡ã‚’èª¿æ•´
        # ç¾æ™‚ç‚¹ã§ã¯å…¨ç‰¹å¾´é‡ã‚’ä½¿ç”¨ï¼ˆä»Šå¾Œã®åˆ†æã§èª¿æ•´å¯èƒ½ï¼‰
        pass
    
    elif not is_turf and is_short:
        # ğŸœï¸ ãƒ€ãƒ¼ãƒˆçŸ­è·é›¢: èŠçŸ­è·é›¢ã®èª¿æ•´ + ãƒ€ãƒ¼ãƒˆç‰¹æœ‰ã®èª¿æ•´
        print("  [PIN] ãƒ€ãƒ¼ãƒˆçŸ­è·é›¢: èŠçŸ­è·é›¢+ãƒ€ãƒ¼ãƒˆç‰¹æœ‰ã®èª¿æ•´")
        features_to_remove = [
            'kohan_3f_index',           # çŸ­è·é›¢ã§ã¯å¾ŒåŠã®è„šã¯é‡è¦åº¦ä½ã„
            'surface_aptitude_score',   # èŠ/ãƒ€ãƒ¼ãƒˆé©æ€§ã‚¹ã‚³ã‚¢ã¯åŠ¹æœè–„
            'wakuban_ratio',            # ãƒ€ãƒ¼ãƒˆçŸ­è·é›¢ã§ã‚‚åŠ¹æœè–„ã„å¯èƒ½æ€§
        ]
    
    else:
        # ãƒã‚¤ãƒ«è·é›¢ãªã©ä¸­é–“
        print("  [PIN] ä¸­é–“è·é›¢ãƒ¢ãƒ‡ãƒ«: å…¨ç‰¹å¾´é‡ã‚’ä½¿ç”¨")
    
    # ç‰¹å¾´é‡ã®å‰Šé™¤å®Ÿè¡Œ
    if features_to_remove:
        print(f"  å‰Šé™¤ã™ã‚‹ç‰¹å¾´é‡: {features_to_remove}")
        for feature in features_to_remove:
            if feature in X.columns:
                X = X.drop(columns=[feature])
                print(f"    [OK] å‰Šé™¤: {feature}")
    
    print(f"  æœ€çµ‚ç‰¹å¾´é‡æ•°: {len(X.columns)}å€‹")
    print(f"  ç‰¹å¾´é‡ãƒªã‚¹ãƒˆ: {list(X.columns)}")

    # ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
    try:
        with open(model_filename, 'rb') as model_file:
            model = pickle.load(model_file)
    except FileNotFoundError:
        print(f"[ERROR] ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ« {model_filename} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return None, None, 0

    # ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¢æ•°ã‚’å®šç¾©
    def sigmoid(x):
        """å€¤ã‚’0-1ã®ç¯„å›²ã«åã‚ã‚‹ã‚ˆï½"""
        return 1 / (1 + np.exp(-x))

    # äºˆæ¸¬ã‚’å®Ÿè¡Œã—ã¦ã€ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¢æ•°ã§å¤‰æ›
    raw_scores = model.predict(X)
    df['predicted_chakujun_score'] = sigmoid(raw_scores)

    # ãƒ‡ãƒ¼ã‚¿ã‚’ã‚½ãƒ¼ãƒˆ
    df = df.sort_values(by=['kaisai_nen', 'kaisai_tsukihi', 'race_bango', 'umaban'], ascending=True)

    # ã‚°ãƒ«ãƒ¼ãƒ—å†…ã§ã®ã‚¹ã‚³ã‚¢é †ä½ã‚’è¨ˆç®—
    df['score_rank'] = df.groupby(['kaisai_nen', 'kaisai_tsukihi', 'race_bango'])['predicted_chakujun_score'].rank(method='min', ascending=False)

    # kakutei_chakujun_numeric ã¨ score_rank ã‚’æ•´æ•°ã«å¤‰æ›
    df['kakutei_chakujun_numeric'] = df['kakutei_chakujun_numeric'].fillna(0).astype(int)
    df['tansho_ninkijun_numeric'] = df['tansho_ninkijun_numeric'].fillna(0).astype(int)
    df['score_rank'] = df['score_rank'].fillna(0).astype(int)
    
    # surface_typeåˆ—ã‚’è¿½åŠ ï¼ˆèŠãƒ»ãƒ€ãƒ¼ãƒˆåŒºåˆ†ï¼‰
    from keiba_constants import get_surface_name
    df['surface_type_name'] = get_surface_name(surface_type)

    # å¿…è¦ãªåˆ—ã‚’é¸æŠ
    output_columns = ['keibajo_name',
                      'kaisai_nen', 
                      'kaisai_tsukihi', 
                      'race_bango',
                      'surface_type_name',
                      'kyori',
                      'umaban', 
                      'bamei', 
                      'tansho_odds', 
                      'tansho_ninkijun_numeric', 
                      'kakutei_chakujun_numeric', 
                      'score_rank', 
                      'predicted_chakujun_score',
                      'è¤‡å‹1ç€é¦¬ç•ª',
                      'è¤‡å‹1ç€ã‚ªãƒƒã‚º',
                      'è¤‡å‹1ç€äººæ°—',
                      'è¤‡å‹2ç€é¦¬ç•ª',
                      'è¤‡å‹2ç€ã‚ªãƒƒã‚º',
                      'è¤‡å‹2ç€äººæ°—',
                      'è¤‡å‹3ç€é¦¬ç•ª',
                      'è¤‡å‹3ç€ã‚ªãƒƒã‚º',
                      'è¤‡å‹3ç€äººæ°—',
                      'é¦¬é€£é¦¬ç•ª1',
                      'é¦¬é€£é¦¬ç•ª2',
                      'é¦¬é€£ã‚ªãƒƒã‚º',
                      'ãƒ¯ã‚¤ãƒ‰1_2é¦¬ç•ª1',
                      'ãƒ¯ã‚¤ãƒ‰1_2é¦¬ç•ª2',
                      'ãƒ¯ã‚¤ãƒ‰2_3ç€é¦¬ç•ª1',
                      'ãƒ¯ã‚¤ãƒ‰2_3ç€é¦¬ç•ª2',
                      'ãƒ¯ã‚¤ãƒ‰1_3ç€é¦¬ç•ª1',
                      'ãƒ¯ã‚¤ãƒ‰1_3ç€é¦¬ç•ª2',
                      'ãƒ¯ã‚¤ãƒ‰1_2ã‚ªãƒƒã‚º',
                      'ãƒ¯ã‚¤ãƒ‰2_3ã‚ªãƒƒã‚º',
                      'ãƒ¯ã‚¤ãƒ‰1_3ã‚ªãƒƒã‚º',
                      'é¦¬å˜é¦¬ç•ª1',
                      'é¦¬å˜é¦¬ç•ª2',
                      'é¦¬å˜ã‚ªãƒƒã‚º',
                      'ï¼“é€£è¤‡ã‚ªãƒƒã‚º',]
    output_df = df[output_columns]

    # åˆ—åã‚’å¤‰æ›´
    output_df = output_df.rename(columns={
        'keibajo_name': 'ç«¶é¦¬å ´',
        'kaisai_nen': 'é–‹å‚¬å¹´',
        'kaisai_tsukihi': 'é–‹å‚¬æ—¥',
        'race_bango': 'ãƒ¬ãƒ¼ã‚¹ç•ªå·',
        'surface_type_name': 'èŠãƒ€åŒºåˆ†',
        'kyori': 'è·é›¢',
        'umaban': 'é¦¬ç•ª',
        'bamei': 'é¦¬å',
        'tansho_odds': 'å˜å‹ã‚ªãƒƒã‚º',
        'tansho_ninkijun_numeric': 'äººæ°—é †',
        'kakutei_chakujun_numeric': 'ç¢ºå®šç€é †',
        'score_rank': 'äºˆæ¸¬é †ä½',
        'predicted_chakujun_score': 'äºˆæ¸¬ã‚¹ã‚³ã‚¢'
    })

    # æ­£ã—ã„ãƒ¬ãƒ¼ã‚¹æ•°ã®è¨ˆç®—æ–¹æ³•ã¯ã“ã‚Œï½ï¼
    race_count = len(output_df.groupby(['é–‹å‚¬å¹´', 'é–‹å‚¬æ—¥', 'ãƒ¬ãƒ¼ã‚¹ç•ªå·']))

    # çš„ä¸­ç‡ãƒ»å›åç‡è¨ˆç®—ï¼ˆå…ƒã®test.pyã‹ã‚‰ç§»æ¤ï¼‰
    # å˜å‹ã®çš„ä¸­ç‡ã¨å›åç‡
    tansho_hit = (output_df['ç¢ºå®šç€é †'] == 1) & (output_df['äºˆæ¸¬é †ä½'] == 1)
    tansho_hitrate = 100 * tansho_hit.sum() / race_count
    tansho_recoveryrate = 100 * (tansho_hit * output_df['å˜å‹ã‚ªãƒƒã‚º']).sum() / race_count

    # è¤‡å‹ã®çš„ä¸­ç‡ã¨å›åç‡
    fukusho_hit = (output_df['ç¢ºå®šç€é †'].isin([1, 2, 3])) & (output_df['äºˆæ¸¬é †ä½'].isin([1, 2, 3]))
    fukusho_hitrate = fukusho_hit.sum() / (race_count * 3) * 100

    # çš„ä¸­é¦¬ã ã‘å–ã‚Šå‡ºã™
    hit_rows = output_df[fukusho_hit].copy()

    def extract_odds(row):
        if row['ç¢ºå®šç€é †'] == 1:
            return row['è¤‡å‹1ç€ã‚ªãƒƒã‚º']
        elif row['ç¢ºå®šç€é †'] == 2:
            return row['è¤‡å‹2ç€ã‚ªãƒƒã‚º']
        elif row['ç¢ºå®šç€é †'] == 3:
            return row['è¤‡å‹3ç€ã‚ªãƒƒã‚º']
        else:
            return 0

    # çš„ä¸­é¦¬ã«å¯¾å¿œã™ã‚‹æ‰•æˆ»ã‚’è¨ˆç®—ï¼ˆ100å††è³­ã‘ãŸã¨ã—ã¦ï¼‰
    hit_rows['çš„ä¸­ã‚ªãƒƒã‚º'] = hit_rows.apply(extract_odds, axis=1)
    total_payout = (hit_rows['çš„ä¸­ã‚ªãƒƒã‚º'] * 100).sum()

    # ç·è³¼å…¥é¡ï¼ˆæ¯ãƒ¬ãƒ¼ã‚¹ã§3é ­ã«100å††ãšã¤ï¼‰
    total_bet = race_count * 3 * 100
    fukusho_recoveryrate = total_payout / total_bet * 100

    # é¦¬é€£ã®çš„ä¸­ç‡ã¨å›åç‡
    umaren_hit = output_df.groupby(['é–‹å‚¬å¹´', 'é–‹å‚¬æ—¥', 'ãƒ¬ãƒ¼ã‚¹ç•ªå·']).apply(
        lambda x: set([1, 2]).issubset(set(x.sort_values('äºˆæ¸¬ã‚¹ã‚³ã‚¢', ascending=False).head(2)['ç¢ºå®šç€é †'].values))
    )
    umaren_hitrate = 100 * umaren_hit.sum() / race_count
    umaren_recoveryrate = 100 * (umaren_hit * output_df.groupby(['é–‹å‚¬å¹´', 'é–‹å‚¬æ—¥', 'ãƒ¬ãƒ¼ã‚¹ç•ªå·'])['é¦¬é€£ã‚ªãƒƒã‚º'].first()).sum() / race_count

    # ãƒ¯ã‚¤ãƒ‰çš„ä¸­ç‡ãƒ»å›åç‡ã‚‚è¨ˆç®—ï¼ˆçœç•¥ã—ã¦ç°¡ç•¥åŒ–ï¼‰
    wide_hitrate = 0  # è¨ˆç®—ãŒè¤‡é›‘ãªã®ã§çœç•¥
    wide_recoveryrate = 0

    # é¦¬å˜ã®çš„ä¸­ç‡ã¨å›åç‡
    umatan_hit = output_df.groupby(['é–‹å‚¬å¹´', 'é–‹å‚¬æ—¥', 'ãƒ¬ãƒ¼ã‚¹ç•ªå·']).apply(
        lambda x: list(x.sort_values('äºˆæ¸¬ã‚¹ã‚³ã‚¢', ascending=False).head(2)['ç¢ºå®šç€é †'].values) == [1, 2]
    )
    umatan_hitrate = 100 * umatan_hit.sum() / race_count
    
    umatan_odds_sum = 0
    for name, race_group in output_df.groupby(['é–‹å‚¬å¹´', 'é–‹å‚¬æ—¥', 'ãƒ¬ãƒ¼ã‚¹ç•ªå·']):
        top_horses = race_group.sort_values('äºˆæ¸¬ã‚¹ã‚³ã‚¢', ascending=False).head(2)
        if list(top_horses['ç¢ºå®šç€é †'].values) == [1, 2]:
            umatan_odds_sum += race_group['é¦¬å˜ã‚ªãƒƒã‚º'].iloc[0]

    umatan_recoveryrate = 100 * umatan_odds_sum / race_count

    # ä¸‰é€£è¤‡ã®çš„ä¸­ç‡ã¨å›åç‡
    sanrenpuku_hit = output_df.groupby(['é–‹å‚¬å¹´', 'é–‹å‚¬æ—¥', 'ãƒ¬ãƒ¼ã‚¹ç•ªå·']).apply(
        lambda x: set([1, 2, 3]).issubset(set(x.sort_values('äºˆæ¸¬ã‚¹ã‚³ã‚¢', ascending=False).head(3)['ç¢ºå®šç€é †'].values))
    )
    sanrenpuku_hitrate = 100 * sanrenpuku_hit.sum() / len(sanrenpuku_hit)
    sanrenpuku_recoveryrate = 100 * (sanrenpuku_hit * output_df.groupby(['é–‹å‚¬å¹´', 'é–‹å‚¬æ—¥', 'ãƒ¬ãƒ¼ã‚¹ç•ªå·'])['ï¼“é€£è¤‡ã‚ªãƒƒã‚º'].first()).sum() / len(sanrenpuku_hit)

    # çµæœã‚’ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«ã¾ã¨ã‚ã‚‹
    summary_df = pd.DataFrame({
        'çš„ä¸­æ•°': [tansho_hit.sum(), fukusho_hit.sum(), umaren_hit.sum(), 0, umatan_hit.sum(), sanrenpuku_hit.sum()],
        'çš„ä¸­ç‡(%)': [tansho_hitrate, fukusho_hitrate, umaren_hitrate, wide_hitrate, umatan_hitrate, sanrenpuku_hitrate],
        'å›åç‡(%)': [tansho_recoveryrate, fukusho_recoveryrate, umaren_recoveryrate, wide_recoveryrate, umatan_recoveryrate, sanrenpuku_recoveryrate]
    }, index=['å˜å‹', 'è¤‡å‹', 'é¦¬é€£', 'ãƒ¯ã‚¤ãƒ‰', 'é¦¬å˜', 'ï¼“é€£è¤‡'])

    # Phase 1çµ±åˆ: æœŸå¾…å€¤ãƒ»ã‚±ãƒªãƒ¼åŸºæº–ãƒ»ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã‚’è¿½åŠ 
    print("[PHASE1] æ–°è³¼å…¥ãƒ­ã‚¸ãƒƒã‚¯(æœ¬å‘½Ã—äºˆæ¸¬ä¸Šä½ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼)ã‚’å®Ÿè¡Œä¸­...")
    try:
        output_df_with_logic = add_purchase_logic(
            output_df,
            prediction_rank_max=3,  # äºˆæ¸¬é †ä½1-3ä½
            popularity_rank_max=3,  # äººæ°—é †1-3ä½
            min_odds=1.5,  # æœ€ä½ã‚ªãƒƒã‚º1.5å€
            max_odds=20.0,  # æœ€é«˜ã‚ªãƒƒã‚º20å€
            min_score_diff=0.05,  # äºˆæ¸¬ã‚¹ã‚³ã‚¢å·®0.05ä»¥ä¸Š
            initial_bankroll=1000000,
            bet_unit=1000  # ä¸€å¾‹1000å††ãƒ™ãƒƒãƒˆ
        )
        print("[PHASE1] è³¼å…¥ãƒ­ã‚¸ãƒƒã‚¯çµ±åˆå®Œäº†!")
        
        # è³¼å…¥æ¨å¥¨é¦¬ã®çµ±è¨ˆ
        buy_count = output_df_with_logic['è³¼å…¥æ¨å¥¨'].sum()
        total_bet = output_df_with_logic['è³¼å…¥é¡'].sum()
        final_bankroll = output_df_with_logic['ç¾åœ¨è³‡é‡‘'].iloc[-1]
        
        # çš„ä¸­æ•°ã‚’è¨ˆç®—
        purchased = output_df_with_logic[output_df_with_logic['è³¼å…¥é¡'] > 0]
        wins = len(purchased[purchased['ç¢ºå®šç€é †'] == 1])
        hit_rate = (wins / len(purchased) * 100) if len(purchased) > 0 else 0
        
        print(f"[STATS] è³¼å…¥æ¨å¥¨é¦¬æ•°: {buy_count}")
        print(f"[STATS] å®Ÿè³¼å…¥é¦¬æ•°: {len(purchased)}")
        print(f"[STATS] çš„ä¸­æ•°: {wins}")
        print(f"[STATS] çš„ä¸­ç‡: {hit_rate:.2f}%")
        print(f"[STATS] ç·æŠ•è³‡é¡: {total_bet:,.0f}å††")
        print(f"[STATS] æœ€çµ‚è³‡é‡‘: {final_bankroll:,.0f}å†† (åˆæœŸ: 1,000,000å††)")
        print(f"[STATS] æç›Š: {final_bankroll - 1000000:+,.0f}å††")
        
        # å›åç‡ã‚’è¨ˆç®—
        if total_bet > 0:
            recovery_rate = (final_bankroll - 1000000 + total_bet) / total_bet * 100
            print(f"[STATS] å›åç‡: {recovery_rate:.2f}%")
        
        output_df = output_df_with_logic
    except Exception as e:
        print(f"[WARNING] Phase 1çµ±åˆã§ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
        print("[WARNING] å¾“æ¥ã®äºˆæ¸¬çµæœã®ã¿è¿”ã—ã¾ã™")
        import traceback
        traceback.print_exc()

    return output_df, summary_df, race_count


def test_multiple_models(test_year_start=2023, test_year_end=2023):
    """
    è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ†ã‚¹ãƒˆã—ã¦çµæœã‚’æ¯”è¼ƒã™ã‚‹é–¢æ•°(è¨­å®šã¯JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿)
    
    Args:
        test_year_start (int): ãƒ†ã‚¹ãƒˆå¯¾è±¡é–‹å§‹å¹´ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 2023)
        test_year_end (int): ãƒ†ã‚¹ãƒˆå¯¾è±¡çµ‚äº†å¹´ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 2023)
    """
    
    # JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å…¨ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’èª­ã¿è¾¼ã¿
    try:
        model_configs = get_all_models()
    except Exception as e:
        print(f"[ERROR] è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return
    
    if not model_configs:
        print("[!] ãƒ†ã‚¹ãƒˆå¯¾è±¡ã®ãƒ¢ãƒ‡ãƒ«è¨­å®šãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return
    
    print("[RACE] è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ãƒ†ã‚¹ãƒˆã‚’é–‹å§‹ã—ã¾ã™ï¼")
    print("=" * 60)
    
    all_results = {}
    # çµ±åˆãƒ•ã‚¡ã‚¤ãƒ«ã®åˆå›æ›¸ãè¾¼ã¿ãƒ•ãƒ©ã‚°
    first_unified_write = True
    
    for i, config in enumerate(model_configs, 1):
        base_model_filename = config['model_filename']
        description = config.get('description', f"ãƒ¢ãƒ‡ãƒ«{i}")
        
        print(f"\nã€{i}/{len(model_configs)}ã€‘ {description} ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ†ã‚¹ãƒˆä¸­...")
        
        # å¹´ç¯„å›²ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
        # ä¾‹: tokyo_turf_3ageup_long_2020-2022.sav
        import glob
        base_name = base_model_filename.replace('.sav', '')
        model_pattern = f"models/{base_name}_*-*.sav"
        matching_models = glob.glob(model_pattern)
        
        # ãƒãƒƒãƒã™ã‚‹ãƒ¢ãƒ‡ãƒ«ãŒãªã‘ã‚Œã°å…ƒã®ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ä½¿ç”¨
        if not matching_models:
            model_filename = base_model_filename
            train_year_range = "unknown"
        else:
            # æœ€æ–°ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åã§ã‚½ãƒ¼ãƒˆï¼‰
            model_filename = sorted(matching_models)[-1]
            # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰å­¦ç¿’æœŸé–“ã‚’æŠ½å‡º
            import re
            match = re.search(r'_(\d{4})-(\d{4})\.sav$', model_filename)
            if match:
                train_year_range = f"{match.group(1)}-{match.group(2)}"
            else:
                train_year_range = "unknown"
        
        print(f"[FILE] ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«: {model_filename}")
        if train_year_range != "unknown":
            print(f"[RUN] å­¦ç¿’æœŸé–“: {train_year_range}")
        
        # ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
        model_path = model_filename
        if not os.path.exists(model_path):
            # modelsãƒ•ã‚©ãƒ«ãƒ€ã‚‚ç¢ºèª
            models_path = f"models/{base_model_filename}"
            if os.path.exists(models_path):
                model_path = models_path
                train_year_range = "unknown"
                print(f"[DIR] modelsãƒ•ã‚©ãƒ«ãƒ€å†…ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨: {models_path}")
            else:
                print(f"[!] ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                continue
        
        try:
            output_df, summary_df, race_count = predict_with_model(
                model_filename=model_path,  # å­˜åœ¨ç¢ºèªæ¸ˆã¿ã®ãƒ‘ã‚¹ã‚’ä½¿ç”¨
                track_code=config['track_code'],
                kyoso_shubetsu_code=config['kyoso_shubetsu_code'],
                surface_type=config['surface_type'],
                min_distance=config['min_distance'],
                max_distance=config['max_distance'],
                test_year_start=test_year_start,
                test_year_end=test_year_end
            )
            
            if output_df is not None:
                # çµæœãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆï¼ˆå­¦ç¿’æœŸé–“ã¨ãƒ†ã‚¹ãƒˆå¹´ã‚’å«ã‚ã‚‹ï¼‰
                base_filename = base_model_filename.replace('.sav', '')
                test_year_str = f"{test_year_start}-{test_year_end}" if test_year_start != test_year_end else str(test_year_start)
                individual_output_file = f"predicted_results_{base_filename}_train{train_year_range}_test{test_year_str}.tsv"
                summary_file = f"betting_summary_{base_filename}_train{train_year_range}_test{test_year_str}.tsv"
                
                # å€‹åˆ¥ãƒ¢ãƒ‡ãƒ«çµæœã‚’ä¸Šæ›¸ãä¿å­˜ï¼ˆè¿½è¨˜ã§ã¯ãªãä¸Šæ›¸ãï¼‰
                save_results_with_append(output_df, individual_output_file, append_mode=False)
                
                # å…¨ãƒ¢ãƒ‡ãƒ«çµ±åˆãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ï¼ˆåˆå›ã¯ä¸Šæ›¸ãã€ä»¥é™ã¯è¿½è¨˜ï¼‰
                unified_output_file = "predicted_results.tsv"
                save_results_with_append(output_df, unified_output_file, append_mode=not first_unified_write)
                first_unified_write = False  # åˆå›æ›¸ãè¾¼ã¿å®Œäº†
                
                # ã‚µãƒãƒªãƒ¼ã¯å€‹åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
                results_dir = Path('results')
                results_dir.mkdir(exist_ok=True)
                summary_filepath = results_dir / summary_file
                summary_df.to_csv(summary_filepath, index=True, sep='\t', encoding='utf-8-sig')
                
                print(f"[OK] å®Œäº†ï¼ãƒ¬ãƒ¼ã‚¹æ•°: {race_count}")
                print(f"  - å€‹åˆ¥çµæœ: {individual_output_file}")
                print(f"  - çµ±åˆçµæœ: {unified_output_file}")
                print(f"  - ã‚µãƒãƒªãƒ¼: {summary_file}")
                
                # çµæœã‚’ä¿å­˜ï¼ˆå¾Œã§æ¯”è¼ƒç”¨ï¼‰
                all_results[description] = {
                    'summary': summary_df,
                    'race_count': race_count,
                    'model_filename': model_filename
                }
                
                # ä¸»è¦ãªçµæœã‚’è¡¨ç¤º
                print(f"  - å˜å‹çš„ä¸­ç‡: {summary_df.loc['å˜å‹', 'çš„ä¸­ç‡(%)']:.2f}%")
                print(f"  - å˜å‹å›åç‡: {summary_df.loc['å˜å‹', 'å›åç‡(%)']:.2f}%")
                print(f"  - è¤‡å‹çš„ä¸­ç‡: {summary_df.loc['è¤‡å‹', 'çš„ä¸­ç‡(%)']:.2f}%")
                print(f"  - è¤‡å‹å›åç‡: {summary_df.loc['è¤‡å‹', 'å›åç‡(%)']:.2f}%")
                
            else:
                print(f"[ERROR] ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                
        except Exception as e:
            print(f"[ERROR] ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            import traceback
            traceback.print_exc()
        
        print("-" * 60)
    
    # è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®æ¯”è¼ƒçµæœã‚’ä½œæˆ
    if len(all_results) > 1:
        print("\n[+] ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒçµæœ")
        print("=" * 60)
        
        comparison_data = []
        for description, result in all_results.items():
            summary = result['summary']
            comparison_data.append({
                'ãƒ¢ãƒ‡ãƒ«': description,
                'ãƒ¬ãƒ¼ã‚¹æ•°': result['race_count'],
                'å˜å‹çš„ä¸­ç‡': f"{summary.loc['å˜å‹', 'çš„ä¸­ç‡(%)']:.2f}%",
                'å˜å‹å›åç‡': f"{summary.loc['å˜å‹', 'å›åç‡(%)']:.2f}%",
                'è¤‡å‹çš„ä¸­ç‡': f"{summary.loc['è¤‡å‹', 'çš„ä¸­ç‡(%)']:.2f}%",
                'è¤‡å‹å›åç‡': f"{summary.loc['è¤‡å‹', 'å›åç‡(%)']:.2f}%",
                'ä¸‰é€£è¤‡çš„ä¸­ç‡': f"{summary.loc['ï¼“é€£è¤‡', 'çš„ä¸­ç‡(%)']:.2f}%",
                'ä¸‰é€£è¤‡å›åç‡': f"{summary.loc['ï¼“é€£è¤‡', 'å›åç‡(%)']:.2f}%"
            })
        
        comparison_df = pd.DataFrame(comparison_data)
        
        # æ¯”è¼ƒçµæœã‚’ä¿å­˜
        comparison_file = 'model_comparison.tsv'
        results_dir = Path('results')
        results_dir.mkdir(exist_ok=True)
        comparison_filepath = results_dir / comparison_file
        
        comparison_df.to_csv(comparison_filepath, index=False, sep='\t', encoding='utf-8-sig')
        
        print(comparison_df.to_string(index=False))
        print(f"\n[LIST] æ¯”è¼ƒçµæœã‚’ {comparison_filepath} ã«ä¿å­˜ã—ã¾ã—ãŸï¼")
    
    print("\n[DONE] ã™ã¹ã¦ã®ãƒ†ã‚¹ãƒˆãŒå®Œäº†ã—ã¾ã—ãŸï¼")


def predict_and_save_results():
    """
    æ—§ãƒãƒ¼ã‚¸ãƒ§ãƒ³äº’æ›æ€§ã®ãŸã‚ã®é–¢æ•°
    é˜ªç¥ç«¶é¦¬å ´ã®ï¼“æ­³ä»¥ä¸ŠèŠä¸­é•·è·é›¢ãƒ¢ãƒ‡ãƒ«ã§ãƒ†ã‚¹ãƒˆ
    """
    output_df, summary_df, race_count = predict_with_model(
        model_filename='hanshin_shiba_3ageup_model.sav',
        track_code='09',  # é˜ªç¥
        kyoso_shubetsu_code='13',  # 3æ­³ä»¥ä¸Š
        surface_type='turf',  # èŠ
        min_distance=1700,  # ä¸­é•·è·é›¢
        max_distance=9999  # ä¸Šé™ãªã—
    )
    
    if output_df is not None:
        # resultsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
        results_dir = Path('results')
        results_dir.mkdir(exist_ok=True)
        
        # çµæœã‚’TSVã«ä¿å­˜ï¼ˆè¿½è¨˜ãƒ¢ãƒ¼ãƒ‰ï¼‰
        output_file = 'predicted_results.tsv'
        save_results_with_append(output_df, output_file, append_mode=True)
        print(f"äºˆæ¸¬çµæœã‚’ results/{output_file} ã«ä¿å­˜ã—ã¾ã—ãŸï¼")

        # çš„ä¸­ç‡ã¨å›åç‡ã‚’åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        summary_file = 'betting_summary.tsv'
        summary_filepath = results_dir / summary_file
        summary_df.to_csv(summary_filepath, index=True, sep='\t', encoding='utf-8-sig')
        print(f"çš„ä¸­ç‡ãƒ»å›åç‡ãƒ»çš„ä¸­æ•°ã‚’ results/{summary_file} ã«ä¿å­˜ã—ã¾ã—ãŸï¼")


if __name__ == '__main__':
    # å®Ÿè¡Œæ–¹æ³•ã‚’é¸æŠã§ãã‚‹ã‚ˆã†ã«
    import sys
    
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ†ã‚¹ãƒˆå¹´ç¯„å›²
    test_year_start = 2023
    test_year_end = 2023
    
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‚’è§£æ
    mode = 'single'  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯å˜ä¸€ãƒ¢ãƒ‡ãƒ«ãƒ†ã‚¹ãƒˆ
    
    for arg in sys.argv[1:]:
        if arg == 'multi':
            mode = 'multi'
        elif '-' in arg and arg[0].isdigit():
            # "2020-2023" å½¢å¼ã®å¹´ç¯„å›²æŒ‡å®š
            try:
                years = arg.split('-')
                if len(years) == 2:
                    test_year_start = int(years[0])
                    test_year_end = int(years[1])
                    print(f"[DATE] ãƒ†ã‚¹ãƒˆå¹´ç¯„å›²æŒ‡å®š: {test_year_start}å¹´~{test_year_end}å¹´")
            except ValueError:
                print(f"[!] ç„¡åŠ¹ãªå¹´ç¯„å›²ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ: {arg} (ä¾‹: 2020-2023)")
        elif arg.isdigit() and len(arg) == 4:
            # "2023" å½¢å¼ã®å˜ä¸€å¹´æŒ‡å®š
            test_year_start = test_year_end = int(arg)
            print(f"[DATE] ãƒ†ã‚¹ãƒˆå¹´æŒ‡å®š: {test_year_start}å¹´")
    
    if mode == 'multi':
        # python universal_test.py multi [å¹´ç¯„å›²]
        test_multiple_models(test_year_start=test_year_start, test_year_end=test_year_end)
    else:
        # python universal_test.py [å¹´ç¯„å›²] (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ)
        # å˜ä¸€ãƒ¢ãƒ‡ãƒ«ãƒ†ã‚¹ãƒˆã§å¹´ç¯„å›²ã‚’ä½¿ç”¨
        output_df, summary_df, race_count = predict_with_model(
            model_filename='hanshin_shiba_3ageup_model.sav',
            track_code='09',  # é˜ªç¥
            kyoso_shubetsu_code='13',  # 3æ­³ä»¥ä¸Š
            surface_type='turf',  # èŠ
            min_distance=1700,  # ä¸­é•·è·é›¢
            max_distance=9999,  # ä¸Šé™ãªã—
            test_year_start=test_year_start,
            test_year_end=test_year_end
        )
        
        if output_df is not None:
            # resultsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
            results_dir = Path('results')
            results_dir.mkdir(exist_ok=True)
            
            # çµæœã‚’TSVã«ä¿å­˜ï¼ˆè¿½è¨˜ãƒ¢ãƒ¼ãƒ‰ï¼‰
            output_file = 'predicted_results.tsv'
            save_results_with_append(output_df, output_file, append_mode=True)
            print(f"äºˆæ¸¬çµæœã‚’ results/{output_file} ã«ä¿å­˜ã—ã¾ã—ãŸ!")

            # çš„ä¸­ç‡ã¨å›åç‡ã‚’åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
            summary_file = 'betting_summary.tsv'
            summary_filepath = results_dir / summary_file
            summary_df.to_csv(summary_filepath, index=True, sep='\t', encoding='utf-8-sig')
            print(f"çš„ä¸­ç‡ãƒ»å›åç‡ãƒ»çš„ä¸­æ•°ã‚’ results/{summary_file} ã«ä¿å­˜ã—ã¾ã—ãŸ!")