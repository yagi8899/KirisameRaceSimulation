#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ±ç”¨ç«¶é¦¬äºˆæ¸¬ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ã€è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã«å¯¾å¿œã—ãŸç«¶é¦¬äºˆæ¸¬ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
model_creator.pyã§ä½œæˆã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦äºˆæ¸¬ã‚’è¡Œã„ã€çµæœã‚’ä¿å­˜ã—ã¾ã™ã€‚
"""

import psycopg2
import pandas as pd
import pickle
import lightgbm as lgb
import numpy as np
import os
from pathlib import Path
from datetime import datetime
from keiba_constants import get_track_name, format_model_description
from model_config_loader import get_all_models, get_legacy_model


def save_results_with_append(df, filename, append_mode=True, output_dir='results'):
    """
    çµæœã‚’TSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ï¼ˆè¿½è¨˜ãƒ¢ãƒ¼ãƒ‰å¯¾å¿œï¼‰
    
    Args:
        df (DataFrame): ä¿å­˜ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        filename (str): ä¿å­˜å…ˆãƒ•ã‚¡ã‚¤ãƒ«å
        append_mode (bool): True=è¿½è¨˜ãƒ¢ãƒ¼ãƒ‰ã€False=ä¸Šæ›¸ããƒ¢ãƒ¼ãƒ‰
        output_dir (str): å‡ºåŠ›å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 'results'ï¼‰
    """
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆï¼ˆå­˜åœ¨ã—ãªã„å ´åˆï¼‰
    output_path = Path(output_dir)
    output_path.mkdir(exist_ok=True)
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’ä½œæˆ
    filepath = output_path / filename
    
    if append_mode and filepath.exists():
        # ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ—¢ã«å­˜åœ¨ã™ã‚‹å ´åˆã¯è¿½è¨˜ï¼ˆãƒ˜ãƒƒãƒ€ãƒ¼ãªã—ï¼‰
        print(f"ğŸ“ æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã«è¿½è¨˜: {filepath}")
        df.to_csv(filepath, mode='a', header=False, index=False, sep='\t', encoding='utf-8-sig')
    else:
        # ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯æ–°è¦ä½œæˆï¼ˆãƒ˜ãƒƒãƒ€ãƒ¼ã‚ã‚Šï¼‰
        print(f"ğŸ“‹ æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ: {filepath}")
        df.to_csv(filepath, index=False, sep='\t', encoding='utf-8-sig')


def predict_with_model(model_filename, track_code, kyoso_shubetsu_code, surface_type, 
                      min_distance, max_distance, test_year=2023):
    """
    æŒ‡å®šã—ãŸãƒ¢ãƒ‡ãƒ«ã§äºˆæ¸¬ã‚’å®Ÿè¡Œã™ã‚‹æ±ç”¨é–¢æ•°
    
    Args:
        model_filename (str): ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«å
        track_code (str): ç«¶é¦¬å ´ã‚³ãƒ¼ãƒ‰
        kyoso_shubetsu_code (str): ç«¶äº‰ç¨®åˆ¥ã‚³ãƒ¼ãƒ‰
        surface_type (str): 'turf' or 'dirt'
        min_distance (int): æœ€å°è·é›¢
        max_distance (int): æœ€å¤§è·é›¢
        test_year (int): ãƒ†ã‚¹ãƒˆå¯¾è±¡å¹´
        
    Returns:
        tuple: (äºˆæ¸¬çµæœDataFrame, ã‚µãƒãƒªãƒ¼DataFrame, ãƒ¬ãƒ¼ã‚¹æ•°)
    """
    
    # PostgreSQL ã‚³ãƒã‚¯ã‚·ãƒ§ãƒ³ã®ä½œæˆ
    conn = psycopg2.connect(
        host='localhost',
        port='5432',
        user='postgres',
        password='ahtaht88',
        dbname='keiba'
    )

    # ãƒˆãƒ©ãƒƒã‚¯æ¡ä»¶ã‚’å‹•çš„ã«è¨­å®š
    if surface_type.lower() == 'turf':
        # èŠã®å ´åˆ
        track_condition = "cast(rase.track_code as integer) between 10 and 22"
        baba_condition = "ra.babajotai_code_shiba"
    else:
        # ãƒ€ãƒ¼ãƒˆã®å ´åˆ
        track_condition = "cast(rase.track_code as integer) between 23 and 29"
        baba_condition = "ra.babajotai_code_dirt"

    # è·é›¢æ¡ä»¶ã‚’è¨­å®š
    if max_distance == 9999:
        distance_condition = f"cast(rase.kyori as integer) >= {min_distance}"
    else:
        distance_condition = f"cast(rase.kyori as integer) between {min_distance} and {max_distance}"

    # ç«¶äº‰ç¨®åˆ¥ã‚’è¨­å®š
    if kyoso_shubetsu_code == '12':
        # 3æ­³æˆ¦
        kyoso_shubetsu_condition = "cast(rase.kyoso_shubetsu_code as integer) = 12"
    elif kyoso_shubetsu_code == '13':
        kyoso_shubetsu_condition = "cast(rase.kyoso_shubetsu_code as integer) >= 13"

    # SQLã‚¯ã‚¨ãƒªã‚’å‹•çš„ã«ç”Ÿæˆ
    sql = f"""
    select * from (
        select
        ra.kaisai_nen,
        ra.kaisai_tsukihi,
        ra.race_bango,
        seum.umaban,
        seum.bamei,
        ra.keibajo_code,
        CASE 
            WHEN ra.keibajo_code = '01' THEN 'æœ­å¹Œ' 
            WHEN ra.keibajo_code = '02' THEN 'å‡½é¤¨' 
            WHEN ra.keibajo_code = '03' THEN 'ç¦å³¶' 
            WHEN ra.keibajo_code = '04' THEN 'æ–°æ½Ÿ' 
            WHEN ra.keibajo_code = '05' THEN 'æ±äº¬' 
            WHEN ra.keibajo_code = '06' THEN 'ä¸­å±±' 
            WHEN ra.keibajo_code = '07' THEN 'ä¸­äº¬' 
            WHEN ra.keibajo_code = '08' THEN 'äº¬éƒ½' 
            WHEN ra.keibajo_code = '09' THEN 'é˜ªç¥' 
            WHEN ra.keibajo_code = '10' THEN 'å°å€‰' 
            ELSE '' 
        END keibajo_name,
        ra.kyori,
        ra.tenko_code,
        {baba_condition} as babajotai_code,
        ra.grade_code,
        ra.kyoso_joken_code,
        ra.kyoso_shubetsu_code,
        ra.track_code,
        seum.ketto_toroku_bango,
        seum.wakuban,
        cast(seum.umaban as integer) as umaban_numeric,
        seum.barei,
        seum.kishu_code,
        seum.chokyoshi_code,
        seum.futan_juryo,
        seum.seibetsu_code,
        CASE WHEN seum.seibetsu_code = '1' THEN '1' ELSE '0' END AS mare_horse,
        CASE WHEN seum.seibetsu_code = '2' THEN '1' ELSE '0' END AS femare_horse,
        CASE WHEN seum.seibetsu_code = '3' THEN '1' ELSE '0' END AS sen_horse,
        CASE WHEN {baba_condition} = '1' THEN '1' ELSE '0' END AS baba_good,
        CASE WHEN {baba_condition} = '2' THEN '1' ELSE '0' END AS baba_slightly_heavy,
        CASE WHEN {baba_condition} = '3' THEN '1' ELSE '0' END AS baba_heavy,
        CASE WHEN {baba_condition} = '4' THEN '1' ELSE '0' END AS baba_defective,
        CASE WHEN ra.tenko_code = '1' THEN '1' ELSE '0' END AS tenko_fine,
        CASE WHEN ra.tenko_code = '2' THEN '1' ELSE '0' END AS tenko_cloudy,
        CASE WHEN ra.tenko_code = '3' THEN '1' ELSE '0' END AS tenko_rainy,
        CASE WHEN ra.tenko_code = '4' THEN '1' ELSE '0' END AS tenko_drizzle,
        CASE WHEN ra.tenko_code = '5' THEN '1' ELSE '0' END AS tenko_snow,
        CASE WHEN ra.tenko_code = '6' THEN '1' ELSE '0' END AS tenko_light_snow,
        nullif(cast(seum.tansho_odds as float), 0) / 10 as tansho_odds,
        nullif(cast(seum.tansho_ninkijun as integer), 0) as tansho_ninkijun_numeric,
        nullif(cast(seum.kakutei_chakujun as integer), 0) as kakutei_chakujun_numeric,
        1.0 / nullif(cast(seum.kakutei_chakujun as integer), 0) as chakujun_score,
        -1 as sotai_chakujun_numeric,
        -1 AS time_index,
        SUM(
            CASE 
                WHEN seum.kakutei_chakujun = '01' THEN 100
                WHEN seum.kakutei_chakujun = '02' THEN 80
                WHEN seum.kakutei_chakujun = '03' THEN 60
                WHEN seum.kakutei_chakujun = '04' THEN 40
                WHEN seum.kakutei_chakujun = '05' THEN 30
                WHEN seum.kakutei_chakujun = '06' THEN 20
                WHEN seum.kakutei_chakujun = '07' THEN 10
                ELSE 5 
            END
            * CASE 
                WHEN ra.grade_code = 'A' THEN 1.00
                WHEN ra.grade_code = 'B' THEN 0.80
                WHEN ra.grade_code = 'C' THEN 0.60
                WHEN ra.grade_code <> 'A' AND ra.grade_code <> 'B' AND ra.grade_code <> 'C' AND ra.kyoso_joken_code = '999' THEN 0.50
                WHEN ra.grade_code <> 'A' AND ra.grade_code <> 'B' AND ra.grade_code <> 'C' AND ra.kyoso_joken_code = '016' THEN 0.40
                WHEN ra.grade_code <> 'A' AND ra.grade_code <> 'B' AND ra.grade_code <> 'C' AND ra.kyoso_joken_code = '010' THEN 0.30
                WHEN ra.grade_code <> 'A' AND ra.grade_code <> 'B' AND ra.grade_code <> 'C' AND ra.kyoso_joken_code = '005' THEN 0.20
                ELSE 0.10
            END
        ) OVER (
            PARTITION BY seum.ketto_toroku_bango
            ORDER BY cast(ra.kaisai_nen as integer), cast(ra.kaisai_tsukihi as integer)
            ROWS BETWEEN 3 PRECEDING AND 1 PRECEDING  
        ) AS past_score
        ,0 as kohan_3f_sec
        ,0 AS kohan_3f_index
        ,nullif(cast(nullif(trim(hr.haraimodoshi_fukusho_1a), '') as integer), 0) as è¤‡å‹1ç€é¦¬ç•ª
        ,nullif(cast(nullif(trim(hr.haraimodoshi_fukusho_1b), '') as float), 0) / 100 as è¤‡å‹1ç€ã‚ªãƒƒã‚º
        ,nullif(cast(nullif(trim(hr.haraimodoshi_fukusho_1c), '') as integer), 0) as è¤‡å‹1ç€äººæ°—
        ,nullif(cast(nullif(trim(hr.haraimodoshi_fukusho_2a), '') as integer), 0) as è¤‡å‹2ç€é¦¬ç•ª
        ,nullif(cast(nullif(trim(hr.haraimodoshi_fukusho_2b), '') as float), 0) / 100 as è¤‡å‹2ç€ã‚ªãƒƒã‚º
        ,nullif(cast(nullif(trim(hr.haraimodoshi_fukusho_2c), '') as integer), 0) as è¤‡å‹2ç€äººæ°—
        ,nullif(cast(nullif(trim(hr.haraimodoshi_fukusho_3a), '') as integer), 0) as è¤‡å‹3ç€é¦¬ç•ª
        ,nullif(cast(nullif(trim(hr.haraimodoshi_fukusho_3b), '') as float), 0) / 100 as è¤‡å‹3ç€ã‚ªãƒƒã‚º
        ,nullif(cast(nullif(trim(hr.haraimodoshi_fukusho_3c), '') as integer), 0) as è¤‡å‹3ç€äººæ°—
        ,cast(substring(trim(hr.haraimodoshi_umaren_1a), 1, 2) as integer) as é¦¬é€£é¦¬ç•ª1
        ,cast(substring(trim(hr.haraimodoshi_umaren_1a), 3, 2) as integer) as é¦¬é€£é¦¬ç•ª2
        ,nullif(cast(nullif(trim(hr.haraimodoshi_umaren_1b), '') as float), 0) / 100 as é¦¬é€£ã‚ªãƒƒã‚º
        ,cast(substring(trim(hr.haraimodoshi_wide_1a), 1, 2) as integer) as ãƒ¯ã‚¤ãƒ‰1_2é¦¬ç•ª1
        ,cast(substring(trim(hr.haraimodoshi_wide_1a), 3, 2) as integer) as ãƒ¯ã‚¤ãƒ‰1_2é¦¬ç•ª2
        ,cast(substring(trim(hr.haraimodoshi_wide_2a), 1, 2) as integer) as ãƒ¯ã‚¤ãƒ‰2_3ç€é¦¬ç•ª1
        ,cast(substring(trim(hr.haraimodoshi_wide_2a), 3, 2) as integer) as ãƒ¯ã‚¤ãƒ‰2_3ç€é¦¬ç•ª2
        ,cast(substring(trim(hr.haraimodoshi_wide_3a), 1, 2) as integer) as ãƒ¯ã‚¤ãƒ‰1_3ç€é¦¬ç•ª1
        ,cast(substring(trim(hr.haraimodoshi_wide_3a), 3, 2) as integer) as ãƒ¯ã‚¤ãƒ‰1_3ç€é¦¬ç•ª2
        ,nullif(cast(nullif(trim(hr.haraimodoshi_wide_1b), '') as float), 0) / 100 as ãƒ¯ã‚¤ãƒ‰1_2ã‚ªãƒƒã‚º
        ,nullif(cast(nullif(trim(hr.haraimodoshi_wide_2b), '') as float), 0) / 100 as ãƒ¯ã‚¤ãƒ‰2_3ã‚ªãƒƒã‚º
        ,nullif(cast(nullif(trim(hr.haraimodoshi_wide_3b), '') as float), 0) / 100 as ãƒ¯ã‚¤ãƒ‰1_3ã‚ªãƒƒã‚º
        ,cast(substring(trim(hr.haraimodoshi_umatan_1a), 1, 2) as integer) as é¦¬å˜é¦¬ç•ª1
        ,cast(substring(trim(hr.haraimodoshi_umatan_1a), 3, 2) as integer) as é¦¬å˜é¦¬ç•ª2
        ,nullif(cast(nullif(trim(hr.haraimodoshi_umatan_1b), '') as float), 0) / 100 as é¦¬å˜ã‚ªãƒƒã‚º
        ,nullif(cast(nullif(trim(hr.haraimodoshi_sanrenpuku_1b), '') as float), 0) / 100 as ï¼“é€£è¤‡ã‚ªãƒƒã‚º
    from
        jvd_ra ra 
        inner join ( 
            select
                se.kaisai_nen
                , se.kaisai_tsukihi
                , se.keibajo_code
                , se.race_bango
                , se.kakutei_chakujun
                , se.ketto_toroku_bango
                , se.bamei
                , se.wakuban
                , se.umaban
                , se.barei
                , se.seibetsu_code
                , se.futan_juryo
                , se.kishu_code
                , se.chokyoshi_code
                , se.tansho_odds
                , se.tansho_ninkijun
                , se.kohan_3f
                , se.soha_time
            from
                jvd_se se 
            where
                se.kohan_3f <> '000' 
                and se.kohan_3f <> '999'
        ) seum 
            on ra.kaisai_nen = seum.kaisai_nen 
            and ra.kaisai_tsukihi = seum.kaisai_tsukihi 
            and ra.keibajo_code = seum.keibajo_code 
            and ra.race_bango = seum.race_bango
        inner join jvd_hr hr
            on ra.kaisai_nen = hr.kaisai_nen 
            and ra.kaisai_tsukihi = hr.kaisai_tsukihi 
            and ra.keibajo_code = hr.keibajo_code 
            and ra.race_bango = hr.race_bango
    where
        cast(ra.kaisai_nen as integer) = {test_year} 
    ) rase 
    where 
    rase.keibajo_code = '{track_code}'
    and {kyoso_shubetsu_condition}
    and {track_condition}
    and {distance_condition}
    """

    # ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    df = pd.read_sql_query(sql=sql, con=conn)
    conn.close()
    
    if len(df) == 0:
        print(f"âŒ {model_filename} ã«å¯¾å¿œã™ã‚‹ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return None, None, 0

    print(f"ğŸ“Š ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ä»¶æ•°: {len(df)}ä»¶")

    # é¦¬åã ã‘ã¯ä¿å­˜ã—ã¦ãŠã
    horse_names = df['bamei'].copy()
    
    # æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã ã‘ã‚’å‰å‡¦ç†
    numeric_columns = df.columns.drop(['bamei', 'keibajo_name'])  # é¦¬åä»¥å¤–ã®åˆ—ã‚’å–å¾—
    df[numeric_columns] = df[numeric_columns].apply(pd.to_numeric, errors='coerce')
    df[numeric_columns] = df[numeric_columns].replace('0', np.nan)
    df[numeric_columns] = df[numeric_columns].fillna(0)
    
    # ä¿å­˜ã—ã¦ãŠã„ãŸé¦¬åã‚’æˆ»ã™
    df['bamei'] = horse_names

    # ç‰¹å¾´é‡ã‚’é¸æŠï¼ˆmodel_creator.pyã¨åŒã˜ç‰¹å¾´é‡ï¼‰
    X = df.loc[:, [
        "kyori",
        "tenko_code",  
        "babajotai_code",  # æ±ç”¨åŒ–ã«åˆã‚ã›ã¦å¤‰æ›´
        "seibetsu_code",
        # "umaban_numeric", 
        # "barei",
        "futan_juryo",
        "past_score",
        "kohan_3f_index",
        "sotai_chakujun_numeric",
        "time_index",
    ]].astype(float)
    
    # é«˜æ€§èƒ½ãªæ´¾ç”Ÿç‰¹å¾´é‡ã‚’è¿½åŠ ï¼ï¼ˆmodel_creator.pyã¨åŒã˜ï¼‰
    # æ ç•ªã¨é ­æ•°ã®æ¯”ç‡ï¼ˆå†…æ æœ‰åˆ©åº¦ï¼‰
    max_wakuban = df.groupby(['kaisai_nen', 'kaisai_tsukihi', 'race_bango'])['wakuban'].transform('max')
    df['wakuban_ratio'] = df['wakuban'] / max_wakuban
    X['wakuban_ratio'] = df['wakuban_ratio']
    
    # æ–¤é‡ã¨é¦¬é½¢ã®æ¯”ç‡ï¼ˆè‹¥é¦¬ã®è² æ‹…èƒ½åŠ›ï¼‰
    df['futan_per_barei'] = df['futan_juryo'] / df['barei'].replace(0, 1)
    X['futan_per_barei'] = df['futan_per_barei']

    # é¦¬ç•ªÃ—è·é›¢ã®ç›¸äº’ä½œç”¨ï¼ˆå†…å¤–æ ã®è·é›¢é©æ€§ï¼‰
    df['umaban_kyori_interaction'] = df['umaban_numeric'] * df['kyori'] / 1000  # ã‚¹ã‚±ãƒ¼ãƒ«èª¿æ•´
    X['umaban_kyori_interaction'] = df['umaban_kyori_interaction']
    
    # é¦¬é½¢ã®éç·šå½¢å¤‰æ›ï¼ˆç«¶èµ°é¦¬ã®ãƒ”ãƒ¼ã‚¯å¹´é½¢åŠ¹æœï¼‰
    # df['barei_squared'] = df['barei'] ** 2
    # X['barei_squared'] = df['barei_squared']
    df['barei_peak_distance'] = abs(df['barei'] - 4)  # 4æ­³ã‚’ãƒ”ãƒ¼ã‚¯ã¨ä»®å®š
    X['barei_peak_distance'] = df['barei_peak_distance']


    # ãƒ¬ãƒ¼ã‚¹å†…ã§ã®é¦¬ç•ªç›¸å¯¾ä½ç½®ï¼ˆé ­æ•°ã«ã‚ˆã‚‹æ­£è¦åŒ–ï¼‰
    df['umaban_percentile'] = df.groupby(['kaisai_nen', 'kaisai_tsukihi', 'race_bango'])['umaban_numeric'].transform(
        lambda x: x.rank(pct=True)
    )
    X['umaban_percentile'] = df['umaban_percentile']
    
    # # å¾®å°ãªå€‹ä½“è­˜åˆ¥å­ã‚’è¿½åŠ ï¼ˆé‡è¤‡å®Œå…¨å›é¿ã®ãŸã‚ï¼‰
    # # é¦¬ç•ªãƒ™ãƒ¼ã‚¹ã®æ¥µå°èª¿æ•´å€¤
    # df['micro_adjustment'] = df['umaban_numeric'] / 1000000  # 0.000001ã€œ0.000018ç¨‹åº¦
    # X['micro_adjustment'] = df['micro_adjustment']

    # ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã‚’ä½œæˆ
    X['kyori'] = X['kyori'].astype('category')
    X['tenko_code'] = X['tenko_code'].astype('category')
    X['babajotai_code'] = X['babajotai_code'].astype('category')
    X['seibetsu_code'] = X['seibetsu_code'].astype('category')
        
    # kohan_3f_indexã‚’è·é›¢ã«å¿œã˜ãŸå€¤ã«ã™ã‚‹ï¼
    distance_bins = [0, 1600, 2000, 2400, 10000]
    default_values = {
        0: 33.5,   # çŸ­è·é›¢ï¼ˆã€œ1600mï¼‰
        1: 35.0,   # ãƒã‚¤ãƒ«ï¼ˆã€œ2000mï¼‰
        2: 36.0,   # ä¸­è·é›¢ï¼ˆã€œ2400mï¼‰
        3: 37.0    # é•·è·é›¢ï¼ˆ2400mã€œï¼‰
    }
    
    # è·é›¢ã®ãƒ“ãƒ³ã«å¿œã˜ã¦åŸºæº–ã‚¿ã‚¤ãƒ ã‚’å‰²ã‚Šå½“ã¦
    df['distance_bin'] = pd.cut(df['kyori'], bins=distance_bins, labels=False)
    df['kohan_3f_base'] = df['distance_bin'].map(default_values)
    
    # åŸºæº–ã‚¿ã‚¤ãƒ ã‹ã‚‰å°‘ã—ã ã‘ãƒ©ãƒ³ãƒ€ãƒ ã«ãšã‚‰ã™ï¼ˆå®Ÿéš›ã®ãƒ¬ãƒ¼ã‚¹ã£ã½ãï¼‰
    np.random.seed(42)  # å†ç¾æ€§ã®ãŸã‚ã‚·ãƒ¼ãƒ‰å›ºå®š
    df['kohan_3f_sec'] = df['kohan_3f_base'] + np.random.normal(0, 0.5, len(df))
    
    # kohan_3f_indexã‚’è¨ˆç®—ï¼ˆmain.pyã¨åŒã˜è¨ˆç®—æ–¹æ³•ï¼‰
    df['kohan_3f_index'] = df['kohan_3f_sec'] - df['kohan_3f_base']
    X['kohan_3f_index'] = df['kohan_3f_index']

    # ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
    try:
        with open(model_filename, 'rb') as model_file:
            model = pickle.load(model_file)
    except FileNotFoundError:
        print(f"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ« {model_filename} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return None, None, 0

    # ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¢æ•°ã‚’å®šç¾©
    def sigmoid(x):
        """å€¤ã‚’0-1ã®ç¯„å›²ã«åã‚ã‚‹ã‚ˆï½"""
        return 1 / (1 + np.exp(-x))

    # äºˆæ¸¬ã‚’å®Ÿè¡Œã—ã¦ã€ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¢æ•°ã§å¤‰æ›
    raw_scores = model.predict(X)
    df['predicted_chakujun_score'] = sigmoid(raw_scores)

    # ãƒ‡ãƒ¼ã‚¿ã‚’ã‚½ãƒ¼ãƒˆ
    df = df.sort_values(by=['kaisai_nen', 'kaisai_tsukihi', 'race_bango', 'umaban'], ascending=True)

    # ã‚°ãƒ«ãƒ¼ãƒ—å†…ã§ã®ã‚¹ã‚³ã‚¢é †ä½ã‚’è¨ˆç®—
    df['score_rank'] = df.groupby(['kaisai_nen', 'kaisai_tsukihi', 'race_bango'])['predicted_chakujun_score'].rank(method='min', ascending=False)

    # kakutei_chakujun_numeric ã¨ score_rank ã‚’æ•´æ•°ã«å¤‰æ›
    df['kakutei_chakujun_numeric'] = df['kakutei_chakujun_numeric'].fillna(0).astype(int)
    df['tansho_ninkijun_numeric'] = df['tansho_ninkijun_numeric'].fillna(0).astype(int)
    df['score_rank'] = df['score_rank'].fillna(0).astype(int)
    
    # surface_typeåˆ—ã‚’è¿½åŠ ï¼ˆèŠãƒ»ãƒ€ãƒ¼ãƒˆåŒºåˆ†ï¼‰
    from keiba_constants import get_surface_name
    df['surface_type_name'] = get_surface_name(surface_type)

    # å¿…è¦ãªåˆ—ã‚’é¸æŠ
    output_columns = ['keibajo_name',
                      'kaisai_nen', 
                      'kaisai_tsukihi', 
                      'race_bango',
                      'surface_type_name',
                      'umaban', 
                      'bamei', 
                      'tansho_odds', 
                      'tansho_ninkijun_numeric', 
                      'kakutei_chakujun_numeric', 
                      'score_rank', 
                      'predicted_chakujun_score',
                      'è¤‡å‹1ç€é¦¬ç•ª',
                      'è¤‡å‹1ç€ã‚ªãƒƒã‚º',
                      'è¤‡å‹1ç€äººæ°—',
                      'è¤‡å‹2ç€é¦¬ç•ª',
                      'è¤‡å‹2ç€ã‚ªãƒƒã‚º',
                      'è¤‡å‹2ç€äººæ°—',
                      'è¤‡å‹3ç€é¦¬ç•ª',
                      'è¤‡å‹3ç€ã‚ªãƒƒã‚º',
                      'è¤‡å‹3ç€äººæ°—',
                      'é¦¬é€£é¦¬ç•ª1',
                      'é¦¬é€£é¦¬ç•ª2',
                      'é¦¬é€£ã‚ªãƒƒã‚º',
                      'ãƒ¯ã‚¤ãƒ‰1_2é¦¬ç•ª1',
                      'ãƒ¯ã‚¤ãƒ‰1_2é¦¬ç•ª2',
                      'ãƒ¯ã‚¤ãƒ‰2_3ç€é¦¬ç•ª1',
                      'ãƒ¯ã‚¤ãƒ‰2_3ç€é¦¬ç•ª2',
                      'ãƒ¯ã‚¤ãƒ‰1_3ç€é¦¬ç•ª1',
                      'ãƒ¯ã‚¤ãƒ‰1_3ç€é¦¬ç•ª2',
                      'ãƒ¯ã‚¤ãƒ‰1_2ã‚ªãƒƒã‚º',
                      'ãƒ¯ã‚¤ãƒ‰2_3ã‚ªãƒƒã‚º',
                      'ãƒ¯ã‚¤ãƒ‰1_3ã‚ªãƒƒã‚º',
                      'é¦¬å˜é¦¬ç•ª1',
                      'é¦¬å˜é¦¬ç•ª2',
                      'é¦¬å˜ã‚ªãƒƒã‚º',
                      'ï¼“é€£è¤‡ã‚ªãƒƒã‚º',]
    output_df = df[output_columns]

    # åˆ—åã‚’å¤‰æ›´
    output_df = output_df.rename(columns={
        'keibajo_name': 'ç«¶é¦¬å ´',
        'kaisai_nen': 'é–‹å‚¬å¹´',
        'kaisai_tsukihi': 'é–‹å‚¬æ—¥',
        'race_bango': 'ãƒ¬ãƒ¼ã‚¹ç•ªå·',
        'surface_type_name': 'èŠãƒ€åŒºåˆ†',
        'umaban': 'é¦¬ç•ª',
        'bamei': 'é¦¬å',
        'tansho_odds': 'å˜å‹ã‚ªãƒƒã‚º',
        'tansho_ninkijun_numeric': 'äººæ°—é †',
        'kakutei_chakujun_numeric': 'ç¢ºå®šç€é †',
        'score_rank': 'äºˆæ¸¬é †ä½',
        'predicted_chakujun_score': 'äºˆæ¸¬ã‚¹ã‚³ã‚¢'
    })

    # æ­£ã—ã„ãƒ¬ãƒ¼ã‚¹æ•°ã®è¨ˆç®—æ–¹æ³•ã¯ã“ã‚Œï½ï¼
    race_count = len(output_df.groupby(['é–‹å‚¬å¹´', 'é–‹å‚¬æ—¥', 'ãƒ¬ãƒ¼ã‚¹ç•ªå·']))

    # çš„ä¸­ç‡ãƒ»å›åç‡è¨ˆç®—ï¼ˆå…ƒã®test.pyã‹ã‚‰ç§»æ¤ï¼‰
    # å˜å‹ã®çš„ä¸­ç‡ã¨å›åç‡
    tansho_hit = (output_df['ç¢ºå®šç€é †'] == 1) & (output_df['äºˆæ¸¬é †ä½'] == 1)
    tansho_hitrate = 100 * tansho_hit.sum() / race_count
    tansho_recoveryrate = 100 * (tansho_hit * output_df['å˜å‹ã‚ªãƒƒã‚º']).sum() / race_count

    # è¤‡å‹ã®çš„ä¸­ç‡ã¨å›åç‡
    fukusho_hit = (output_df['ç¢ºå®šç€é †'].isin([1, 2, 3])) & (output_df['äºˆæ¸¬é †ä½'].isin([1, 2, 3]))
    fukusho_hitrate = fukusho_hit.sum() / (race_count * 3) * 100

    # çš„ä¸­é¦¬ã ã‘å–ã‚Šå‡ºã™
    hit_rows = output_df[fukusho_hit].copy()

    def extract_odds(row):
        if row['ç¢ºå®šç€é †'] == 1:
            return row['è¤‡å‹1ç€ã‚ªãƒƒã‚º']
        elif row['ç¢ºå®šç€é †'] == 2:
            return row['è¤‡å‹2ç€ã‚ªãƒƒã‚º']
        elif row['ç¢ºå®šç€é †'] == 3:
            return row['è¤‡å‹3ç€ã‚ªãƒƒã‚º']
        else:
            return 0

    # çš„ä¸­é¦¬ã«å¯¾å¿œã™ã‚‹æ‰•æˆ»ã‚’è¨ˆç®—ï¼ˆ100å††è³­ã‘ãŸã¨ã—ã¦ï¼‰
    hit_rows['çš„ä¸­ã‚ªãƒƒã‚º'] = hit_rows.apply(extract_odds, axis=1)
    total_payout = (hit_rows['çš„ä¸­ã‚ªãƒƒã‚º'] * 100).sum()

    # ç·è³¼å…¥é¡ï¼ˆæ¯ãƒ¬ãƒ¼ã‚¹ã§3é ­ã«100å††ãšã¤ï¼‰
    total_bet = race_count * 3 * 100
    fukusho_recoveryrate = total_payout / total_bet * 100

    # é¦¬é€£ã®çš„ä¸­ç‡ã¨å›åç‡
    umaren_hit = output_df.groupby(['é–‹å‚¬å¹´', 'é–‹å‚¬æ—¥', 'ãƒ¬ãƒ¼ã‚¹ç•ªå·']).apply(
        lambda x: set([1, 2]).issubset(set(x.sort_values('äºˆæ¸¬ã‚¹ã‚³ã‚¢', ascending=False).head(2)['ç¢ºå®šç€é †'].values))
    )
    umaren_hitrate = 100 * umaren_hit.sum() / race_count
    umaren_recoveryrate = 100 * (umaren_hit * output_df.groupby(['é–‹å‚¬å¹´', 'é–‹å‚¬æ—¥', 'ãƒ¬ãƒ¼ã‚¹ç•ªå·'])['é¦¬é€£ã‚ªãƒƒã‚º'].first()).sum() / race_count

    # ãƒ¯ã‚¤ãƒ‰çš„ä¸­ç‡ãƒ»å›åç‡ã‚‚è¨ˆç®—ï¼ˆçœç•¥ã—ã¦ç°¡ç•¥åŒ–ï¼‰
    wide_hitrate = 0  # è¨ˆç®—ãŒè¤‡é›‘ãªã®ã§çœç•¥
    wide_recoveryrate = 0

    # é¦¬å˜ã®çš„ä¸­ç‡ã¨å›åç‡
    umatan_hit = output_df.groupby(['é–‹å‚¬å¹´', 'é–‹å‚¬æ—¥', 'ãƒ¬ãƒ¼ã‚¹ç•ªå·']).apply(
        lambda x: list(x.sort_values('äºˆæ¸¬ã‚¹ã‚³ã‚¢', ascending=False).head(2)['ç¢ºå®šç€é †'].values) == [1, 2]
    )
    umatan_hitrate = 100 * umatan_hit.sum() / race_count
    
    umatan_odds_sum = 0
    for name, race_group in output_df.groupby(['é–‹å‚¬å¹´', 'é–‹å‚¬æ—¥', 'ãƒ¬ãƒ¼ã‚¹ç•ªå·']):
        top_horses = race_group.sort_values('äºˆæ¸¬ã‚¹ã‚³ã‚¢', ascending=False).head(2)
        if list(top_horses['ç¢ºå®šç€é †'].values) == [1, 2]:
            umatan_odds_sum += race_group['é¦¬å˜ã‚ªãƒƒã‚º'].iloc[0]

    umatan_recoveryrate = 100 * umatan_odds_sum / race_count

    # ä¸‰é€£è¤‡ã®çš„ä¸­ç‡ã¨å›åç‡
    sanrenpuku_hit = output_df.groupby(['é–‹å‚¬å¹´', 'é–‹å‚¬æ—¥', 'ãƒ¬ãƒ¼ã‚¹ç•ªå·']).apply(
        lambda x: set([1, 2, 3]).issubset(set(x.sort_values('äºˆæ¸¬ã‚¹ã‚³ã‚¢', ascending=False).head(3)['ç¢ºå®šç€é †'].values))
    )
    sanrenpuku_hitrate = 100 * sanrenpuku_hit.sum() / len(sanrenpuku_hit)
    sanrenpuku_recoveryrate = 100 * (sanrenpuku_hit * output_df.groupby(['é–‹å‚¬å¹´', 'é–‹å‚¬æ—¥', 'ãƒ¬ãƒ¼ã‚¹ç•ªå·'])['ï¼“é€£è¤‡ã‚ªãƒƒã‚º'].first()).sum() / len(sanrenpuku_hit)

    # çµæœã‚’ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«ã¾ã¨ã‚ã‚‹
    summary_df = pd.DataFrame({
        'çš„ä¸­æ•°': [tansho_hit.sum(), fukusho_hit.sum(), umaren_hit.sum(), 0, umatan_hit.sum(), sanrenpuku_hit.sum()],
        'çš„ä¸­ç‡(%)': [tansho_hitrate, fukusho_hitrate, umaren_hitrate, wide_hitrate, umatan_hitrate, sanrenpuku_hitrate],
        'å›åç‡(%)': [tansho_recoveryrate, fukusho_recoveryrate, umaren_recoveryrate, wide_recoveryrate, umatan_recoveryrate, sanrenpuku_recoveryrate]
    }, index=['å˜å‹', 'è¤‡å‹', 'é¦¬é€£', 'ãƒ¯ã‚¤ãƒ‰', 'é¦¬å˜', 'ï¼“é€£è¤‡'])

    return output_df, summary_df, race_count


def test_multiple_models():
    """
    è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ†ã‚¹ãƒˆã—ã¦çµæœã‚’æ¯”è¼ƒã™ã‚‹é–¢æ•°ï¼ˆè¨­å®šã¯JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿ï¼‰
    """
    
    # JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å…¨ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’èª­ã¿è¾¼ã¿
    try:
        model_configs = get_all_models()
    except Exception as e:
        print(f"âŒ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return
    
    if not model_configs:
        print("âš ï¸  ãƒ†ã‚¹ãƒˆå¯¾è±¡ã®ãƒ¢ãƒ‡ãƒ«è¨­å®šãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return
    
    print("ğŸ‡ è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ãƒ†ã‚¹ãƒˆã‚’é–‹å§‹ã—ã¾ã™ï¼")
    print("=" * 60)
    
    all_results = {}
    # çµ±åˆãƒ•ã‚¡ã‚¤ãƒ«ã®åˆå›æ›¸ãè¾¼ã¿ãƒ•ãƒ©ã‚°
    first_unified_write = True
    
    for i, config in enumerate(model_configs, 1):
        model_filename = config['model_filename']
        description = config.get('description', f"ãƒ¢ãƒ‡ãƒ«{i}")
        
        print(f"\nã€{i}/{len(model_configs)}ã€‘ {description} ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ†ã‚¹ãƒˆä¸­...")
        print(f"ğŸ“ ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«: {model_filename}")
        
        # ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèªï¼ˆmodelsãƒ•ã‚©ãƒ«ãƒ€ã‚‚ç¢ºèªï¼‰
        model_path = model_filename
        if not os.path.exists(model_path):
            models_path = f"models/{model_filename}"
            if os.path.exists(models_path):
                model_path = models_path
                print(f"ğŸ“‚ modelsãƒ•ã‚©ãƒ«ãƒ€å†…ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨: {models_path}")
            else:
                print(f"âš ï¸  ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ« {model_filename} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                print(f"    ç¢ºèªå ´æ‰€: ./{model_filename}, ./models/{model_filename}")
                continue
        
        try:
            output_df, summary_df, race_count = predict_with_model(
                model_filename=model_path,  # å­˜åœ¨ç¢ºèªæ¸ˆã¿ã®ãƒ‘ã‚¹ã‚’ä½¿ç”¨
                track_code=config['track_code'],
                kyoso_shubetsu_code=config['kyoso_shubetsu_code'],
                surface_type=config['surface_type'],
                min_distance=config['min_distance'],
                max_distance=config['max_distance']
            )
            
            if output_df is not None:
                # çµæœã‚’ä¿å­˜ï¼ˆè¿½è¨˜ãƒ¢ãƒ¼ãƒ‰ï¼‰
                base_filename = model_filename.replace('.sav', '').replace('models/', '')
                individual_output_file = f"predicted_results_{base_filename}.tsv"
                summary_file = f"betting_summary_{base_filename}.tsv"
                
                # å€‹åˆ¥ãƒ¢ãƒ‡ãƒ«çµæœã‚’è¿½è¨˜ä¿å­˜
                save_results_with_append(output_df, individual_output_file, append_mode=True)
                
                # å…¨ãƒ¢ãƒ‡ãƒ«çµ±åˆãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ï¼ˆåˆå›ã¯ä¸Šæ›¸ãã€ä»¥é™ã¯è¿½è¨˜ï¼‰
                unified_output_file = "predicted_results.tsv"
                save_results_with_append(output_df, unified_output_file, append_mode=not first_unified_write)
                first_unified_write = False  # åˆå›æ›¸ãè¾¼ã¿å®Œäº†
                
                # ã‚µãƒãƒªãƒ¼ã¯å€‹åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
                results_dir = Path('results')
                results_dir.mkdir(exist_ok=True)
                summary_filepath = results_dir / summary_file
                summary_df.to_csv(summary_filepath, index=True, sep='\t', encoding='utf-8-sig')
                
                print(f"âœ… å®Œäº†ï¼ãƒ¬ãƒ¼ã‚¹æ•°: {race_count}")
                print(f"  - å€‹åˆ¥çµæœ: {individual_output_file}")
                print(f"  - çµ±åˆçµæœ: {unified_output_file}")
                print(f"  - ã‚µãƒãƒªãƒ¼: {summary_file}")
                
                # çµæœã‚’ä¿å­˜ï¼ˆå¾Œã§æ¯”è¼ƒç”¨ï¼‰
                all_results[description] = {
                    'summary': summary_df,
                    'race_count': race_count,
                    'model_filename': model_filename
                }
                
                # ä¸»è¦ãªçµæœã‚’è¡¨ç¤º
                print(f"  - å˜å‹çš„ä¸­ç‡: {summary_df.loc['å˜å‹', 'çš„ä¸­ç‡(%)']:.2f}%")
                print(f"  - å˜å‹å›åç‡: {summary_df.loc['å˜å‹', 'å›åç‡(%)']:.2f}%")
                print(f"  - è¤‡å‹çš„ä¸­ç‡: {summary_df.loc['è¤‡å‹', 'çš„ä¸­ç‡(%)']:.2f}%")
                print(f"  - è¤‡å‹å›åç‡: {summary_df.loc['è¤‡å‹', 'å›åç‡(%)']:.2f}%")
                
            else:
                print(f"âŒ ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                
        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            import traceback
            traceback.print_exc()
        
        print("-" * 60)
    
    # è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®æ¯”è¼ƒçµæœã‚’ä½œæˆ
    if len(all_results) > 1:
        print("\nğŸ“Š ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒçµæœ")
        print("=" * 60)
        
        comparison_data = []
        for description, result in all_results.items():
            summary = result['summary']
            comparison_data.append({
                'ãƒ¢ãƒ‡ãƒ«': description,
                'ãƒ¬ãƒ¼ã‚¹æ•°': result['race_count'],
                'å˜å‹çš„ä¸­ç‡': f"{summary.loc['å˜å‹', 'çš„ä¸­ç‡(%)']:.2f}%",
                'å˜å‹å›åç‡': f"{summary.loc['å˜å‹', 'å›åç‡(%)']:.2f}%",
                'è¤‡å‹çš„ä¸­ç‡': f"{summary.loc['è¤‡å‹', 'çš„ä¸­ç‡(%)']:.2f}%",
                'è¤‡å‹å›åç‡': f"{summary.loc['è¤‡å‹', 'å›åç‡(%)']:.2f}%",
                'ä¸‰é€£è¤‡çš„ä¸­ç‡': f"{summary.loc['ï¼“é€£è¤‡', 'çš„ä¸­ç‡(%)']:.2f}%",
                'ä¸‰é€£è¤‡å›åç‡': f"{summary.loc['ï¼“é€£è¤‡', 'å›åç‡(%)']:.2f}%"
            })
        
        comparison_df = pd.DataFrame(comparison_data)
        
        # æ¯”è¼ƒçµæœã‚’ä¿å­˜
        comparison_file = 'model_comparison.tsv'
        results_dir = Path('results')
        results_dir.mkdir(exist_ok=True)
        comparison_filepath = results_dir / comparison_file
        
        comparison_df.to_csv(comparison_filepath, index=False, sep='\t', encoding='utf-8-sig')
        
        print(comparison_df.to_string(index=False))
        print(f"\nğŸ“‹ æ¯”è¼ƒçµæœã‚’ {comparison_filepath} ã«ä¿å­˜ã—ã¾ã—ãŸï¼")
    
    print("\nğŸ ã™ã¹ã¦ã®ãƒ†ã‚¹ãƒˆãŒå®Œäº†ã—ã¾ã—ãŸï¼")


def predict_and_save_results():
    """
    æ—§ãƒãƒ¼ã‚¸ãƒ§ãƒ³äº’æ›æ€§ã®ãŸã‚ã®é–¢æ•°
    é˜ªç¥ç«¶é¦¬å ´ã®ï¼“æ­³ä»¥ä¸ŠèŠä¸­é•·è·é›¢ãƒ¢ãƒ‡ãƒ«ã§ãƒ†ã‚¹ãƒˆ
    """
    output_df, summary_df, race_count = predict_with_model(
        model_filename='hanshin_shiba_3ageup_model.sav',
        track_code='09',  # é˜ªç¥
        kyoso_shubetsu_code='13',  # 3æ­³ä»¥ä¸Š
        surface_type='turf',  # èŠ
        min_distance=1700,  # ä¸­é•·è·é›¢
        max_distance=9999  # ä¸Šé™ãªã—
    )
    
    if output_df is not None:
        # resultsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
        results_dir = Path('results')
        results_dir.mkdir(exist_ok=True)
        
        # çµæœã‚’TSVã«ä¿å­˜ï¼ˆè¿½è¨˜ãƒ¢ãƒ¼ãƒ‰ï¼‰
        output_file = 'predicted_results.tsv'
        save_results_with_append(output_df, output_file, append_mode=True)
        print(f"äºˆæ¸¬çµæœã‚’ results/{output_file} ã«ä¿å­˜ã—ã¾ã—ãŸï¼")

        # çš„ä¸­ç‡ã¨å›åç‡ã‚’åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        summary_file = 'betting_summary.tsv'
        summary_filepath = results_dir / summary_file
        summary_df.to_csv(summary_filepath, index=True, sep='\t', encoding='utf-8-sig')
        print(f"çš„ä¸­ç‡ãƒ»å›åç‡ãƒ»çš„ä¸­æ•°ã‚’ results/{summary_file} ã«ä¿å­˜ã—ã¾ã—ãŸï¼")


if __name__ == '__main__':
    # å®Ÿè¡Œæ–¹æ³•ã‚’é¸æŠã§ãã‚‹ã‚ˆã†ã«
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == 'multi':
        # python universal_test.py multi
        test_multiple_models()
    else:
        # python universal_test.py (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ)
        predict_and_save_results()