"""
ç©´é¦¬ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆ

å®Ÿéš›ã«çš„ä¸­ã—ãŸç©´é¦¬ã®ç‰¹å¾´ã‚’åˆ†æã—ã€Phase 2è¨­è¨ˆã®çŸ¥è¦‹ã‚’å¾—ã‚‹

åˆ†æé …ç›®:
1. ç©´é¦¬ã®åŸºæœ¬çµ±è¨ˆï¼ˆäººæ°—åˆ†å¸ƒã€çš„ä¸­ç‡ã€ã‚ªãƒƒã‚ºåˆ†å¸ƒï¼‰
2. ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬ã¨ã®é–¢ä¿‚ï¼ˆäºˆæ¸¬é †ä½ã€äºˆæ¸¬ã‚¹ã‚³ã‚¢ã€ä¹–é›¢åº¦ï¼‰
3. ç©´é¦¬ç‰¹æœ‰ã®ç‰¹å¾´é‡ãƒ‘ã‚¿ãƒ¼ãƒ³
4. ãƒ¬ãƒ¼ã‚¹æ¡ä»¶ã¨ã®é–¢ä¿‚ï¼ˆè·é›¢ã€ã‚¯ãƒ©ã‚¹ã€é¦¬å ´çŠ¶æ…‹ãªã©ï¼‰
"""

import psycopg2
import pandas as pd
import numpy as np
import pickle
import json
from pathlib import Path
import matplotlib.pyplot as plt

from db_query_builder import build_race_data_query
from data_preprocessing import preprocess_race_data
from feature_engineering import create_features, add_advanced_features, add_upset_features, add_upset_specific_features

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.sans-serif'] = ['MS Gothic', 'Yu Gothic', 'Meiryo']
plt.rcParams['axes.unicode_minus'] = False


def load_db_config(config_path: str = 'db_config.json') -> dict:
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­å®šã‚’èª­ã¿è¾¼ã¿"""
    with open(config_path, 'r', encoding='utf-8') as f:
        config = json.load(f)
    return config['database']


def get_data_with_predictions(
    model_path: str,
    years: list,
    track_codes: list = None,
    surface_type: str = 'turf',
    distance_min: int = 1000,
    distance_max: int = 9999,
    kyoso_shubetsu_code: str = None,
    use_cache: bool = True  # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä½¿ç”¨ãƒ•ãƒ©ã‚°
) -> pd.DataFrame:
    """
    ãƒ‡ãƒ¼ã‚¿å–å¾— + ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬ã‚’å®Ÿè¡Œï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥å¯¾å¿œï¼‰
    
    Args:
        model_path: ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        years: å¯¾è±¡å¹´ãƒªã‚¹ãƒˆ
        track_codes: ç«¶é¦¬å ´ã‚³ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆï¼ˆNoneã®å ´åˆã¯å…¨10ç«¶é¦¬å ´ï¼‰
        surface_type: è·¯é¢ã‚¿ã‚¤ãƒ— ('turf' or 'dirt' or None)
        distance_min: æœ€å°è·é›¢
        distance_max: æœ€å¤§è·é›¢
        kyoso_shubetsu_code: ç«¶äº‰ç¨®åˆ¥ã‚³ãƒ¼ãƒ‰ ('12'=3æ­³, '13'=3æ­³ä»¥ä¸Š, None=å…¨å¹´é½¢)
        use_cache: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½¿ç”¨ã™ã‚‹ã‹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆTrueï¼‰
        
    Returns:
        pd.DataFrame: äºˆæ¸¬çµæœä»˜ããƒ‡ãƒ¼ã‚¿
    """
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆ
    if use_cache:
        from pathlib import Path
        cache_dir = Path("cache")
        cache_dir.mkdir(exist_ok=True)
        
        year_str = f"{min(years)}-{max(years)}"
        track_str = "all" if track_codes is None else f"{len(track_codes)}tracks"
        surf_str = surface_type or "both"
        cache_file = cache_dir / f"universal_predictions_{year_str}_{track_str}_{surf_str}.pkl"
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒå­˜åœ¨ã™ã‚Œã°èª­ã¿è¾¼ã¿
        if cache_file.exists():
            print(f"\n{'='*80}")
            print(f"ğŸš€ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰äºˆæ¸¬çµæœã‚’èª­ã¿è¾¼ã¿")
            print(f"{'='*80}")
            print(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«: {cache_file}")
            try:
                df_cached = pd.read_pickle(cache_file)
                print(f"âœ… èª­ã¿è¾¼ã¿å®Œäº†: {len(df_cached):,}é ­")
                print(f"â±ï¸  æ™‚é–“ç¯€ç´„: ç´„30-60åˆ†")
                return df_cached
            except Exception as e:
                print(f"âš ï¸  ã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
                print(f"   æ–°è¦ã«äºˆæ¸¬ã‚’å®Ÿè¡Œã—ã¾ã™...")
    
    # å…¨ç«¶é¦¬å ´å¯¾å¿œï¼ˆPhase 2.5ï¼‰
    if track_codes is None:
        from keiba_constants import TRACK_CODES
        track_codes = list(TRACK_CODES.keys())
    
    print(f"\n{'='*80}")
    print(f"ãƒ‡ãƒ¼ã‚¿å–å¾— & äºˆæ¸¬å®Ÿè¡Œ")
    print(f"{'='*80}")
    print(f"å¯¾è±¡å¹´: {years}")
    print(f"ç«¶é¦¬å ´: {', '.join(track_codes)} ({len(track_codes)}ç«¶é¦¬å ´)")
    print(f"è·¯é¢ã‚¿ã‚¤ãƒ—: {'èŠãƒ»ãƒ€ãƒ¼ãƒˆä¸¡æ–¹' if surface_type is None else surface_type}")
    print(f"è·é›¢: {distance_min}m - {distance_max}m")
    print()
    
    # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
    print(f"ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿: {model_path}")
    with open(model_path, 'rb') as f:
        model = pickle.load(f)
    
    # DBæ¥ç¶š
    db_config = load_db_config()
    conn = psycopg2.connect(
        host=db_config['host'],
        port=db_config['port'],
        user=db_config['user'],
        password=db_config['password'],
        dbname=db_config['dbname']
    )
    
    all_data = []
    
    # surface_typeãŒNoneã®å ´åˆã¯èŠãƒ»ãƒ€ãƒ¼ãƒˆä¸¡æ–¹ã‚’å–å¾—
    surface_types = [surface_type] if surface_type else ['turf', 'dirt']
    
    for track_code in track_codes:
        for year in years:
            for surf_type in surface_types:
                print(f"\n{year}å¹´ - ç«¶é¦¬å ´ã‚³ãƒ¼ãƒ‰{track_code} - {surf_type}ã®ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­...")
                
                sql = build_race_data_query(
                    track_code=track_code,
                    year_start=year,
                    year_end=year,
                    surface_type=surf_type,
                    distance_min=distance_min,
                    distance_max=distance_max,
                    kyoso_shubetsu_code=kyoso_shubetsu_code,
                    include_payout=True
                )
                
                df = pd.read_sql_query(sql, conn)
                
                # æ­£ã—ã„ãƒ¬ãƒ¼ã‚¹æ•°ã‚«ã‚¦ãƒ³ãƒˆ
                total_races = df.groupby(['kaisai_nen', 'kaisai_tsukihi', 'keibajo_code', 'race_bango']).ngroups
                print(f"  ãƒ¬ãƒ¼ã‚¹æ•°: {total_races}, å‡ºèµ°é ­æ•°: {len(df)}é ­")
                
                # ãƒ‡ãƒ¼ã‚¿ãŒ0ä»¶ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆæ”¹ä¿®å·¥äº‹ç­‰ã§ä¼‘æ­¢ã—ã¦ã„ãŸç«¶é¦¬å ´å¯¾å¿œï¼‰
                if len(df) == 0:
                    print(f"  âš  ãƒ‡ãƒ¼ã‚¿ãªã—ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                    continue
                
                # ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
                df = preprocess_race_data(df, verbose=False)
                
                # ç‰¹å¾´é‡ç”Ÿæˆ
                X = create_features(df)
                X = add_advanced_features(
                    df=df,
                    X=X,
                    surface_type=surf_type,
                    min_distance=distance_min,
                    max_distance=distance_max,
                    logger=None,
                    inverse_rank=True,
                    include_upset_phase1=False  # Universal Rankerç”¨ãªã®ã§Phase 1ç‰¹å¾´é‡ã¯å«ã‚ãªã„
                )
                
                # äºˆæ¸¬
                df['predicted_score'] = model.predict(X)
                df['predicted_rank'] = df.groupby(
                    ['kaisai_nen', 'kaisai_tsukihi', 'keibajo_code', 'race_bango']
                )['predicted_score'].rank(ascending=False, method='first')
                
                df['popularity_rank'] = df['tansho_ninkijun_numeric']
                df['value_gap'] = df['predicted_rank'] - df['popularity_rank']
                
                all_data.append(df)
    
    conn.close()
    
    # çµåˆ
    df_all = pd.concat(all_data, ignore_index=True)
    print(f"\nåˆè¨ˆ: {len(df_all)}é ­ã®ãƒ‡ãƒ¼ã‚¿")
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜
    if use_cache:
        try:
            df_all.to_pickle(cache_file)
            print(f"âœ… ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜å®Œäº†: {cache_file}")
            print(f"   æ¬¡å›ä»¥é™ã¯ç´„30-60åˆ†ã®æ™‚é–“çŸ­ç¸®ãŒè¦‹è¾¼ã‚ã¾ã™")
        except Exception as e:
            print(f"âš ï¸  ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    return df_all


def analyze_upset_basics(df: pd.DataFrame, popularity_threshold: int = 7):
    """
    ç©´é¦¬ã®åŸºæœ¬çµ±è¨ˆã‚’åˆ†æ
    """
    print(f"\n{'='*80}")
    print(f"1. ç©´é¦¬ã®åŸºæœ¬çµ±è¨ˆï¼ˆ{popularity_threshold}ç•ªäººæ°—ä»¥ä¸‹ï¼‰")
    print(f"{'='*80}")
    
    # ç©´é¦¬ã®å®šç¾©
    df_unpopular = df[df['popularity_rank'] >= popularity_threshold].copy()
    df_upset = df_unpopular[df_unpopular['kakutei_chakujun_numeric'] <= 3].copy()
    
    print(f"\näººæ°—è–„é¦¬: {len(df_unpopular)}é ­")
    print(f"ç©´é¦¬ï¼ˆ3ç€ä»¥å†…ï¼‰: {len(df_upset)}é ­")
    print(f"ç©´é¦¬çš„ä¸­ç‡: {len(df_upset) / len(df_unpopular) * 100:.2f}%")
    
    # äººæ°—åˆ¥ã®çš„ä¸­ç‡
    print(f"\näººæ°—åˆ¥ã®3ç€ä»¥å†…ç‡:")
    for pop in range(popularity_threshold, min(df['popularity_rank'].max().astype(int) + 1, 19)):
        pop_horses = df[df['popularity_rank'] == pop]
        if len(pop_horses) > 0:
            hit_rate = len(pop_horses[pop_horses['kakutei_chakujun_numeric'] <= 3]) / len(pop_horses) * 100
            print(f"  {pop:2d}ç•ªäººæ°—: {hit_rate:5.2f}% ({len(pop_horses):3d}é ­)")
    
    # ã‚ªãƒƒã‚ºåˆ†å¸ƒ
    print(f"\nç©´é¦¬ã®ã‚ªãƒƒã‚ºåˆ†å¸ƒ:")
    print(f"  æœ€å°: {df_upset['tansho_odds'].min():.1f}å€")
    print(f"  æœ€å¤§: {df_upset['tansho_odds'].max():.1f}å€")
    print(f"  å¹³å‡: {df_upset['tansho_odds'].mean():.1f}å€")
    print(f"  ä¸­å¤®å€¤: {df_upset['tansho_odds'].median():.1f}å€")
    
    # ç€é †åˆ†å¸ƒ
    print(f"\nç©´é¦¬ã®ç€é †åˆ†å¸ƒ:")
    for rank in [1, 2, 3]:
        count = len(df_upset[df_upset['kakutei_chakujun_numeric'] == rank])
        pct = count / len(df_upset) * 100
        print(f"  {rank}ç€: {count}é ­ ({pct:.1f}%)")
    
    return df_upset


def analyze_model_predictions(df: pd.DataFrame, df_upset: pd.DataFrame):
    """
    ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬ã¨ç©´é¦¬ã®é–¢ä¿‚ã‚’åˆ†æ
    """
    print(f"\n{'='*80}")
    print(f"2. ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬ã¨ã®é–¢ä¿‚")
    print(f"{'='*80}")
    
    # äºˆæ¸¬é †ä½åˆ†å¸ƒ
    print(f"\nç©´é¦¬ã®äºˆæ¸¬é †ä½åˆ†å¸ƒ:")
    for rank_range in [(1, 3), (4, 6), (7, 9), (10, 18)]:
        start, end = rank_range
        count = len(df_upset[(df_upset['predicted_rank'] >= start) & (df_upset['predicted_rank'] <= end)])
        pct = count / len(df_upset) * 100
        print(f"  äºˆæ¸¬{start:2d}-{end:2d}ä½: {count:3d}é ­ ({pct:5.1f}%)")
    
    # äºˆæ¸¬3ä½ä»¥å†…ã®ç©´é¦¬
    df_upset_top3 = df_upset[df_upset['predicted_rank'] <= 3]
    print(f"\näºˆæ¸¬3ä½ä»¥å†…ã®ç©´é¦¬: {len(df_upset_top3)}é ­ ({len(df_upset_top3) / len(df_upset) * 100:.1f}%)")
    
    if len(df_upset_top3) > 0:
        print(f"  çš„ä¸­ã—ãŸç©´é¦¬ã®ã†ã¡äºˆæ¸¬3ä½ä»¥å†…: {len(df_upset_top3) / len(df_upset) * 100:.1f}%")
        print(f"  å¹³å‡äººæ°—é †ä½: {df_upset_top3['popularity_rank'].mean():.1f}ç•ªäººæ°—")
        print(f"  å¹³å‡ã‚ªãƒƒã‚º: {df_upset_top3['tansho_odds'].mean():.1f}å€")
    
    # ä¹–é›¢åº¦åˆ†å¸ƒ
    print(f"\nç©´é¦¬ã®ä¹–é›¢åº¦ï¼ˆpredicted_rank - popularity_rankï¼‰åˆ†å¸ƒ:")
    print(f"  æœ€å°: {df_upset['value_gap'].min():.1f}")
    print(f"  æœ€å¤§: {df_upset['value_gap'].max():.1f}")
    print(f"  å¹³å‡: {df_upset['value_gap'].mean():.1f}")
    print(f"  ä¸­å¤®å€¤: {df_upset['value_gap'].median():.1f}")
    
    # ä¹–é›¢åº¦åˆ¥ã®åˆ†å¸ƒ
    print(f"\nä¹–é›¢åº¦åˆ¥ã®ç©´é¦¬åˆ†å¸ƒ:")
    for threshold in [0, -2, -4, -6, -8, -10]:
        count = len(df_upset[df_upset['value_gap'] < threshold])
        pct = count / len(df_upset) * 100
        print(f"  ä¹–é›¢åº¦ < {threshold:3d}: {count:3d}é ­ ({pct:5.1f}%)")
    
    # Phase 1ã§æ¤œå‡ºã§ããŸç©´é¦¬
    phase1_detected = df_upset[
        (df_upset['predicted_rank'] <= 3) &
        (df_upset['value_gap'] < -5.0)
    ]
    print(f"\nPhase 1ã§æ¤œå‡ºã§ããŸç©´é¦¬ï¼ˆäºˆæ¸¬3ä½ä»¥å†… & ä¹–é›¢åº¦<-5ï¼‰:")
    print(f"  {len(phase1_detected)}é ­ / {len(df_upset)}é ­ ({len(phase1_detected) / len(df_upset) * 100:.1f}%)")


def analyze_feature_patterns(df: pd.DataFrame, df_upset: pd.DataFrame):
    """
    ç©´é¦¬ç‰¹æœ‰ã®ç‰¹å¾´é‡ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æ
    """
    print(f"\n{'='*80}")
    print(f"3. ç©´é¦¬ç‰¹æœ‰ã®ç‰¹å¾´é‡ãƒ‘ã‚¿ãƒ¼ãƒ³")
    print(f"{'='*80}")
    
    # é‡è¦ç‰¹å¾´é‡ã®ãƒªã‚¹ãƒˆ
    key_features = [
        'past_score', 'past_avg_sotai_chakujun', 'kohan_3f_index',
        'time_index', 'relative_ability', 'current_class_score',
        'class_score_change', 'past_score_mean'
    ]
    
    # äººæ°—é¦¬ï¼ˆ1-3ç•ªäººæ°—ï¼‰ã¨ã®æ¯”è¼ƒ
    df_popular = df[(df['popularity_rank'] <= 3) & (df['kakutei_chakujun_numeric'] <= 3)]
    
    print(f"\näººæ°—é¦¬ï¼ˆ1-3ç•ªäººæ°—ã§3ç€ä»¥å†…ï¼‰vs ç©´é¦¬ã®ç‰¹å¾´é‡æ¯”è¼ƒ:")
    print(f"{'ç‰¹å¾´é‡':<30} {'äººæ°—é¦¬å¹³å‡':>12} {'ç©´é¦¬å¹³å‡':>12} {'å·®åˆ†':>12}")
    print(f"{'-'*70}")
    
    for feat in key_features:
        if feat in df.columns:
            popular_mean = df_popular[feat].mean()
            upset_mean = df_upset[feat].mean()
            diff = upset_mean - popular_mean
            print(f"{feat:<30} {popular_mean:>12.3f} {upset_mean:>12.3f} {diff:>+12.3f}")


def analyze_race_conditions(df: pd.DataFrame, df_upset: pd.DataFrame):
    """
    ãƒ¬ãƒ¼ã‚¹æ¡ä»¶ã¨ç©´é¦¬ã®é–¢ä¿‚ã‚’åˆ†æ
    """
    print(f"\n{'='*80}")
    print(f"4. ãƒ¬ãƒ¼ã‚¹æ¡ä»¶ã¨ç©´é¦¬ã®é–¢ä¿‚")
    print(f"{'='*80}")
    
    # è·é›¢åˆ¥
    print(f"\nè·é›¢åˆ¥ã®ç©´é¦¬å‡ºç¾ç‡:")
    distance_ranges = [(1700, 2000), (2001, 2400), (2401, 3000), (3001, 9999)]
    for d_min, d_max in distance_ranges:
        df_range = df[(df['kyori'] >= d_min) & (df['kyori'] <= d_max)]
        df_upset_range = df_upset[(df_upset['kyori'] >= d_min) & (df_upset['kyori'] <= d_max)]
        
        if len(df_range) > 0:
            rate = len(df_upset_range) / len(df_range) * 100
            print(f"  {d_min}-{d_max}m: {len(df_upset_range):3d}é ­ / {len(df_range):4d}é ­ ({rate:.2f}%)")
    
    # ã‚¯ãƒ©ã‚¹åˆ¥
    if 'current_class_score' in df.columns:
        print(f"\nã‚¯ãƒ©ã‚¹åˆ¥ã®ç©´é¦¬å‡ºç¾ç‡:")
        class_ranges = [(0, 50), (51, 100), (101, 150), (151, 200)]
        for c_min, c_max in class_ranges:
            df_class = df[(df['current_class_score'] >= c_min) & (df['current_class_score'] <= c_max)]
            df_upset_class = df_upset[(df_upset['current_class_score'] >= c_min) & (df_upset['current_class_score'] <= c_max)]
            
            if len(df_class) > 0:
                rate = len(df_upset_class) / len(df_class) * 100
                print(f"  ã‚¯ãƒ©ã‚¹ã‚¹ã‚³ã‚¢{c_min:3d}-{c_max:3d}: {len(df_upset_class):3d}é ­ / {len(df_class):4d}é ­ ({rate:.2f}%)")


def save_upset_data(df_upset: pd.DataFrame, output_dir: str = 'results'):
    """
    ç©´é¦¬ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
    """
    Path(output_dir).mkdir(exist_ok=True)
    
    output_cols = [
        'kaisai_nen', 'kaisai_tsukihi', 'keibajo_name', 'race_bango',
        'bamei', 'umaban', 'kakutei_chakujun', 'kakutei_chakujun_numeric',
        'popularity_rank', 'tansho_odds', 'predicted_rank', 'predicted_score',
        'value_gap', 'kyori', 'past_score', 'relative_ability',
        'current_class_score', 'class_score_change'
    ]
    
    output_cols = [col for col in output_cols if col in df_upset.columns]
    
    output_file = Path(output_dir) / 'upset_horses_analysis.tsv'
    df_upset[output_cols].to_csv(output_file, sep='\t', index=False, encoding='utf-8', float_format='%.8f')
    print(f"\nç©´é¦¬ãƒ‡ãƒ¼ã‚¿ã‚’ {output_file} ã«ä¿å­˜ã—ã¾ã—ãŸ")


def create_training_dataset(df: pd.DataFrame, popularity_min: int = 7, popularity_max: int = 12):
    """
    Phase 2ç”¨ã®è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ
    
    Args:
        df: å…¨ãƒ‡ãƒ¼ã‚¿
        popularity_min: ç©´é¦¬å®šç¾©ã®æœ€å°äººæ°—é †ä½
        popularity_max: ç©´é¦¬å®šç¾©ã®æœ€å¤§äººæ°—é †ä½
    
    Returns:
        è¨“ç·´ç”¨ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
    """
    print(f"\n{'='*80}")
    print(f"Phase 2è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ")
    print(f"{'='*80}")
    
    # å±•é–‹è¦å› ç‰¹å¾´é‡ã®è¿½åŠ ï¼ˆfeature_engineering.pyã®å…±é€šé–¢æ•°ã‚’ä½¿ç”¨ï¼‰
    print("\nå±•é–‹è¦å› ç‰¹å¾´é‡ã‚’è¨ˆç®—ä¸­...")
    df = add_upset_features(df)
    print("  âœ“ å±•é–‹è¦å› ç‰¹å¾´é‡ã‚’è¿½åŠ ã—ã¾ã—ãŸ")
    
    # Phase 3.5ç‰¹å¾´é‡ã‚’è¿½åŠ ï¼ˆé¨æ‰‹ãƒ»èª¿æ•™å¸«ãƒ»é¦¬ã®çµ±è¨ˆæƒ…å ±ï¼‰
    print("\nPhase 3.5ç‰¹å¾´é‡ï¼ˆé¨æ‰‹ãƒ»èª¿æ•™å¸«ãƒ»é¦¬çµ±è¨ˆï¼‰ã‚’è¨ˆç®—ä¸­...")
    # ã¾ãšåŸºæœ¬ç‰¹å¾´é‡ã‚’ç”Ÿæˆ
    X_temp = create_features(df)
    # æ¬¡ã«é«˜åº¦ãªç‰¹å¾´é‡ã‚’è¿½åŠ ï¼ˆPhase 3.5å«ã‚€ï¼‰
    X_temp = add_advanced_features(
        df=df,
        X=X_temp,
        surface_type=None,  # èŠãƒ»ãƒ€ãƒ¼ãƒˆä¸¡æ–¹
        min_distance=1000,
        max_distance=9999,
        logger=None,
        inverse_rank=True,
        include_upset_phase1=True  # ğŸ†• Phase 1ç©´é¦¬äºˆæ¸¬å¼·åŒ–ç‰¹å¾´é‡ã‚’å«ã‚ã‚‹
    )
    # DataFrameã«çµ±åˆ
    for col in X_temp.columns:
        if col not in df.columns:
            df[col] = X_temp[col]
    print("  âœ“ Phase 3.5ç‰¹å¾´é‡ã‚’è¿½åŠ ã—ã¾ã—ãŸ")
    print("  âœ“ Phase 1ç©´é¦¬äºˆæ¸¬å¼·åŒ–ç‰¹å¾´é‡ã‚’è¿½åŠ ã—ã¾ã—ãŸ")
    
    # ğŸ”¥ Phase 3.5.1: ç©´é¦¬ç‰¹åŒ–ç‰¹å¾´é‡ã‚’è¿½åŠ ï¼ˆadd_upset_specific_featuresï¼‰
    print("\nPhase 3.5.1ç‰¹å¾´é‡ï¼ˆç©´é¦¬ç‰¹åŒ–ï¼‰ã‚’è¨ˆç®—ä¸­...")
    X_upset = create_features(df)  # åŸºæœ¬ç‰¹å¾´é‡ã‚’å†ç”Ÿæˆ
    X_upset = add_upset_specific_features(X_upset, df, log=print)
    # add_upset_specific_featuresãŒè¿”ã—ãŸç‰¹å¾´é‡ã®ã¿ã‚’DataFrameã«è¿½åŠ 
    for col in X_upset.columns:
        if col not in df.columns or col in ['jockey_win_rate', 'jockey_place_rate', 'jockey_recent_form',
                                              'trainer_win_rate', 'trainer_place_rate', 'trainer_recent_form',
                                              'horse_career_win_rate', 'horse_career_place_rate',
                                              'rest_weeks',
                                              'past_score_std', 'past_chakujun_variance',
                                              'zenso_oikomi_power', 'zenso_kakoi_komon',
                                              'zenso_ninki_gap', 'zenso_nigeba', 'zenso_taihai',
                                              'zenso_agari_rank', 'saikin_kaikakuritsu']:
            df[col] = X_upset[col]
    print("  âœ“ Phase 3.5.1ç‰¹å¾´é‡ã‚’è¿½åŠ ã—ã¾ã—ãŸ")
    
    # ãƒ©ãƒ™ãƒ«ä½œæˆ: 7-12ç•ªäººæ°—ã§3ç€ä»¥å†… = 1ï¼ˆå…¨äººæ°—ã§è¨“ç·´ã€è©•ä¾¡å¯¾è±¡ã®ã¿ã‚’æ­£ä¾‹ã¨ã™ã‚‹ï¼‰
    print(f"\nç©´é¦¬ãƒ©ãƒ™ãƒ«ä½œæˆä¸­...")
    print(f"  å®šç¾©: {popularity_min}-{popularity_max}ç•ªäººæ°— ã‹ã¤ 3ç€ä»¥å†…")
    df['is_upset'] = (
        (df['popularity_rank'] >= popularity_min) &
        (df['popularity_rank'] <= popularity_max) &
        (df['kakutei_chakujun_numeric'] <= 3)
    ).astype(int)
    
    # çµ±è¨ˆæƒ…å ±
    n_upset = df['is_upset'].sum()
    n_total = len(df)
    upset_rate = n_upset / n_total * 100
    
    print(f"\nãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆçµ±è¨ˆ:")
    print(f"  ç·ãƒ‡ãƒ¼ã‚¿æ•°: {n_total}é ­")
    print(f"  ç©´é¦¬ï¼ˆis_upset=1ï¼‰: {n_upset}é ­ ({upset_rate:.2f}%)")
    print(f"  éç©´é¦¬ï¼ˆis_upset=0ï¼‰: {n_total - n_upset}é ­ ({100 - upset_rate:.2f}%)")
    print(f"  ä¸å‡è¡¡æ¯”ç‡: 1:{(n_total - n_upset) / n_upset:.1f}")
    
    # è¨“ç·´ç”¨ç‰¹å¾´é‡ã‚’é¸æŠ
    feature_cols = [
        # ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›
        'predicted_rank', 'predicted_score',
        
        # äººæ°—ãƒ»ã‚ªãƒƒã‚ºæƒ…å ±
        'popularity_rank', 'tansho_odds', 'value_gap',
        
        # æ—¢å­˜ã®é‡è¦ç‰¹å¾´é‡
        'past_score', 'past_avg_sotai_chakujun', 'kohan_3f_index',
        'time_index', 'relative_ability', 'current_class_score',
        'class_score_change', 'past_score_mean',
        
        # å±•é–‹è¦å› 
        'avg_4corner_position',
        # âš ï¸ prev_rank_change ã‚’å‰Šé™¤ï¼ˆ2026-01-21ï¼‰
        # ç†ç”±: è¨ˆç®—å¼ãŒã€Œå‰èµ°ç€é † - ä»Šå›ç¢ºå®šç€é †ã€ã§ãƒ‡ãƒ¼ã‚¿ãƒªãƒ¼ã‚¯
        # è¨“ç·´æ™‚ã¯ä»Šå›ã®çµæœãŒå«ã¾ã‚Œã‚‹ãŒã€ãƒ†ã‚¹ãƒˆæ™‚ã¯ä¸æ˜ã®ãŸã‚ãƒªãƒ¼ã‚¯
        
        # ğŸ”¥ Phase 3: ç©´é¦¬ç‰¹åŒ–ç‰¹å¾´é‡ï¼ˆ4å€‹æ®‹å­˜ï¼‰
        'past_score_std', 'past_chakujun_variance',
        'zenso_oikomi_power', 'zenso_kakoi_komon',
        
        # ğŸ†• Phase 3.5: æ–°è¦è¿½åŠ ç‰¹å¾´é‡ï¼ˆ5å€‹ï¼‰
        'zenso_ninki_gap', 'zenso_nigeba', 'zenso_taihai',
        'zenso_agari_rank', 'saikin_kaikakuritsu',
        
        # ğŸ†• Phase 3.5: é¨æ‰‹ãƒ»èª¿æ•™å¸«ãƒ»é¦¬ã®çµ±è¨ˆæƒ…å ±ï¼ˆ8å€‹ï¼‰
        'jockey_win_rate', 'jockey_place_rate', 'jockey_recent_form',
        'trainer_win_rate', 'trainer_place_rate', 'trainer_recent_form',
        'horse_career_win_rate', 'horse_career_place_rate',
        
        # ğŸ†• Phase 3.5: ä¼‘é¤Šæƒ…å ±ï¼ˆ1å€‹ï¼‰
        'rest_weeks',
        
        # ğŸ†• Phase 1: ç©´é¦¬äºˆæ¸¬å¼·åŒ–ç‰¹å¾´é‡ï¼ˆ8å€‹ï¼‰- 2026-01-20è¿½åŠ 
        'is_turf_bad_condition',  # èŠä¸è‰¯ãƒ•ãƒ©ã‚° (+3.35%)
        'is_turf_heavy',          # èŠé‡ãƒ•ãƒ©ã‚° (+1.73%)
        'is_local_track',         # ãƒ­ãƒ¼ã‚«ãƒ«ç«¶é¦¬å ´ãƒ•ãƒ©ã‚° (+1.47%)
        'is_open_class',          # ã‚ªãƒ¼ãƒ—ãƒ³ã‚¯ãƒ©ã‚¹ãƒ•ãƒ©ã‚° (+2.38%)
        'is_3win_class',          # 3å‹ã‚¯ãƒ©ã‚¹ãƒ•ãƒ©ã‚° (+2.22%)
        'is_age_prime',           # æœ€ç››æœŸå¹´é½¢ãƒ•ãƒ©ã‚° (+1.50%)
        'zenso_top6',             # å‰èµ°6ç€ä»¥å†…ãƒ•ãƒ©ã‚° (+1.82%)
        'rest_days_fresh',        # ä¼‘é¤Š1-3é€±ãƒ•ãƒ©ã‚° (+0.5%)
        
        # ãƒ¬ãƒ¼ã‚¹æ¡ä»¶
        'kyori', 'baba_jotai_code_numeric',
        
        # ç«¶é¦¬å ´ã‚³ãƒ¼ãƒ‰ï¼ˆPhase 2.5ã§è¿½åŠ ï¼‰
        'keibajo_code_numeric'
    ]
    # æ³¨: Phase 3.5ã§å‰Šé™¤ã—ãŸç‰¹å¾´é‡
    # - wakuban_inner, wakuban_outer (çŸ­è·é›¢å°‚ç”¨ã€æ±ç”¨ãƒ¢ãƒ‡ãƒ«ã«ä¸è¦)
    # - estimated_running_style (æ¨å®šå€¤ã§ãƒã‚¤ã‚ºå¤šã„)
    # - tenko_code (åŠ¹æœä¸æ˜ç­)
    # - distance_change (è·é›¢é©æ€§ã‚¹ã‚³ã‚¢ã§å¸å)
    
    # keibajo_codeã‚’æ•°å€¤åŒ–ï¼ˆPhase 2.5ï¼‰
    if 'keibajo_code' in df.columns:
        df['keibajo_code_numeric'] = df['keibajo_code'].astype(int)
    else:
        df['keibajo_code_numeric'] = 9  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼ˆé˜ªç¥ï¼‰
    
    # è·é›¢é©æ€§ã‚¹ã‚³ã‚¢ãªã©ã®è¿½åŠ ç‰¹å¾´é‡ï¼ˆã‚ã‚Œã°ï¼‰
    optional_features = [
        'distance_aptitude_score', 'baba_score', 
        'kishu_score', 'chokyoshi_score'
    ]
    
    for feat in optional_features:
        if feat in df.columns:
            feature_cols.append(feat)
    
    # å­˜åœ¨ã™ã‚‹ç‰¹å¾´é‡ã®ã¿é¸æŠ
    available_features = [col for col in feature_cols if col in df.columns]
    
    print(f"\nä½¿ç”¨ç‰¹å¾´é‡æ•°: {len(available_features)}å€‹")
    print(f"ç‰¹å¾´é‡: {', '.join(available_features[:10])}...")
    
    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ä½œæˆ
    training_cols = available_features + [
        'is_upset',
        'kaisai_nen', 'kaisai_tsukihi', 'keibajo_code', 'race_bango',
        'bamei', 'umaban', 'kakutei_chakujun_numeric'
    ]
    
    df_training = df[[col for col in training_cols if col in df.columns]].copy()
    
    # æ¬ æå€¤ã‚’0ã§åŸ‹ã‚ã‚‹
    df_training = df_training.fillna(0)
    
    return df_training, available_features


def main():
    """
    ãƒ¡ã‚¤ãƒ³å‡¦ç†
    """
    import argparse
    
    parser = argparse.ArgumentParser(description='ç©´é¦¬ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æï¼†è¨“ç·´ãƒ‡ãƒ¼ã‚¿ä½œæˆ')
    parser.add_argument('--all-tracks', action='store_true', help='å…¨10ç«¶é¦¬å ´å¯¾è±¡ï¼ˆPhase 2.5ï¼‰')
    parser.add_argument('--track-code', type=str, default='09', help='ç«¶é¦¬å ´ã‚³ãƒ¼ãƒ‰ï¼ˆå˜ä¸€ç«¶é¦¬å ´ã®å ´åˆï¼‰')
    args = parser.parse_args()
    
    print("="*80)
    print("ç©´é¦¬ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ")
    print("="*80)
    
    # è¨­å®š
    model_path = 'models/hanshin_turf_3ageup_long.sav'
    years = [2013, 2014, 2015, 2016, 2017, 2018, 2019, 2021, 2022, 2023]  # 2020å¹´é™¤å¤–ï¼ˆPhase 1ã¨çµ±ä¸€ï¼‰
    popularity_threshold = 7  # 7ç•ªäººæ°—ä»¥ä¸‹ã‚’ç©´é¦¬ã¨ã™ã‚‹
    
    # ç«¶é¦¬å ´è¨­å®šï¼ˆPhase 2.5å¯¾å¿œï¼‰
    if args.all_tracks:
        print("ãƒ¢ãƒ¼ãƒ‰: å…¨10ç«¶é¦¬å ´çµ±åˆï¼ˆPhase 2.5ï¼‰")
        track_codes = None  # Noneã§å…¨ç«¶é¦¬å ´
        output_suffix = '_universal'
    else:
        print(f"ãƒ¢ãƒ¼ãƒ‰: å˜ä¸€ç«¶é¦¬å ´ï¼ˆã‚³ãƒ¼ãƒ‰: {args.track_code}ï¼‰")
        track_codes = [args.track_code]
        output_suffix = f'_{args.track_code}'
    
    # ãƒ‡ãƒ¼ã‚¿å–å¾— & äºˆæ¸¬
    df = get_data_with_predictions(
        model_path=model_path,
        years=years,
        track_codes=track_codes
    )
    
    # åˆ†æå®Ÿè¡Œ
    df_upset = analyze_upset_basics(df, popularity_threshold)
    analyze_model_predictions(df, df_upset)
    analyze_feature_patterns(df, df_upset)
    analyze_race_conditions(df, df_upset)
    
    # ãƒ‡ãƒ¼ã‚¿ä¿å­˜
    save_upset_data(df_upset)
    
    # Phase 2è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
    df_training, feature_cols = create_training_dataset(df, popularity_min=7, popularity_max=12)
    
    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ï¼ˆPhase 2.5å¯¾å¿œï¼‰
    output_file = Path('results') / f'upset_training_data{output_suffix}.tsv'
    df_training.to_csv(output_file, sep='\t', index=False, encoding='utf-8', float_format='%.8f')
    print(f"\nè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’ {output_file} ã«ä¿å­˜ã—ã¾ã—ãŸ")
    print(f"ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(df_training):,}ä»¶")
    print(f"ç©´é¦¬ã‚µãƒ³ãƒ—ãƒ«æ•°: {df_training['is_upset'].sum():,}ä»¶")
    print(f"ç‰¹å¾´é‡æ•°: {len(feature_cols)}å€‹")
    
    print(f"\n{'='*80}")
    print("åˆ†æå®Œäº†!")
    print(f"{'='*80}")
    
    # Phase 2ã¸ã®æè¨€
    print(f"\nã€Phase 2è¨­è¨ˆã¸ã®çŸ¥è¦‹ã€‘")
    print(f"1. Phase 1ã§æ¤œå‡ºã§ãã‚‹ç©´é¦¬ã®å‰²åˆã‚’ç¢ºèª")
    print(f"2. äºˆæ¸¬é †ä½ãŒä½ãã¦ã‚‚çš„ä¸­ã™ã‚‹ç©´é¦¬ã®ç‰¹å¾´ã‚’ç‰¹å®š")
    print(f"3. äººæ°—é¦¬ã¨ã®ç‰¹å¾´é‡ã®å·®åˆ†ã‚’ç¢ºèª")
    print(f"4. ãƒ¬ãƒ¼ã‚¹æ¡ä»¶ã«ã‚ˆã‚‹ç©´é¦¬å‡ºç¾ç‡ã®é•ã„ã‚’ç¢ºèª")
    print(f"\nã“ã‚Œã‚‰ã®çŸ¥è¦‹ã‚’å…ƒã«ã€Phase 2ï¼ˆé‡ã¿ä»˜ãoräºŒæ®µéšãƒ¢ãƒ‡ãƒ«ï¼‰ã‚’è¨­è¨ˆã—ã¦ãã ã•ã„ã€‚")


if __name__ == '__main__':
    main()
